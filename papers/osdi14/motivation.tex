
\section{Background and Motivation}
\label{sec:motivation}

Our work focuses on identifying the mismatch and opportunities for
improvement between commodity operating systems and a large, but
well-defined, class of relevant applications.
%  In this section, we
% first define the class of applications in our focus
% (\S\ref{sec:motivation:web}), then describe the system software
% challenges associated with that class of applications
% (\S\ref{sec:motivation:challenges}), and identify an alternative model
% -- the dataplane -- which addresses these challenges
% (\S\ref{sec:motivation:dp}.

\subsection{Event-driven, Web-scale Applications}
\label{sec:motivation:web}

Modern web-scale applications include interactive services such as
search, social networking, webmail and instant messaging, online maps,
automatic translation, software as a service (SaaS), and e-commerce
platforms.  We have come to expect that these services provide
millions of users with instantaneous, personalized, and contextual
access to petabytes of data.  For example, the Google search engine
updates query results interactively as the user types, predicting the
most likely query, performing the search and showing the results
within a few tens of milliseconds~\cite{DBLP:journals/cacm/DeanB13}.

Internally, such applications use a service-oriented architecture and
consist of tens of distinct, well-defined services such as
load-balancing and http proxies, web and application serving, content
distribution and streaming, memory caching, queuing services,
relational databases and object
storage~\cite{Alonso:2010:WSC,Eriksen:2013:YSF}. \christos{what is the
  authoritative ref for SOA?} Collectively, these services occupy
hundreds to thousands of servers connected through high-speed
networking. Different services are routinely implemented using
different programming languages and paradigms, but are connected
through a unifying framework for RPC, serialization, service
discovery, and logging \cite{protocolbuffers, thrift, fingale,
  others}. An incoming request from an external user leads to tens to
hundreds of internal requests across the various services, with some
requests issued sequentially due to dependencies in application logic,
while other requests are issued in a parallel, fan-out manner.
% In addition, web-scale applications also rely on massive,
% batch-oriented services that prepare the data for interactive serving
% (e.g., the search engine crawlers), identify patterns and behavior
% from the massive data sets and the users' behaviors via machine
% learning~\cite{missing}.

Each node for these services responds to external requests such as new
incoming connections, data requests from existing connections, or
replies to requests made to a downstream service.  The most efficient
implementations today use event-driven framworks in which the service
logic interacts with the framework exclusively via non-blocking
calls. This is in contrast with the classic thread-oriented
programming paradigm, in which sessions are divided among a large pool
of (kernel) threads, which block while waiting for incoming traffic or
outstanding replies. For example, the event-oriented nginx HTTP server
and proxy~\cite{reese2008nginx} outperforms the thread-oriented Apache
server~\cite{misc:apache}.

High-level libraries simplify the development of event-driven
applications~\cite{provos2003libevent,libev,libuv}.  For example, the
popular \texttt{libevent} library~\cite{provos2003libevent} provides a
level of abstraction that exposes the paradigm to applications running
on top of Linux, *BSD, and Windows.  Many web-scale applications, such
as memcached~\cite{missing}, are built on top of libevent.  Others
such as Nginx and node.js are built using similar frameworks.


\subsection{Main Requirements}
\label{sec:motivation:challenges}

Although the event-based paradigm has several system-level benefits,
such as lower context switching rates, its use in web-scale
applications poses many unique challenges to system software:

{\bf High packet rates:} The requests and, often times, the replies
between services in web-scale applications are quite small. In the
memcached service at Facebook, for example, the vast majority of
requests use keys shorter than 50 bytes and involve values shorter
than 500 bytes~\cite{Atikoglu:2012:WAL}. As a result, each service
node must ideally scale to extremely high packet rates, measured in
millions of packets per second on 10 GbE or 40 GbE network interfaces (NICs).
In such conditions, common hardware optimizations found in network
interfaces, such as TCP segmentation and large receive offloading,
have a marginal impact on performance. 

{\bf Micro-second latency:} To enable rich interactions between a
large number of services in a web-scale application without impacting
the overall latency experienced by the user, it is essential to
minimize the latency for each service
request~\cite{luiz-isscc,Rumble:2011:TLL}. Today's networking
technologies allow for one-way communication across a large-scale
datacenter within a few \microsecond : 3~\microsecond latency across
a pair of 10 GbE NICs~\cite{cisco-sereno}, one to five switch
crossings with cut-through latencies of a few hundred $ns$ each, and
propagation delays of 500 $ns$ for 100 meters distance. Unfortunately,
system software is designed to operate at a totally different
timescale. Interrupt coalescing used to improve throughput, queuing
latency due to device driver processing intervals, and protocol
processing of incoming packets without necessarily immediately
scheduling the receiving application frequently add several $ms$ of
latency to to remote procedure calls (see measurements in Section
\ref{sec:eval}).

% As a result, the remote procedure call latency observed by application
% software is one to two orders of magnitude greater than the intrinsic
% latency.  For example, we measure the average NetPIPE latency between
% two vanilla Linux servers with Intel XXX NICs at XXX\microsecond, when
% in fact such communication is possible in only XXX\microsecond (see
% \S\ref{missing}).

These issues compound when considering the long tail of the latency
distributions of RPCs across thousands of
servers~\cite{DBLP:journals/cacm/DeanB13}. Although tail-tolerance is
actually an end-to-end challenge, the system software stack plays a
significant role in exacerbating the problem~\cite{Leverich:RHSU:2014}.
Protocol-processing techniques that reduce latency to \microsecond in
the average, but do no improve 95th or 99th percentile latency are
unlikely to result into end-to-end application gains.

{\bf Connection scalability:} For applications that use thousands
multi-core servers, it is desirable to efficiently support both a
large number of concurrent connections per server, as well as high
churn in these connections.  Ideally, the only limitation to a
server's concurrent connection count should be its memory capacity and
the packet rate and tail latency should be independent of the nunber
of concurrent connections or the rate of churn.
 
Connection scalability has been a historical challenge for commodity
operating systems, reaching 10,000 concurrent connections a decade
ago~\cite{theC10Kproblem} and exceeding one million connections
today~\cite{theC10Mproblem}. For example, the nature of all-to-all
communication between Facebook's application and memcached servers
made it impractical to use TCP sockets between these two tiers,
resulting in deployments that use UDP datagrams for \texttt{get}
operations and an aggregation proxy for \texttt{put}
operations~\cite{nishtala2013scaling}.

{\bf Resource elasticity:} The load of web-scale applications varies
significantly due to diurnal patterns and unexpected spikes in user
traffic. Ideally, each service node will use the minimum amount of
resources (cores, memory, or IOPS) needed to satisfy packet rate and
tail latency requirements at any point in time. The remaining
resources can be allocated to other applications in a shared
datacenter (e.g., background
analytics)~\cite{Hindman:2011:MPF,DBLP:conf/asplos/DelimitrouK14,Leverich:RHSU:2014}
or placed into low power modes in order to achieve energy
proportionality~\cite{DBLP:journals/computer/BarrosoH07}.
\christos{keeping this simple since we will not do much in this paper}

% Datacenters for web-scale applications are
% essentially built today using thousands of commodity Xeon-class
% processors that draw hundreds of Watts each.  The dimension of each
% building block -- number of sockets, cores, DRAM, disk capacity, IOPS
% and bandwidth, and networking bandwidth -- are largely determined by
% architectural trends.  Ideally, applications would only consume
% resources (incl. energy) that are proportional to their current load.
% Unfortunately, mapping such applications efficiently onto the
% scale-out model remains an open challenge~\cite{missing} as it requires
% a combination of cluster-wide, and machine-local resource management,
% workload placement and rebalancing techniques~\cite{missing,missing}.

{\bf Security and protection:} Finally, as multiple foreground and
background services share multi-core servers even in private
datacenters~\cite{Schwarzkopf:2013:OFS,DBLP:journals/cacm/DeanB13},
there is need for isolated protocol stacks. For single-tenant
deployments where all services are administered by the same
organization, the use of TCP-based protocol running with commodity
kernels such as Linux largely addresses the problem.  Challenges
emerge when the networking stack is lifted into the user-space and
application bugs can corrupt the networking stack and impact other
services.


\subsection{Current Approaches}
\label{sec:motivation:current}

\christos{If we leave this here, it is essentially related work. It
  needs to be beefed up and will overlap with the one at the end. I
  suggest we inline the critique to current approaches in the two
  previous section. In 2.1, we add a paragrah that lists the main
  approarch. Then as we review requirements, we inline in each
  paragraph how some current approaches fail it. }

The conventional wisdom is that these challenges are rooted in a
mismatch between existing commodity operating systems, existing
network protocols, and the unique requirements of web-scale
applications.  Indeed, with today's technology, commodity Xeon
processors support 8-10 cores with dedicated L1 caches, between 16-40
hyperthreads, up to 50 Gbps of PCIe bandwidth \christos{Intel E5 chips
  have 24GBytes/sec PCIe bandwidth per socket}, and NIC that transfer
frames in a few \microsecond.  Unfortunately, the distinct nature of
web-scale applications prevents them from saturating the hardware.
Different approaches have been proposed to address each a subset of
these challenges:

\paragraph{Running the networking stack in user-space.}  

Approaches such as OpenOnload~\cite{openonload} or
mTCP~\cite{jeong2014mtcp} run the entire networking stack in
userspace, which are designed to offer load-latency communication
(e.g. OpenOnload) or connection scalability (e.g. mTCP), at the
expense of protection.  mTCP's design has the TCP stack running on
dedicated hyperthreads which synchronizes at relatively coarse grain
with the application. Although tail latency can be bounded, the focus
is measured at ms-scale rather than \microsecond-scale.

\paragraph{Alternatives to TCP.}

Low-latency key-value stores such as
RAMcloud~\cite{DBLP:conf/sosp/OngaroRSOR11} rely on kernel-bypass and
RDMA to offload all protocol processing on dedicated Infiniband Host Channel
Adapters.  Using standard hardware and networking fabrics, Facebook's
deployment of memcached uses UDP to avoid any connection scalability
limitations.  Even though UDP itself is running in the kernel, the
responsibilities of congestion management and flow control and
entrusted into applications.

\paragraph{Alternatives to POSIX API.}

MegaPipe~\cite{han2012megapipe} replaces the POSIX API with
lightweight sockets implement by in-memory command rings.  This
substantially reduces software overheads and substantially increases
packet rates.  The pragmatic use of the operating system's TCP stack
and interrupt model presents any microsecond-computing solution.

\paragraph{Enhancements to the OS.}

Operating system modifications tradeoff superior ease of deployability
(and quite frankly, pragmatism in terms of production use) even though
the improvements might be more incremental.
Affinity-Accept~\cite{DBLP:conf/eurosys/PesterevSZM12} improves the
affinity of incoming and outgoing network processing by considering
the affinity of flows to core and controlling the NIC hardware.  Linux
\texttt{SO\_REUSE} allows multi-threaded applications to accept
incoming connections in parallel.  Although such approaches minimize
communication between core, they do not eliminate them.  The lack of
commutativity~\cite{DBLP:conf/sosp/ClementsKZMK13} in the POSIX socket
API implies that cross-core communication is necessary whenever a
connection is accepted or closed.  When microsecond latencies are
irrelevant, such as Internet-facing communication services, properly
tuned stacks can support millions of concurrent connections in
production~\cite{whatsapp-2mil}.  
