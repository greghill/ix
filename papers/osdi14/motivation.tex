
\section{Background and Motivation}
\label{sec:motivation}

Our work focuses on improving operating systems for the large and
well-defined class of web-scale applications. 

\subsection{Event-driven, Web-scale Applications}
\label{sec:motivation:web}

Modern web-scale applications include interactive services such as
search, social networking and instant messaging, automatic
translation, software as a service, and e-commerce platforms.
We have come to expect that these services provide millions of users
with instantaneous, personalized, and contextual access to petabytes
of data.  For example, the Google search engine updates query results
interactively as the user types, predicting the most likely query,
performing the search and showing the results within a few tens of
milliseconds~\cite{DBLP:journals/cacm/DeanB13}.

Internally, such applications use a service-oriented architecture and
consist of tens of distinct, well-defined services such as
load-balancing and HTTP proxies, web and application serving, content
distribution and streaming, memory caching, queuing services,
relational databases and object
storage~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07,Alonso:2010:WSC,Eriksen:2013:YSF}.
Collectively, these services occupy hundreds to thousands of servers
connected through high-speed networking. An incoming request from an
external user leads to tens to hundreds of internal requests across
the various services, with some requests issued sequentially due to
dependencies in application logic, while other requests are issued in
a parallel, fan-out manner.  Different services are routinely
implemented using different programming languages, but are connected
through a unifying framework for RPC, serialization, service
discovery, and logging~\cite{protocolbuffers, thrift,
  finagle} and most commonly use TCP/IP as their transport
layer.

Each node for these services responds to requests such as new incoming
connections, data requests from existing connections, or replies to
requests made to a downstream service.  The most efficient
implementations use event-driven frameworks in which the service logic
interacts with the framework exclusively via non-blocking
calls~\cite{DBLP:conf/usenix/PaiDZ99,DBLP:conf/sosp/WelshCB01}.
High-level libraries simplify the development of event-driven
applications~\cite{provos2003libevent,libev,libuv}.  For example, the
popular \texttt{libevent} library provides a level of abstraction that
exposes the paradigm to Linux, *BSD, and Windows
applications~\cite{provos2003libevent}. For example,
\texttt{memcached} is built on top of
\texttt{libevent}~\cite{url:memcached}, while \texttt{nginx} and
\texttt{node.js} are built using similar frameworks.


\subsection{Main Requirements}
\label{sec:motivation:challenges}

Web-scale applications pose unique challenges to system
software and their networking stacks:


\myparagraph{High packet rates:} The requests and, often times, the
replies between services in web-scale applications are quite small. In
Facebook's \texttt{memcached} service, for example, the vast majority
of requests use keys shorter than 50 bytes and involves values shorter
than 500 bytes~\cite{Atikoglu:2012:WAL}. As a result, each service
node must ideally scale to extremely high packet rates, measured in
millions of packets per second on 10 GbE or 40 GbE NICs.  In such
conditions, common NIC hardware optimizations, such as TCP
segmentation and receive side coalescing, have a marginal impact on
performance.

\myparagraph{Micro-second latency:} To enable rich interactions
between a large number of services
without impacting the overall latency experienced by the user, it is essential
to minimize the latency for each service
request~\cite{luiz-isscc,DBLP:conf/hotos/RumbleOSRO11}. Today's networking technologies
allow for one-way communication across a large-scale datacenter within
a few \microsecond: 3\microsecond latency across a pair of 10 GbE
NICs~\cite{cisco-sereno}, one to five switch crossings with
cut-through latencies of a few hundred ns each, and propagation delays
of 500ns for 100 meters distance. Unfortunately, system software is
designed to operate at a totally different timescale. Interrupt
coalescing used to improve throughput, queuing latency due to device
driver processing intervals, and protocol processing of incoming
packets without immediately scheduling the receiving application
frequently add several hundred \microsecond of latency to remote
procedure calls (see measurements in \S\ref{sec:eval}).


These issues compound when considering the long tail of the latency
distributions of RPCs across thousands of
servers~\cite{DBLP:journals/cacm/DeanB13}. Although tail-tolerance is
actually an end-to-end challenge, the system software stack plays a
significant role in exacerbating the
problem~\cite{DBLP:conf/eurosys/LeverichK14}.  Protocol-processing techniques
that reduce average latency, but do not improve 95th or 99th
percentile latency are unlikely to result in end-to-end application
gains.

\myparagraph{Connection scalability:} For applications that use
thousands of multi-core servers, it is desirable to efficiently
support both a large number of concurrent connections per server, as
well as high connections churn.  Ideally, the only limitation to a
server's concurrent connection count should be its memory
capacity. Furthermore, the packet rate and tail latency should be
independent of the number of concurrent connections or the rate of
churn.
 
Connection scalability has been a historical challenge for commodity
operating systems, reaching 10,000 concurrent connections a decade
ago~\cite{theC10Kproblem} and exceeding one million connections
today~\cite{theC10Mproblem}.  For example, the communication patterns
between application and \texttt{memcached} servers at Facebook made it
impractical to use TCP sockets between these two tiers, resulting in
deployments that use UDP datagrams for \texttt{get} operations and an
aggregation proxy for \texttt{put}
operations~\cite{DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}.

\myparagraph{Resource elasticity:} The load of web-scale applications
varies significantly due to diurnal patterns and unexpected spikes in
user traffic. Ideally, each service node will use the fewest
resources (cores, memory, or IOPS) needed to satisfy packet rate
and tail latency requirements at any point. The remaining
resources can be allocated to other applications in a shared
datacenter (e.g., background
analytics)~\cite{Hindman:2011:MPF,DBLP:conf/asplos/DelimitrouK14,DBLP:conf/eurosys/LeverichK14}
or placed into low power modes in order to achieve energy
proportionality~\cite{DBLP:journals/computer/BarrosoH07, DBLP:conf/isca/LoCGBK14}.

\myparagraph{Protection:} Finally, as multiple foreground and
background services share multi-core servers even in private
datacenters~\cite{Hindman:2011:MPF,Schwarzkopf:2013:OFS,DBLP:journals/cacm/DeanB13},
there is need for isolated network stacks. For single-tenant
deployments where all services are administered by the same
organization, the use of kernel-based networking largely addresses the
problem.  However, security tradeoffs emerge when networking is lifted
into the user-space and application bugs can corrupt the networking
stack and impact other services.


\subsection{Alternative Approaches}
\label{sec:motivation:current}

TCP-based network stacks implemented within commodity kernels such as
Linux do not meet these requirements, despite the abundance of hardware
resources (tens of cores and tens of GBytes/s DRAM and PCIe
bandwidth per server, 10 to 40 GbE interfaces, and \microsecond-level
latency across datacenters).
Alternative approaches have been proposed, each addressing a subset,
but not all, of
the requirements for web-scale applications.


\myparagraph{User-space networking stacks:} Systems such as
OpenOnload~\cite{openonload}, mTCP~\cite{jeong2014mtcp} or
Sandstorm~\cite{DBLP:conf/sigcomm/MarinosWH14} run the entire networking stack in
user-space, offering either low latency or connection scalability at
the expense of protection. There are still tradeoffs between packet
rate and latency. For instance, mTCP uses dedicated threads for the
TCP stack, which communicate at relatively coarse grain with
application threads. This amortizes switching overheads at the expense
of higher latency.

\myparagraph{Alternatives to TCP:} Some low-latency object stores rely on
kernel-bypass and RDMA to offload protocol processing on dedicated
Infiniband host channel
adapters~\cite{DBLP:conf/sosp/OngaroRSOR11,DBLP:conf/icpp/JoseSLZHWIOWSP11,mitchell:rdma,dragojevic14farm}.
Using commodity Ethernet networking, Facebook's memcached deployment
uses UDP to avoid connection scalability
limitations~\cite{DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}. Even though UDP is running in
the kernel, reliable communication and congestion management are
entrusted to applications.

\myparagraph{Alternatives to POSIX API:} MegaPipe replaces the POSIX
API with lightweight sockets implemented with in-memory command
rings~\cite{DBLP:conf/osdi/HanMCR12}. This substantially reduces software
overheads and increases packet rates, but retains all other challenges
of using an existing, kernel-based networking stack.

\myparagraph{OS enhancements:} Tuning kernel-based stacks provides
incremental benefits with superior ease of deployment.  Linux
\texttt{SO\_REUSEPORT} allows multi-threaded applications to accept
incoming connections in parallel. Affinity-accept reduces overheads by
considering the affinity of network flows to specific
cores~\cite{DBLP:conf/eurosys/PesterevSZM12}. These approaches cannot
completely eliminate communication between cores, as the lack of
commutativity in the POSIX socket API implies that cross-core
communication is needed when connections are accepted or
closed~\cite{DBLP:conf/sosp/ClementsKZMK13}. When microsecond
latencies are irrelevant\ana{(tolerable?)}, as with Internet-facing
messaging services, properly tuned stacks can support millions of
concurrent connections~\cite{whatsapp-2mil}.
