
\section{Background and Motivation}
\label{sec:motivation}

Our work focuses on identifying the mismatch and opportunities for
improvement between commodity operating systems and a large, but
well-defined, class of relevant applications.
%  In this section, we
% first define the class of applications in our focus
% (\S\ref{sec:motivation:web}), then describe the system software
% challenges associated with that class of applications
% (\S\ref{sec:motivation:challenges}), and identify an alternative model
% -- the dataplane -- which addresses these challenges
% (\S\ref{sec:motivation:dp}.

\subsection{Event-driven, Web-scale Applications}
\label{sec:motivation:web}

Modern web-scale applications include interactive services such as
search, social networking, webmail and instant messaging, online maps,
automatic translation, software as a service (SaaS), and e-commerce
platforms.  We have come to expect that these services provide
millions of users with instantaneous, personalized, and contextual
access to petabytes of data.  For example, the Google search engine
updates query results interactively as the user types, predicting the
most likely query, performing the search and showing the results
within a few tens of milliseconds~\cite{DBLP:journals/cacm/DeanB13}.

Internally, such applications use a service-oriented architecture and
consist of tens of distinct, well-defined services such as
load-balancing and HTTP proxies, web and application serving, content
distribution and streaming, memory caching, queuing services,
relational databases and object
storage~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07,Alonso:2010:WSC,Eriksen:2013:YSF}. 
Collectively, these services occupy
hundreds to thousands of servers connected through high-speed
networking. Different services are routinely implemented using
different programming languages and paradigms, but are connected
through a unifying framework for RPC, serialization, service
discovery, and logging~\cite{protocolbuffers, thrift, fingale,
  others}. An incoming request from an external user leads to tens to
hundreds of internal requests across the various services, with some
requests issued sequentially due to dependencies in application logic,
while other requests are issued in a parallel, fan-out manner.
% In addition, web-scale applications also rely on massive,
% batch-oriented services that prepare the data for interactive serving
% (e.g., the search engine crawlers), identify patterns and behavior
% from the massive data sets and the users' behaviors via machine
% learning~\cite{missing}.

Each node for these services responds to requests such as new incoming
connections, data requests from existing connections, or replies to
requests made to a downstream service.  The most efficient
implementations use event-driven frameworks in which the service logic
interacts with the framework exclusively via non-blocking
calls~\cite{DBLP:conf/usenix/PaiDZ99,missing,missing}. This is in
contrast with the classic thread-oriented programming paradigm, in
which sessions are divided among a large pool of (kernel) threads,
which block while waiting for incoming traffic or outstanding
replies. For example, event-oriented nginx HTTP server and
proxy~\cite{reese2008nginx} outperforms the thread-oriented Apache
server~\cite{misc:apache}.

High-level libraries simplify the development of event-driven
applications~\cite{provos2003libevent,libev,libuv}.  For example, the
popular \texttt{libevent} library~\cite{provos2003libevent} provides a
level of abstraction that exposes the paradigm to applications running
on top of Linux, *BSD, and Windows.  Many web-scale applications, such
as memcached~\cite{missing}, are built on top of \texttt{libevent}.  Others
such as nginx and node.js are built using similar frameworks.


\subsection{Main Requirements}
\label{sec:motivation:challenges}

Although the event-based paradigm has several system-level benefits,
such as lower context switching rates~\cite{missing}, its use in web-scale
applications poses many unique challenges to system software:

\myparagraph{High packet rates:} The requests and, often times, the replies
between services in web-scale applications are quite small. In the
memcached service at Facebook, for example, the vast majority of
requests use keys shorter than 50 bytes and involve values shorter
than 500 bytes~\cite{Atikoglu:2012:WAL}. As a result, each service
node must ideally scale to extremely high packet rates, measured in
millions of packets per second on 10 GbE or 40 GbE network interfaces (NICs).
In such conditions, common hardware optimizations found in network
interfaces, such as TCP segmentation and large receive offloading,
have a marginal impact on performance. 

\myparagraph{Micro-second latency:} To enable rich interactions between a
large number of services in a web-scale application without impacting
the overall latency experienced by the user, it is essential to
minimize the latency for each service
request~\cite{luiz-isscc,rumble2011s}. Today's networking
technologies allow for one-way communication across a large-scale
datacenter within a few \microsecond: 3\microsecond latency across
a pair of 10 GbE NICs~\cite{cisco-sereno}, one to five switch
crossings with cut-through latencies of a few hundred $ns$ each, and
propagation delays of 500$ns$ for 100 meters distance. Unfortunately,
system software is designed to operate at a totally different
timescale. Interrupt coalescing used to improve throughput, queuing
latency due to device driver processing intervals, and protocol
processing of incoming packets without necessarily immediately
scheduling the receiving application frequently add several $ms$ of
latency to remote procedure calls (see measurements in \S\ref{sec:eval}).

% As a result, the remote procedure call latency observed by application
% software is one to two orders of magnitude greater than the intrinsic
% latency.  For example, we measure the average NetPIPE latency between
% two vanilla Linux servers with Intel XXX NICs at XXX\microsecond, when
% in fact such communication is possible in only XXX\microsecond (see
% \S\ref{missing}).

These issues compound when considering the long tail of the latency
distributions of RPCs across thousands of
servers~\cite{DBLP:journals/cacm/DeanB13}. Although tail-tolerance is
actually an end-to-end challenge, the system software stack plays a
significant role in exacerbating the problem~\cite{Leverich:RHSU:2014}.
Protocol-processing techniques that reduce latency to \microsecond in
the average, but do no improve 95th or 99th percentile latency are
unlikely to result into end-to-end application gains.

\myparagraph{Connection scalability:} For applications that use thousands
multi-core servers, it is desirable to efficiently support both a
large number of concurrent connections per server, as well as high
churn in these connections.  Ideally, the only limitation to a
server's concurrent connection count should be its memory capacity, and
the packet rate and tail latency should be independent of the number
of concurrent connections or the rate of churn.
 
Connection scalability has been a historical challenge for commodity
operating systems, reaching 10,000 concurrent connections a decade
ago~\cite{theC10Kproblem} and exceeding one million connections
today~\cite{theC10Mproblem}. For example, the nature of all-to-all
communication between Facebook's application and memcached servers
made it impractical to use TCP sockets between these two tiers,
resulting in deployments that use UDP datagrams for \texttt{get}
operations and an aggregation proxy for \texttt{put}
operations~\cite{nishtala2013scaling}.

\myparagraph{Resource elasticity:} The load of web-scale applications
varies significantly due to diurnal patterns and unexpected spikes in
user traffic. Ideally, each service node will use the minimum amount
of resources (cores, memory, or IOPS) needed to satisfy packet rate
and tail latency requirements at any point in time. The remaining
resources can be allocated to other applications in a shared
datacenter (e.g., background
analytics)~\cite{Hindman:2011:MPF,DBLP:conf/asplos/DelimitrouK14,Leverich:RHSU:2014}
or placed into low power modes in order to achieve energy
proportionality~\cite{DBLP:journals/computer/BarrosoH07, Lo:2014:TWE}.
%\christos{keeping this simple since we will not do much in this paper}

% Datacenters for web-scale applications are
% essentially built today using thousands of commodity Xeon-class
% processors that draw hundreds of Watts each.  The dimension of each
% building block -- number of sockets, cores, DRAM, disk capacity, IOPS
% and bandwidth, and networking bandwidth -- are largely determined by
% architectural trends.  Ideally, applications would only consume
% resources (incl. energy) that are proportional to their current load.
% Unfortunately, mapping such applications efficiently onto the
% scale-out model remains an open challenge~\cite{missing} as it requires
% a combination of cluster-wide, and machine-local resource management,
% workload placement and rebalancing techniques~\cite{missing,missing}.

\myparagraph{Protection:} Finally, as multiple foreground and
background services share multi-core servers even in private
datacenters~\cite{Schwarzkopf:2013:OFS,DBLP:journals/cacm/DeanB13},
there is need for isolated protocol stacks. For single-tenant
deployments where all services are administered by the same
organization, the use of kernel-based protocol stacks largely
addresses the problem.  Challenges emerge when the networking stack is
lifted into the user-space and application bugs can corrupt the
networking stack and impact other services.


\subsection{Alternative Approaches}
\label{sec:motivation:current}

TCP-based network stacks implemented within commodity kernels such as
Linux do not meet these requirements, despite the abundance of hardware
resources (tens of cores and tens of GBytes/sec DRAM and PCIe
bandwidth per server, 10 to 40 GbE interfaces, and \microsecond-level
latency across datacenters).
% The conventional wisdom is that these challenges are rooted in a
% mismatch between existing commodity operating systems, existing
% network protocols, and the unique requirements of web-scale
% applications.  Indeed, with today's technology, commodity Xeon
% processors support 8-10 cores with dedicated L1 caches, between 16-40
% hyperthreads, up to 50 Gbps of PCIe bandwidth \christos{Intel E5 chips
%   have 24GBytes/sec PCIe bandwidth per socket}, and NIC that transfer
% frames in a few \microsecond. 
%Unfortunately, the distinct nature of
%web-scale applications prevents them from saturating the hardware.
Alternative approaches have been proposed, each addressing a subset of
the requirements for web-scale applications. However, no
proposal meets all requirements on commodity hardware. 

\myparagraph{User-space networking stacks:} Systems such as
OpenOnload~\cite{openonload} or mTCP~\cite{jeong2014mtcp} run the
entire networking stack in user-space, offering either low latency
(OpenOnload) or connection scalability (mTCP) at the expense of
protection. There are still tradeoffs between packet rate and
latency. For instance, mTCP uses dedicated threads for the TCP
stack, which communicate at relatively coarse grain with application
threads. This amortizes switching overheads at the
expense of \microsecond-scale latency.

\myparagraph{Alternatives to TCP:} Some low-latency key-value stores or object managers rely on
kernel-bypass and RDMA to offload protocol processing on dedicated
Infiniband host channel
adapters~\cite{DBLP:conf/sosp/OngaroRSOR11,Jose:2011:MDH,mitchell:rdma,dragojevic14farm}.  Using
commodity Ethernet networking, Facebook's memcached deployment uses
UDP to avoid connection scalability
limitations~\cite{nishtala2013scaling}. Even though UDP is running in
the kernel, congestion management and flow control are entrusted into
applications.

\myparagraph{Alternatives to POSIX API:} MegaPipe~\cite{han2012megapipe}
replaces the POSIX API with lightweight sockets implemented with
in-memory command rings. This substantially reduces software overheads
and increases packet rates. However, the pragmatic use of the kernel
TCP stack and interrupt model maintains the latency challenges of the
original system.

\myparagraph{OS enhancements:} Tuning kernel-based stacks provides incremental
benefits with superior ease of deployment.  Linux \texttt{SO\_REUSEPORT}
allows multi-threaded applications to accept incoming connections in
parallel. Affinity-accept reduces overheads by considering the
affinity of network flows to specific
cores~\cite{DBLP:conf/eurosys/PesterevSZM12}. These approaches cannot
completely eliminate communication between cores, as the lack of
commutativity in the POSIX socket API implies that cross-core
communication is needed when connections are accepted or
closed~\cite{DBLP:conf/sosp/ClementsKZMK13}. When microsecond
latencies are irrelevant, such as Internet-facing communication
services, properly tuned stacks can support millions of concurrent
connections~\cite{whatsapp-2mil}.
