
\section{Background and Motivation}
\label{sec:motivation}

Our work focuses on improving operating systems for applications with
aggressive networking requirements running on multi-core servers.


\subsection{Application Requirements}
\label{sec:motivation:challenges}

% \christos{We need to make sure that the requirements in 2.2 and
%   approach in 3 are aligned. Currently, they don't match that well}

Large-scale, distributed applications pose unique challenges to system
software and their networking stacks:

\myparagraph{Micro-second tail latency:} To enable rich interactions
between a large number of services without impacting the overall
latency experienced by the user, it is essential to reduce the latency
for each service request to a few tens of
\microsecond~\cite{luiz-isscc,DBLP:conf/hotos/RumbleOSRO11}.  Since
each user request often involves hundreds of servers, we must also
consider the long tail of the latency distributions of RPC requests
across the datacenter~\cite{DBLP:journals/cacm/DeanB13}. Although
tail-tolerance is actually an end-to-end challenge, the system
software stack plays a significant role in exacerbating the
problem~\cite{DBLP:conf/eurosys/LeverichK14}. Overall, each service
node must ideally provide tight bounds on the 99th percentile request
latency.
% Protocol-processing techniques that reduce average latency, but do
% not improve 95th or 99th percentile latency are unlikely to result
% in end-to-end application gains.

\myparagraph{High packet rates:} The requests and, often times, the
replies between the various services that comprise a web-scale
application are quite small. In Facebook's \texttt{memcached} service,
for example, the vast majority of requests uses keys shorter than 50
bytes and involves values shorter than 500
bytes~\cite{Atikoglu:2012:WAL}. As a result, each service node must
ideally scale to extremely high packet rates, measured in millions of
packets per second. 

% Executive call: downgrade a little the discussion about connections
% \christos{How do we explain IX's higher number of connections?
% Better data structures?  Less locking? Right now it is not
% explained. Based on how we explain it later we should edit this,
% potentially moving it further down the list or merging it with the
% high throughput}
The high packet rate must be sustainable under a large number of
concurrent connections and high connection
churn~\cite{theC10Mproblem}. If the system software cannot handle
large connection counts, there can be significant implications for
applications.  The large connection count between
application and \texttt{memcached} servers at Facebook made it
impractical to use TCP sockets between these two tiers, resulting in
deployments that use UDP datagrams for \texttt{get} operations and an
aggregation proxy for \texttt{put}
operations~\cite{DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}.

\myparagraph{Protection:} Since multiple services commonly share the
servers in both public and private
datacenters~\cite{DBLP:journals/cacm/DeanB13,Hindman:2011:MPF,Schwarzkopf:2013:OFS},
there is need for isolation between applications. The use of
kernel-based or hypervisor-based networking stacks largely addresses
the problem. A trusted network stack can firewall applications,
enforce access control lists (ACLs), implement limiters and other
policies based on bandwith metering


\myparagraph{Resource efficiency:} The load of web-scale applications
varies significantly due to diurnal patterns and spikes in user
traffic. Ideally, each service node will use the fewest resources
(cores, memory, or IOPS) needed to satisfy packet rate and tail
latency requirements at any point. The remaining server resources can
be allocated to other
applications~\cite{DBLP:conf/asplos/DelimitrouK14,Hindman:2011:MPF} or
placed into low power mode for energy
efficiency~\cite{DBLP:journals/computer/BarrosoH07}. Existing
operating systems can support such resource usage
policies~\cite{DBLP:conf/eurosys/LeverichK14,DBLP:conf/isca/LoCGBK14}.

\subsection{The Hardware -- Software Mismatch}
\label{sec:motivation:web}

\edb{Use Hardware-Kernel mistmatch instead?}


The wealth of hardware resources in modern servers should allow for
low latency and high packet rates for web-scale applications.  A
typical server includes one or two processor sockets, each with ten or
more multithreaded cores and multiple, high-speed channels to DRAM and
PCIe devices. Solid-state drives and PCIe-based Flash storage are also
increasingly popular. For networking, 10 GbE NICs and switches are
widely deployed in datacenters, with 40 GbE and 100 GbE technologies
right around the corner. The combination of tens of hardware threads
and 10 GbE NICs should allow for rates of 20M packets/sec with minimum
sized packets.  We should also achieve 10-20\microsecond round-trip
latencies given 3\microsecond latency across a pair of 10 GbE NICs,
one to five switch crossings with cut-through latencies of a few
hundred ns each, and propagation delays of 500ns for 100 meters
distance within a datacenter.

% \dm{But it might be worth another paragraph addressing why now
%   Basically there's a fundamental question of granularity here.  E.g.,
%   page sizes have been 4--8K since machines had less than a Megabyte
%   of DRAM\@.  Schedulers have been designed under the premise of more
%   applications than CPUs.  Network APIs were designed when packet
%   inter-arrival times were many times the system call/interrupt
%   latency.}

Unfortunately, \edb{commodity operating systems have\sout{existing system software has}} been designed under very
different hardware assumptions. Kernel schedulers, networking APIs,
and network stacks have been designed under the assumptions of
multiple applications sharing a single processing core and packet
inter-arrival times many times higher than the latency of interrupts
and system calls. 
\edb{As a result, such operating systems trade-off both latency and throughput in favor of fine-grain resource scheduling.}
Interrupt coalescing used to reduce processing
overheads, queuing latency due to device driver processing intervals,
and \edb{the use of intermediate buffering and CPU scheduling delays \sout{protocol processing of incoming packets without immediately
scheduling the receiving application}} frequently add \edb{up to} several hundred
\microsecond of latency to remote requests. \edb{NOT BACKED UP\sout{see measurements in
\S\ref{sec:eval}).}} The overheads of buffering and synchronization
needed to support flexible scheduling of applications to cores leads
\edb{increases CPU and memory system overheads, which limits throughput.\sout{
as well as the limited locality achieved on instruction and data
caches, lead to low packet rates}} \christos{add related refs}.  As
\edb{RPCs between tiers of web-scale application often consist of small packets\sout{packets are small}}~\cite{Atikoglu:2012:WAL}, common NIC hardware optimizations, such as TCP
segmentation and receive side coalescing, have a marginal impact on
packet rate.


% Modern web-scale applications include interactive services such as
% search, social networking and instant messaging, automatic
% translation, software as a service, and e-commerce platforms.
% We have come to expect that these services provide millions of users
% with instantaneous, personalized, and contextual access to petabytes
% of data.  For example, the Google search engine updates query results
% interactively as the user types, predicting the most likely query,
% performing the search and showing the results within a few tens of
% milliseconds~\cite{DBLP:journals/cacm/DeanB13}.

% Internally, such applications use a service-oriented architecture and
% consist of tens of distinct, well-defined services such as
% load-balancing and HTTP proxies, web and application serving, content
% distribution and streaming, memory caching, queuing services,
% relational databases and object
% storage~\cite{Alonso:2010:WSC,DBLP:conf/sosp/DeCandiaHJKLPSVV07,Eriksen:2013:YSF}.
% Collectively, these services occupy hundreds to thousands of servers
% connected through high-speed networking. An incoming request from an
% external user leads to tens to hundreds of internal requests across
% the various services, with some requests issued sequentially due to
% dependencies in application logic, while other requests are issued in
% a parallel, fan-out manner.  Different services are routinely
% implemented using different programming languages, but are connected
% through a unifying framework for RPC, serialization, service
% discovery, and logging~\cite{finagle, protocolbuffers, thrift} and most commonly use TCP/IP as their transport
% layer.

% Each node for these services responds to requests such as new incoming
% connections, data requests from existing connections, or replies to
% requests made to a downstream service.  The most efficient
% implementations use event-driven frameworks in which the service logic
% interacts with the framework exclusively via non-blocking
% calls~\cite{DBLP:conf/usenix/PaiDZ99,DBLP:conf/sosp/WelshCB01}.
% High-level libraries simplify the development of event-driven
% applications~\cite{libev,libuv,provos2003libevent}.  For example, the
% popular \texttt{libevent} library provides a level of abstraction that
% exposes the paradigm to Linux, *BSD, and Windows
% applications~\cite{provos2003libevent}. For example,
% \texttt{memcached} is built on top of
% \texttt{libevent}~\cite{url:memcached}, while \texttt{nginx} and
% \texttt{node.js} are built using similar frameworks.



\subsection{Alternative Approaches}
\label{sec:motivation:current}

\christos{we need another pass below to state more clearly which
  requirements get better and which worse for each alternative}

Since the network stacks within commodity kernels cannot take
advantage of the abundance of hardware resources, a number of
alternative approaches have been suggested. Each alternative addresses a
subset, but not all, of the requirements for web-scale applications.

\myparagraph{User-space networking stacks:} Systems such as
OpenOnload~\cite{openonload}, mTCP~\cite{jeong2014mtcp} or
Sandstorm~\cite{sandstorm} run the entire networking stack in
user-space in order to eliminate kernel crossing overheads and
optimize packet processing without incurring the complexity of kernel
modifications. However, there are still tradeoffs between packet rate
and latency. For instance, mTCP uses dedicated threads for the TCP
stack, which communicate at relatively coarse grain with application
threads. This amortizes switching overheads at the expense of higher
latency (see \S\ref{sec:eval}). It also complicates resource sharing
as the network stack must use a large number of hardware threads
regardless of actual load. More important, security tradeoffs emerge
when networking is lifted into the user-space and application bugs can
corrupt the networking stack. For example, an attacker may be able
transmit raw packets, a capability that normally requires root
privileges, to exploit weaknesses in network protocols and impact
other services~\cite{DBLP:conf/acsac/Bellovin04}. It is difficult to enforce any
security or metering policies beyond what is directly supported by the
NIC hardware.

\myparagraph{Alternatives to TCP:} In addition to kernel bypass, some
low-latency object stores rely on RDMA to offload protocol processing
on dedicated Infiniband host channel
adapters~\cite{DBLP:conf/icpp/JoseSLZHWIOWSP11,DBLP:conf/sosp/OngaroRSOR11,mitchell:rdma,dragojevic14farm}.
Using commodity Ethernet networking, Facebook's memcached deployment
uses UDP to avoid connection scalability
limitations~\cite{DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}. Even though
UDP is running in the kernel, reliable communication and congestion
management are entrusted to applications.

\myparagraph{Alternatives to POSIX API:} MegaPipe replaces the POSIX
API with lightweight sockets implemented with in-memory command
rings~\cite{DBLP:conf/osdi/HanMCR12}. This substantially reduces
software overheads and increases packet rates, but retains all other
challenges of using an existing, kernel-based networking stack.

\myparagraph{OS enhancements:} Tuning kernel-based stacks provides
incremental benefits with superior ease of deployment.  Linux
\texttt{SO\_REUSEPORT} allows multi-threaded applications to accept
incoming connections in parallel. Affinity-accept reduces overheads by
considering the affinity of network flows to specific
cores~\cite{DBLP:conf/eurosys/PesterevSZM12}. These approaches cannot
completely eliminate synchronization between cores, as the lack of
commutativity in the POSIX socket API implies that cross-core
communication is needed when connections are accepted or
closed~\cite{DBLP:conf/sosp/ClementsKZMK13}. When microsecond
latencies are irrelevant, as with Internet-facing messaging services,
properly tuned stacks can support millions of concurrent
connections~\cite{whatsapp-2mil}.
