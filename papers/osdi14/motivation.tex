
\section{Motivation}
\label{sec:motivation}

The research project focuses on identifying the mismatch and
opportunities for improvement between commodity operating systems and
a large, but well-defined, class of relevant applications.  In this
section, we first define the class of applications in our focus
(\S\ref{sec:motivation:web}), then describe the system software
challenges associated with that class of applications
(\S\ref{sec:motivation:challenges}), and identify an alternative model --
the dataplane -- which addresses these challenges
(\S\ref{sec:motivation:dp}.

\subsection{Event-driven, Web-scale Applications}
\label{sec:motivation:web}

Today's web-scale applications, which our lives have grown accustomed
to to retrieve information (search), communicate (email, instant
message), share (social networking) or buy (e-tailers) simultaneously
offer quick response times to millions of concurrent users, while
accessing massive data sets spread out over thousands of servers.
Today's interactive web-scale applications have further redefined
expectations of responsiveness. For example, the Google search engine
updates query results interactively as the user types, predicting the
most likely query based on the prefix typed so far, performing the
search and showing the results within a few tens of
milliseconds~\cite{DBLP:journals/cacm/DeanB13}.

Internally, such applications are deployed on thousands of servers
operating distinct, well-defined services, such as load-balancing,
application serving, content distribution and streaming, memory
caching, relational databases and object storage~\cite{missing}.  In
addition, web-scale applications also rely on massive, batch-oriented
services that prepare the data for interactive serving (e.g., the
search engine crawlers), identify patterns and behavior from the
massive data sets and the users' behaviors via machine
learning~\cite{missing}.  Because of this diversity, different
services are routinely implemented using different programming
languages, framework and paradigms.

The event-driven paradigm is the most commonly used for services that
are directly involved in serving interactive applications.  In this
paradigm, each application node is primarily responding to external
events such as a new incoming connections, a request from an incoming
connection, or a reply to a request made to a downstream service.
This category includes stateless load balancers, HTTP proxies, web
servers, application severs, key-value stores, queuing services,
caching servers, and many others.  

For each category, the most efficient implementations today are
implemented using event-driven frameworks in which application logic
interacts with the framework exclusively via non-blocking calls.  This
is in contrast with the classic thread-oriented programming paradigm,
in which sessions are divided among a large pool of (kernel)
threads, which block while waiting for incoming traffic or outstanding
replies.  For example, the event-oriented nginx HTTP
server and proxy~\cite{reese2008nginx} outperforms the thread-oriented Apache server~\cite{misc:apache}.

Libraries simplify the development of event-driven applications.  For
example, the popular \texttt{libevent} library~\cite{provos2003libevent} provides a level of
abstraction that exposes the paradigm to applications running on top
of Linux, *BSD, and Windows~\cite{missing}.  Many web-scale
applications, including memcached~\cite{missing}, XXX~\cite{missing} are built on top
of libevent.  Others such as Nginx are built using a similar approach.


\subsection{Main Requirements}
\label{sec:motivation:challenges}

Web-scale applications pose unique challenges to system software.
Although the use of an event-oriented paradigm has system-level
benefits, in particular with respect to context switching rates, many
challenges remain that must be addressed, in particular:

\paragraph{High packet rates.}

The typical traffic between tiers of a web-scale application consists
of distinct procedure calls.  The requests are typically small; for
example, many applications use small (e.g. 16 byte) memcached keys.
The replies are also often small, typically a few
kilobytes~\cite{missing}.  As a result, web-scale servers must ideally
scale to extremely high packet rates, measured in millions of packets
per second on a 10 GbE interface..  In such conditions, the standard
optimizations found in conventional hardware such as TCP send offload
and large-scale receive offloading, have a marginal impact on
performance.

\paragraph{Micro-second computing.}

The notion behind \emph{micro-second computing}~\cite{luiz-isscc} is
that it is essential to minimize the latency of interactions within a
datacenter, and in particular of the remote procedure calls issued by
interactive applications that must access and aggregate massive
amounts of data.  Using today's networking technologies, the typical
communication delay between two servers in a datacenter involves a
pair of 10GE NICs (with a one-way latency as low as
3~\microsecond~\cite{cisco-sereno}), one, three or five cut-through
switches organized in a fat-tree topology (a few hundred $ns$ each, in
the absence of contention, and rapidly shrinking) and propagation
delays (500 $ns$ for 100 meters, with little recent improvements).
Put together, technology enables intrinsic one-way communication
latency of a handful of \microsecond.  

And yet, software ---and in particular system software--- is designed
to operate at a totally different timescale: interrupt coalescing
improve throughput but can delay packets by milliseconds; queues build
up during device driver processing intervals, introducing noticeable
queuing delays; protocol processing of incoming packets terminates in
the socket layer without necessarily immediately scheduling pending
applications. As a result, the remote procedure call latency observed
by application software is one to two orders of magnitude greater than
the intrinsic latency.  For example, we measure the average NetPIPE
latency between two vanilla Linux servers with Intel XXX NICs at
XXX\microsecond, when in fact such communication is possible in only
XXX\microsecond (see \S\ref{missing}).

These issues compound when considering the long tail of the latency
distributions of remote procedure
calls~\cite{DBLP:journals/cacm/DeanB13} . Although tail-tolerance
itself is actually an end-to-end problem, the protocol-processing
stacks play a significant role in exacerbating the problem.  In
particular, protocol-processing techniques that reduce latency to
microseconds in the common case, but don't improve the 95\% or 99\%
latency are unlikely to result into end-to-end application gains.

\paragraph{Connection scalability.}

Connection scalability is the desirable property to efficiently
support both a large number of concurrent connections, as well as high
churn in these connections.  Ideally, an operating system would
support a number of concurrent connections limited only by the
server's DRAM.  Furthermore, the operating system would feed
applications with the data stream with the same throughput and
latency, independently of the number of concurrent connections, or the
rate of churn.
 
This has been a historical challenge for commodity operating systems.
A decade ago, connection scalability challenges emerged with less than
10,000 concurrent connections~\cite{theC10Kproblem}, which ended up
requiring changes to the kernel and to applications.  Connection
scalability remains a challenge today~\cite{theC10Mproblem}.  For
example, the nature of all-to-all communication between Facebook's
application and memcache servers made it impractical to use TCP
sockets between these two tiers~\cite{nishtala2013scaling}; resulting
deployments use a UDP-based protocol for \texttt{get} operations, and
an aggregation proxy for \texttt{put} operations.


\paragraph{Resource elasticity and proportionality.}

\edb{Christos should redo}. Datacenters for web-scale applications are
essentially built today using thousands of commodity Xeon-class
processors that draw hundreds of Watts each.  The dimension of each
building block -- number of sockets, cores, DRAM, disk capacity, IOPS
and bandwidth, and networking bandwidth -- are largely determined by
architectural trends.  Ideally, applications would only consume
resources (incl. energy) that are proportional to their current load.
Unfortunately, mapping such applications efficiently onto the
scale-out model remains an open challenge~\cite{missing} as it requires
a combination of cluster-wide, and machine-local resource management,
workload placement and rebalancing techniques~\cite{missing,missing}.


\paragraph{Security and protection.}

Finally, web-scale applications, as they are deployed today, depend on
the ubiquitous presence of protected, isolated network protocol
stacks used for communication.  For single-tenant deployments where
all applications and infrastructure are administered by the same
organization, the combination of commodity operating systems such as
Linux and standard TCP-based protocols largely addresses the problem.
Challenges emerge when the networking stack is lifted from the
protected operating system and into user-space, and application bugs
can corrupt the networking stack and impact the rest of the
infrastructure.


\subsection{Current Approaches}
\label{sec:motivation:current}

\christos{If we leave this here, it is essentially related work. It
  needs to be beefed up and will overlap with the one at the end. I
  suggest we inline the critique to current approaches in the two
  previous section. In 2.1, we add a paragrah that lists the main
  approarch. Then as we review requirements, we inline in each
  paragraph how some current approaches fail it. }

The conventional wisdom is that these challenges are rooted in a
mismatch between existing commodity operating systems, existing
network protocols, and the unique requirements of web-scale
applications.  Indeed, with today's technology, commodity Xeon
processors support 8-10 cores with dedicated L1 caches, between 16-40
hyperthreads, up to 50 Gbps of PCIe bandwidth \christos{Intel E5 chips
  have 24GBytes/sec PCIe bandwidth per socket}, and NIC that transfer
frames in a few \microsecond.  Unfortunately, the distinct nature of
web-scale applications prevents them from saturating the hardware.
Different approaches have been proposed to address each a subset of
these challenges:

\paragraph{Running the networking stack in user-space.}  

Approaches such as OpenOnload~\cite{openonload} or
mTCP~\cite{jeong2014mtcp} run the entire networking stack in
userspace, which are designed to offer load-latency communication
(e.g. OpenOnload) or connection scalability (e.g. mTCP), at the
expense of protection.  mTCP's design has the TCP stack running on
dedicated hyperthreads which synchronizes at relatively coarse grain
with the application. Although tail latency can be bounded, the focus
is measured at ms-scale rather than \microsecond-scale.

\paragraph{Alternatives to TCP.}

Low-latency key-value stores such as
RAMcloud~\cite{DBLP:conf/sosp/OngaroRSOR11} rely on kernel-bypass and
RDMA to offload all protocol processing on dedicated Infiniband Host Channel
Adapters.  Using standard hardware and networking fabrics, Facebook's
deployment of memcached uses UDP to avoid any connection scalability
limitations.  Even though UDP itself is running in the kernel, the
responsibilities of congestion management and flow control and
entrusted into applications.

\paragraph{Alternatives to POSIX API.}

MegaPipe~\cite{han2012megapipe} replaces the POSIX API with
lightweight sockets implement by in-memory command rings.  This
substantially reduces software overheads and substantially increases
packet rates.  The pragmatic use of the operating system's TCP stack
and interrupt model presents any microsecond-computing solution.

\paragraph{Enhancements to the OS.}

Operating system modifications tradeoff superior ease of deployability
(and quite frankly, pragmatism in terms of production use) even though
the improvements might be more incremental.
Affinity-Accept~\cite{DBLP:conf/eurosys/PesterevSZM12} improves the
affinity of incoming and outgoing network processing by considering
the affinity of flows to core and controlling the NIC hardware.  Linux
\texttt{SO\_REUSE} allows multi-threaded applications to accept
incoming connections in parallel.  Although such approaches minimize
communication between core, they do not eliminate them.  The lack of
commutativity~\cite{DBLP:conf/sosp/ClementsKZMK13} in the POSIX socket
API implies that cross-core communication is necessary whenever a
connection is accepted or closed.  When microsecond latencies are
irrelevant, such as Internet-facing communication services, properly
tuned stacks can support millions of concurrent connections in
production~\cite{whatsapp-2mil}.  
