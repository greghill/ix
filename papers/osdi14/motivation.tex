
\section{Background and Motivation}
\label{sec:motivation}

Our work focuses on improving operating systems for applications with
aggressive networking requirements running on multi-core servers.

\subsection{Challenges for Datacenter Applications}
\label{sec:motivation:challenges}


% \christos{We need to make sure that the requirements in 2.2 and
%   approach in 3 are aligned. Currently, they don't match that well}

Large-scale, datacenter applications pose unique challenges to system
software and their networking stacks:

\myparagraph{Microsecond tail latency:} To enable rich interactions
between a large number of services without impacting the overall
latency experienced by the user, it is essential to reduce the latency
for some service requests to a few tens of
\microsecond~\cite{luiz-isscc,DBLP:conf/hotos/RumbleOSRO11}.  Because
each user request often involves hundreds of servers, we must also
consider the long tail of the latency distributions of RPC requests
across the datacenter~\cite{DBLP:journals/cacm/DeanB13}. Although
tail-tolerance is actually an end-to-end challenge, the system
software stack plays a significant role in exacerbating the
problem~\cite{DBLP:conf/eurosys/LeverichK14}. Overall, each service
node must ideally provide tight bounds on the 99th percentile request
latency.
% Protocol-processing techniques that reduce average latency, but do
% not improve 95th or 99th percentile latency are unlikely to result
% in end-to-end application gains.

\myparagraph{High packet rates:} The requests and, often times, the
replies between the various services that comprise a datacenter
application are quite small. In Facebook's \texttt{memcached} service,
for example, the vast majority of requests uses keys shorter than 50
bytes and involves values shorter than 500
bytes~\cite{Atikoglu:2012:WAL}, and each node can scale to serve millions of requests per second~\cite{DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}.

% Executive call: downgrade a little the discussion about connections
% \christos{How do we explain IX's higher number of connections?
% Better data structures?  Less locking? Right now it is not
% explained. Based on how we explain it later we should edit this,
% potentially moving it further down the list or merging it with the
% high throughput}
The high packet rate must also be sustainable under a large number of
concurrent connections and high connection
churn~\cite{theC10Mproblem}. If the system software cannot handle
large connection counts, there can be significant implications for
applications.  The large connection count between
application and \texttt{memcached} servers at Facebook made it
impractical to use TCP sockets between these two tiers, resulting in
deployments that use UDP datagrams for \texttt{get} operations and an
aggregation proxy for \texttt{put}
operations~\cite{DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}.

\myparagraph{Protection:} Since multiple services commonly share
servers in both public and private
datacenters~\cite{DBLP:journals/cacm/DeanB13,Hindman:2011:MPF,Schwarzkopf:2013:OFS},
there is need for isolation between applications. The use of
kernel-based or hypervisor-based networking stacks largely addresses
the problem. A trusted network stack can firewall applications,
enforce access control lists (ACLs), and implement limiters and other
policies based on bandwidth metering.

\myparagraph{Resource efficiency:} The load of datacenter applications
varies significantly due to diurnal patterns and spikes in user
traffic. Ideally, each service node will use the fewest resources
(cores, memory, or IOPS) needed to satisfy packet rate and tail
latency requirements at any point. The remaining server resources can
be allocated to other
applications~\cite{DBLP:conf/asplos/DelimitrouK14,Hindman:2011:MPF} or
placed into low power mode for energy
efficiency~\cite{DBLP:journals/computer/BarrosoH07}. Existing
operating systems can support such resource usage
policies~\cite{DBLP:conf/eurosys/LeverichK14,DBLP:conf/isca/LoCGBK14}.

\subsection{The Hardware -- OS Mismatch}
\label{sec:motivation:web}

The wealth of hardware resources in modern servers should allow for
low latency and high packet rates for datacenter applications.  A
typical server includes one or two processor sockets, each with eight or
more multithreaded cores and multiple, high-speed channels to DRAM and
PCIe devices. Solid-state drives and PCIe-based Flash storage are also
increasingly popular. For networking, 10 GbE NICs and switches are
widely deployed in datacenters, with 40 GbE and 100 GbE technologies
right around the corner. The combination of tens of hardware threads
and 10 GbE NICs should allow for rates of 15M packets/sec with minimum
sized packets.  We should also achieve 10--20\microsecond round-trip
latencies given 3\microsecond latency across a pair of 10 GbE NICs,
one to five switch crossings with cut-through latencies of a few
hundred ns each, and propagation delays of 500ns for 100 meters
of distance within a datacenter.


Unfortunately, commodity operating systems have been designed under
very different hardware assumptions. Kernel schedulers, networking
APIs, and network stacks have been designed under the assumptions of
multiple applications sharing a single processing core and packet
inter-arrival times being many times higher than the latency of interrupts
and system calls.  As a result, such operating systems trade off both
latency and throughput in favor of fine-grain resource scheduling.
Interrupt coalescing (used to reduce processing overheads), queuing
latency due to device driver processing intervals, the use of
intermediate buffering, and CPU scheduling delays frequently add up to
several hundred \microsecond of latency to remote requests.  The
overheads of buffering and synchronization needed to support flexible,
fine-grain scheduling of applications to cores increases CPU and
memory system overheads, which limits throughput.  As requests between
service tiers of datacenter applications often consist of small packets,
common NIC hardware optimizations, such as TCP segmentation and
receive side coalescing, have a marginal impact on packet rate.


% Modern web-scale applications include interactive services such as
% search, social networking and instant messaging, automatic
% translation, software as a service, and e-commerce platforms.
% We have come to expect that these services provide millions of users
% with instantaneous, personalized, and contextual access to petabytes
% of data.  For example, the Google search engine updates query results
% interactively as the user types, predicting the most likely query,
% performing the search and showing the results within a few tens of
% milliseconds~\cite{DBLP:journals/cacm/DeanB13}.

% Internally, such applications use a service-oriented architecture and
% consist of tens of distinct, well-defined services such as
% load-balancing and HTTP proxies, web and application serving, content
% distribution and streaming, memory caching, queuing services,
% relational databases and object
% storage~\cite{Alonso:2010:WSC,DBLP:conf/sosp/DeCandiaHJKLPSVV07,Eriksen:2013:YSF}.
% Collectively, these services occupy hundreds to thousands of servers
% connected through high-speed networking. An incoming request from an
% external user leads to tens to hundreds of internal requests across
% the various services, with some requests issued sequentially due to
% dependencies in application logic, while other requests are issued in
% a parallel, fan-out manner.  Different services are routinely
% implemented using different programming languages, but are connected
% through a unifying framework for RPC, serialization, service
% discovery, and logging~\cite{finagle, protocolbuffers, thrift} and most commonly use TCP/IP as their transport
% layer.

% Each node for these services responds to requests such as new incoming
% connections, data requests from existing connections, or replies to
% requests made to a downstream service.  The most efficient
% implementations use event-driven frameworks in which the service logic
% interacts with the framework exclusively via non-blocking
% calls~\cite{DBLP:conf/usenix/PaiDZ99,DBLP:conf/sosp/WelshCB01}.
% High-level libraries simplify the development of event-driven
% applications~\cite{libev,libuv,provos2003libevent}.  For example, the
% popular \texttt{libevent} library provides a level of abstraction that
% exposes the paradigm to Linux, *BSD, and Windows
% applications~\cite{provos2003libevent}. For example,
% \texttt{memcached} is built on top of
% \texttt{libevent}~\cite{url:memcached}, while \texttt{nginx} and
% \texttt{node.js} are built using similar frameworks.



\subsection{Alternative Approaches}
\label{sec:motivation:current}

%\christos{need another pass below to state more clearly which
 % reqs get better and which worse for each alternative}

Since the network stacks within commodity kernels cannot take
advantage of the abundance of hardware resources, a number of
alternative approaches have been suggested. Each alternative addresses
a subset, but not all, of the requirements for datacenter applications.

\myparagraph{User-space networking stacks:} Systems such as
OpenOnload~\cite{openonload}, mTCP~\cite{jeong2014mtcp}, and
Sandstorm~\cite{sandstorm} run the entire networking stack in
user-space in order to eliminate kernel crossing overheads and
optimize packet processing without incurring the complexity of kernel
modifications. However, there are still tradeoffs between packet rate
and latency. For instance, mTCP uses dedicated threads for the TCP
stack, which communicate at relatively coarse granularity with application
threads. This aggressive batching amortizes switching overheads at the expense of higher
latency (see \S\ref{sec:eval}). It also complicates resource sharing
as the network stack must use a large number of hardware threads
regardless of the actual load. More importantly, security tradeoffs emerge
when networking is lifted into the user-space and application bugs can
corrupt the networking stack. For example, an attacker may be able
transmit raw packets (a capability that normally requires root
privileges) to exploit weaknesses in network protocols and impact
other services~\cite{DBLP:conf/acsac/Bellovin04}. It is difficult to
enforce any security or metering policies beyond what is directly
supported by the NIC hardware.

\myparagraph{Alternatives to TCP:} In addition to kernel bypass, some
low-latency object stores rely on RDMA to offload protocol processing
on dedicated Infiniband host channel
adapters~\cite{dragojevic14farm,DBLP:conf/icpp/JoseSLZHWIOWSP11,mitchell:rdma,DBLP:conf/sosp/OngaroRSOR11}.
RDMA can reduce latency, but requires that specialized adapters be present at both ends of the connection.
Using commodity Ethernet
networking, Facebook's memcached deployment uses UDP to avoid
connection scalability
limitations~\cite{DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}. Even though
UDP is running in the kernel, reliable communication and congestion
management are entrusted to applications.

\myparagraph{Alternatives to POSIX API:} MegaPipe replaces the POSIX
API with lightweight sockets implemented with in-memory command
rings~\cite{DBLP:conf/osdi/HanMCR12}. This reduces some
software overheads and increases packet rates, but retains all other
challenges of using an existing, kernel-based networking stack.

\myparagraph{OS enhancements:} Tuning kernel-based stacks provides
incremental benefits with superior ease of deployment.  Linux
\texttt{SO\_REUSEPORT} allows multi-threaded applications to accept
incoming connections in parallel. Affinity-accept reduces overheads by
ensuring all processing for a network flow is affinitized to the same
core~\cite{DBLP:conf/eurosys/PesterevSZM12}. Recent Linux Kernels
support a busy polling driver mode that trades increased CPU utilization
for reduced latency~\cite{intel-busypoll}, but it is not yet compatible with
\texttt{epoll}.
%These approaches cannot
%completely eliminate synchronization between cores, as the lack of
%commutativity in the POSIX socket API implies that cross-core
%communication is needed when connections are accepted or
%closed~\cite{DBLP:conf/sosp/ClementsKZMK13}. 
When microsecond latencies are
irrelevant, properly tuned stacks can maintain millions of open
connections~\cite{whatsapp-2mil}.

