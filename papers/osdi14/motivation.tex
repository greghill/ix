
\section{Background and Motivation}
\label{sec:motivation}

Our work focuses on identifying the mismatch and opportunities for
improvement between commodity operating systems and a large, but
well-defined, class of relevant applications.
%  In this section, we
% first define the class of applications in our focus
% (\S\ref{sec:motivation:web}), then describe the system software
% challenges associated with that class of applications
% (\S\ref{sec:motivation:challenges}), and identify an alternative model
% -- the dataplane -- which addresses these challenges
% (\S\ref{sec:motivation:dp}.

\subsection{Event-driven, Web-scale Applications}
\label{sec:motivation:web}

Modern web-scale applications include interactive services such as
search, social networking, webmail and instant messaging, online maps,
automatic translation, software as a service (SaaS), and e-commerce
platforms.  We have come to expect that these services provide
millions of users with instantaneous, personalized, and contextual
access to petabytes of data.  For example, the Google search engine
updates query results interactively as the user types, predicting the
most likely query, performing the search and showing the results
within a few tens of milliseconds~\cite{DBLP:journals/cacm/DeanB13}.

Internally, such applications use a service-oriented architecture and
consist of tens of distinct, well-defined services such as
load-balancing and http proxies, web and application serving, content
distribution and streaming, memory caching, queuing services,
relational databases and object
storage~\cite{Alonso:2010:WSC,Eriksen:2013:YSF}. \christos{what is the
  authoritative ref for SOA?} Collectively, these services occupy
hundreds to thousands of servers connected through high-speed
networking. Different services are routinely implemented using
different programming languages and paradigms, but are connected
through a unifying framework for RPC, serialization, service
discovery, and logging \cite{protocolbuffers, thrift, fingale,
  others}. An incoming request from an external user leads to tens to
hundreds of internal requests across the various services, with some
requests issued sequentially due to dependencies in application logic,
while other requests are issued in a parallel, fan-out manner.
% In addition, web-scale applications also rely on massive,
% batch-oriented services that prepare the data for interactive serving
% (e.g., the search engine crawlers), identify patterns and behavior
% from the massive data sets and the users' behaviors via machine
% learning~\cite{missing}.

Each node for these services responds to external requests such as new
incoming connections, data requests from existing connections, or
replies to requests made to a downstream service.  The most efficient
implementations today use event-driven framworks in which the service
logic interacts with the framework exclusively via non-blocking
calls. This is in contrast with the classic thread-oriented
programming paradigm, in which sessions are divided among a large pool
of (kernel) threads, which block while waiting for incoming traffic or
outstanding replies. For example, the event-oriented nginx HTTP server
and proxy~\cite{reese2008nginx} outperforms the thread-oriented Apache
server~\cite{misc:apache}.

High-level libraries simplify the development of event-driven
applications~\cite{provos2003libevent,libev,libuv}.  For example, the
popular \texttt{libevent} library~\cite{provos2003libevent} provides a
level of abstraction that exposes the paradigm to applications running
on top of Linux, *BSD, and Windows.  Many web-scale applications, such
as memcached~\cite{missing}, are built on top of libevent.  Others
such as Nginx and node.js are built using similar frameworks.


\subsection{Main Requirements}
\label{sec:motivation:challenges}

Web-scale applications pose unique challenges to system software.
Although the use of an event-oriented paradigm has system-level
benefits, in particular with respect to context switching rates, many
challenges remain that must be addressed, in particular:

\paragraph{High packet rates.}

The typical traffic between tiers of a web-scale application consists
of distinct procedure calls.  The requests are typically small; for
example, many applications use small (e.g. 16 byte) memcached keys.
The replies are also often small, typically a few
kilobytes~\cite{missing}.  As a result, web-scale servers must ideally
scale to extremely high packet rates, measured in millions of packets
per second on a 10 GbE interface..  In such conditions, the standard
optimizations found in conventional hardware such as TCP send offload
and large-scale receive offloading, have a marginal impact on
performance.

\paragraph{Micro-second computing.}

The notion behind \emph{micro-second computing}~\cite{luiz-isscc} is
that it is essential to minimize the latency of interactions within a
datacenter, and in particular of the remote procedure calls issued by
interactive applications that must access and aggregate massive
amounts of data.  Using today's networking technologies, the typical
communication delay between two servers in a datacenter involves a
pair of 10GE NICs (with a one-way latency as low as
3~\microsecond~\cite{cisco-sereno}), one, three or five cut-through
switches organized in a fat-tree topology (a few hundred $ns$ each, in
the absence of contention, and rapidly shrinking) and propagation
delays (500 $ns$ for 100 meters, with little recent improvements).
Put together, technology enables intrinsic one-way communication
latency of a handful of \microsecond.  

And yet, software ---and in particular system software--- is designed
to operate at a totally different timescale: interrupt coalescing
improve throughput but can delay packets by milliseconds; queues build
up during device driver processing intervals, introducing noticeable
queuing delays; protocol processing of incoming packets terminates in
the socket layer without necessarily immediately scheduling pending
applications. As a result, the remote procedure call latency observed
by application software is one to two orders of magnitude greater than
the intrinsic latency.  For example, we measure the average NetPIPE
latency between two vanilla Linux servers with Intel XXX NICs at
XXX\microsecond, when in fact such communication is possible in only
XXX\microsecond (see \S\ref{missing}).

These issues compound when considering the long tail of the latency
distributions of remote procedure
calls~\cite{DBLP:journals/cacm/DeanB13} . Although tail-tolerance
itself is actually an end-to-end problem, the protocol-processing
stacks play a significant role in exacerbating the problem.  In
particular, protocol-processing techniques that reduce latency to
microseconds in the common case, but don't improve the 95\% or 99\%
latency are unlikely to result into end-to-end application gains.

\paragraph{Connection scalability.}

Connection scalability is the desirable property to efficiently
support both a large number of concurrent connections, as well as high
churn in these connections.  Ideally, an operating system would
support a number of concurrent connections limited only by the
server's DRAM.  Furthermore, the operating system would feed
applications with the data stream with the same throughput and
latency, independently of the number of concurrent connections, or the
rate of churn.
 
This has been a historical challenge for commodity operating systems.
A decade ago, connection scalability challenges emerged with less than
10,000 concurrent connections~\cite{theC10Kproblem}, which ended up
requiring changes to the kernel and to applications.  Connection
scalability remains a challenge today~\cite{theC10Mproblem}.  For
example, the nature of all-to-all communication between Facebook's
application and memcache servers made it impractical to use TCP
sockets between these two tiers~\cite{nishtala2013scaling}; resulting
deployments use a UDP-based protocol for \texttt{get} operations, and
an aggregation proxy for \texttt{put} operations.


\paragraph{Resource elasticity and proportionality.}

\edb{Christos should redo}. Datacenters for web-scale applications are
essentially built today using thousands of commodity Xeon-class
processors that draw hundreds of Watts each.  The dimension of each
building block -- number of sockets, cores, DRAM, disk capacity, IOPS
and bandwidth, and networking bandwidth -- are largely determined by
architectural trends.  Ideally, applications would only consume
resources (incl. energy) that are proportional to their current load.
Unfortunately, mapping such applications efficiently onto the
scale-out model remains an open challenge~\cite{missing} as it requires
a combination of cluster-wide, and machine-local resource management,
workload placement and rebalancing techniques~\cite{missing,missing}.


\paragraph{Security and protection.}

Finally, web-scale applications, as they are deployed today, depend on
the ubiquitous presence of protected, isolated network protocol
stacks used for communication.  For single-tenant deployments where
all applications and infrastructure are administered by the same
organization, the combination of commodity operating systems such as
Linux and standard TCP-based protocols largely addresses the problem.
Challenges emerge when the networking stack is lifted from the
protected operating system and into user-space, and application bugs
can corrupt the networking stack and impact the rest of the
infrastructure.


\subsection{Current Approaches}
\label{sec:motivation:current}

\christos{If we leave this here, it is essentially related work. It
  needs to be beefed up and will overlap with the one at the end. I
  suggest we inline the critique to current approaches in the two
  previous section. In 2.1, we add a paragrah that lists the main
  approarch. Then as we review requirements, we inline in each
  paragraph how some current approaches fail it. }

The conventional wisdom is that these challenges are rooted in a
mismatch between existing commodity operating systems, existing
network protocols, and the unique requirements of web-scale
applications.  Indeed, with today's technology, commodity Xeon
processors support 8-10 cores with dedicated L1 caches, between 16-40
hyperthreads, up to 50 Gbps of PCIe bandwidth \christos{Intel E5 chips
  have 24GBytes/sec PCIe bandwidth per socket}, and NIC that transfer
frames in a few \microsecond.  Unfortunately, the distinct nature of
web-scale applications prevents them from saturating the hardware.
Different approaches have been proposed to address each a subset of
these challenges:

\paragraph{Running the networking stack in user-space.}  

Approaches such as OpenOnload~\cite{openonload} or
mTCP~\cite{jeong2014mtcp} run the entire networking stack in
userspace, which are designed to offer load-latency communication
(e.g. OpenOnload) or connection scalability (e.g. mTCP), at the
expense of protection.  mTCP's design has the TCP stack running on
dedicated hyperthreads which synchronizes at relatively coarse grain
with the application. Although tail latency can be bounded, the focus
is measured at ms-scale rather than \microsecond-scale.

\paragraph{Alternatives to TCP.}

Low-latency key-value stores such as
RAMcloud~\cite{DBLP:conf/sosp/OngaroRSOR11} rely on kernel-bypass and
RDMA to offload all protocol processing on dedicated Infiniband Host Channel
Adapters.  Using standard hardware and networking fabrics, Facebook's
deployment of memcached uses UDP to avoid any connection scalability
limitations.  Even though UDP itself is running in the kernel, the
responsibilities of congestion management and flow control and
entrusted into applications.

\paragraph{Alternatives to POSIX API.}

MegaPipe~\cite{han2012megapipe} replaces the POSIX API with
lightweight sockets implement by in-memory command rings.  This
substantially reduces software overheads and substantially increases
packet rates.  The pragmatic use of the operating system's TCP stack
and interrupt model presents any microsecond-computing solution.

\paragraph{Enhancements to the OS.}

Operating system modifications tradeoff superior ease of deployability
(and quite frankly, pragmatism in terms of production use) even though
the improvements might be more incremental.
Affinity-Accept~\cite{DBLP:conf/eurosys/PesterevSZM12} improves the
affinity of incoming and outgoing network processing by considering
the affinity of flows to core and controlling the NIC hardware.  Linux
\texttt{SO\_REUSE} allows multi-threaded applications to accept
incoming connections in parallel.  Although such approaches minimize
communication between core, they do not eliminate them.  The lack of
commutativity~\cite{DBLP:conf/sosp/ClementsKZMK13} in the POSIX socket
API implies that cross-core communication is necessary whenever a
connection is accepted or closed.  When microsecond latencies are
irrelevant, such as Internet-facing communication services, properly
tuned stacks can support millions of concurrent connections in
production~\cite{whatsapp-2mil}.  
