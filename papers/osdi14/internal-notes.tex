
\section{Internal notes}


\subsection{ CUT from Intro}



In addition, such applications must scale in terms of bandwidth and
concurrent connection counts, and ideally solve the \emph{C10M
  problem}~\cite{theC10Mproblem}: for a given server, scale to 10
million concurrent TCP connections, saturate 10 GbE interfaces, and
deliver 10~\microsecond latency (mean), with an additional latency
jitter of not more than an extra 10~\microsecond.  Today, million+
concurrent connections are desirable in Internet-facing notification
servers~\cite{DBLP:conf/sosp/AdyaCMP11} or messaging
servers~\cite{whatsapp-2mil}. 



\subsection{Stuff to cite somehow}
\begin{itemize}


\item DPDK
\item IXGBE
\item some kind of user-mode zero-copy network driver, but I think it is proprietary. \url{http://www.ntop.org/products/pf_ring/}
\item same for FreeBSD \url{https://www.usenix.org/system/files/conference/atc12/atc12-final186.pdf}
\item mTCP - highly scalable userspace multicore TCP kernel~\cite{jeong2014mtcp}.  Site at \url{http://shader.kaist.edu/mtcp/}
\item Megapipe~\cite{han2012megapipe},
\item Affinity-accept~\cite{DBLP:conf/eurosys/PesterevSZM12}
\item Netmap~\cite{rizzo2012netmap}.
\item SO\_RESUEPORT (socket option).  
\item What's App 2milion socket post~\cite{whatsapp-2mil}
\item Dune~\cite{belay2012dune} ...
\end{itemize}

\subsection{Names and Concepts of IX}

\begin{itemize}
\item {\bf cpu}. A hardware thread, labelled x/y with x the core and y the HT
\end{itemize}


\subsection{To put in related work}

\paragraph*{Exokernel}   Exokernels~\cite{DBLP:conf/sosp/EnglerKO95} safely expose hardware to library operating systems.  We propose


\paragraph*{Akkaris}.  
Akkaris~\cite{peter2013arrakis} is the closest system.  It is
apparently built on the barrelfish multikernel~\cite{DBLP:conf/sosp/BaumannBDHIPRSS09} and uses VT-x
like Dune~\cite{belay2012dune}.

\paragraph*{Megapipe} 
Megapipe~\cite{han2012megapipe} proposes a high-performance socket
alternative for TCP/IP based applications for scalable network
applications.  Our API is inspired by it. This is similar to
Netmap~\cite{rizzo2012netmap}, which also ping shared buffers, but is
focused on libpcap applications.


\paragraph{mTCP}

MTCP is a user-level stakc (NSDI 2012) that outperform Megapipe and
Linux (w/ SO\_REUSEPORT) by large factors.  The design calls for a
userlevel TCP stack that runs in the same address space as the
application. The threading model calls for a 1:1 correspondance
between application threads and TCP threads, ideally running on the
two different hyperthreads of the same core.  This gives TCP threads
to run timers with high precision.  To offset the high cost of IPC,
mTCP relies on extensive batching of commands before waking the
distinct thread.  There is no protection between application and TCP
thread; a malicious (or owned) application can easily corrupt the TCP
protocol blocks and header space.  mTCP is built on top of the PSIO
engin, which provides an event-driven packet IO interface.

In constast, IX runs the TCP stack (in kernel model) and the
application on the same threads of control; an IX application can
therefore take advantage of all hyperthreads of the CPU (instead of
only half).  Furthermore, IX runs without any locks, lock-free data
structures, or atomic instructions, whereas mTCP relies on lock-free
data structures (which themsevles require atomic instructions).

mTCP does not expose the dual-thread model directly to
applications. Instead, it hides it behind a nearly-identical POSIX
variant (epoll --> mepoll).  IX exposes an event-driven API directly
to applications, via the libevent framework.

MTCP has some great design ideas that IX could/should leverage:

\begin{itemize}

\item Efficient timer management for high-socket count.  One bucket
  per remaining milisecond for near-term events, and an ordered list
  for longer-term events (TCP cleanup events).

\item They claim optimization for
short-lived connections, including the prioritization of SYN and ACK
packets on TX (which seems like a hack)

\end{itemize}

The evaluation of MTCP includes micro-benchmarks as well as basic
applications (ab + lighttpd, SSLshader).  Fig 7 shows the connection
setup/taredown throughput based on the number of cores, showing linear
speedup up to the measured 8 cores.  At 8 cores, they can setup, send
a 64B message and close a connection 150,000 times per second, using 8
cores (i.e. 16 threads).  They can actually accept/close 300,000
connections per second (Fig 8).

SSLsharer runs with up to 16K connections; they measure median and 95
percentile latency for response time.

They use the Jain index on a benchmark that downloads a 10MiB file
repeatedely across 8K connections to measure fairness.  8K is a small
number of connections.  Table 4 looks at the connection count and
thrhougput of WebReplay across 4x10GbE connections.  The workload
creates up to 15K connections per second and has up to 25k concurrent
connection (the text also mentions having up to 270,000 conurrent
connections).




\paragraph*{Kernel-bypass approaches}  Solarflare Openonload~\cite{openonload}, ...

\paragraph*{Containers}  Linux containers, BSD jail, ...


\paragraph*{Application-specific kernels}  
Exokernels had a bunch of them.
SplashOS~\cite{DBLP:journals/tocs/BugnionDGR97} proposed this in the
context of hypervisors (Disco).
Libra~\cite{DBLP:conf/vee/AmmonsABSGKKRHW07} was a JVM that ran
directly on top of Xen.  It Relied on a separate Linux VM to provide
networking and storage (like Azul).  The
Unikernel~\cite{DBLP:conf/asplos/MadhavapeddyMRSSGSHC13} (ASPLOS 13)
lays out the case for BaremetalOS because it lays out the case for
single app operating systems.  The Unikernel ported the Ocaml runtime
to run on top of Xen hypervisor (like libra).  Actually makes a case
for reduced latency jitter, at least in terms of thread scheduling -
I/O performance basically sucks though because still not taking
advantage of HW (uses Xen PV-NIC).

\section{Techniques and Concepts}

\paragraph*{The C10M problem} As stated by Robert Graham(?)~\cite{theC10Mproblem}.


\paragraph*{Scheduler activations} 
If apps are able to yield cores, and if apps are able to
asynchronously take advantage of new cores, scheduler
activations~\cite{DBLP:journals/tocs/AndersonBLL92} seem like a
perfect mechanism to make those cores available to other processes. 
The idea never really made it into POSIX, but we can use here because the API is different. 

\paragraph*{Ballooning}
Ballooing was introduced for memory
management~\cite{DBLP:conf/osdi/Waldspurger02}.  The balloon keeps
track of the amount of relinquished resources.  CPU ballooning could
be used to (i) start a max core pool, and (ii) dynamically resize it
according to the SLA and the global load.

\paragraph*{Virtual memory performance}  
Direct segment mapping~\cite{DBLP:conf/isca/BasuGCHS13} proposes to
replace the traditional TLB with a dedicated direct segment.
Drepper~\cite{DBLP:journals/queue/Drepper08} measured additional cost
of running workloads in virtualized domains due to two-dimensional
page walking.

\paragraph*{Latency distribution}  
Dean and Barroso~\cite{DBLP:journals/cacm/DeanB13} describe the
problems associated with tail latency in web-scale services.
Dynamo~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07} talked about the
importance to evaluate key-value stores based on their 99.9\% latency.


\paragraph*{Commutativity and scalability} 
The scalable commutativy rule~\cite{DBLP:conf/sosp/ClementsKZMK13}
states that only commutative APIs can be implemented in a truly
scalable manner; our API and our implementation should support that.
RadixVM~\cite{DBLP:conf/eurosys/ClementsKZ13} provides scalable
address spaces for multithreaded applications; we address this as well.

\paragraph{The case against user-level networking}
Margo's original paper (same title)~\cite{magoutis2004case}

\paragraph*{Latency measurements}  

\section{Workloads}

\paragraph*{Memcached} 
Scaling memcached at Facebook~\cite{nishtala2013scaling} required
unnatural tricks such as relying on UDP and on in-network connection
proxies to compensate for the lack of scoket scalability of Linux.

Memcached is so important that folks are prototyping it in
FPGA~\cite{DBLP:conf/fpga/ChalamalasettiLWARM13}.  The authors's main evaluation is
that they can get comparable performance to an industry server with
10x the Performance/Watt, measured in KOPS/S/Watt (Table 3. memcached
``gets'' with 128B size data). To beat their numbers, we need to beat
30 Kops/sec/W to win.  Can we?  The comparison is a case of hardware
specialization vs. software specialization (of the OS).



\paragraph*{ngnx}
