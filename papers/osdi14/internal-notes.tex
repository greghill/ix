
\section{Internal notes}


\subsection{ CUT from Intro}



In addition, such applications must scale in terms of bandwidth and
concurrent connection counts, and ideally solve the \emph{C10M
  problem}~\cite{theC10Mproblem}: for a given server, scale to 10
million concurrent TCP connections, saturate 10 GbE interfaces, and
deliver 10~\microsecond latency (mean), with an additional latency
jitter of not more than an extra 10~\microsecond.  Today, million+
concurrent connections are desirable in Internet-facing notification
servers~\cite{DBLP:conf/sosp/AdyaCMP11} or messaging
servers~\cite{whatsapp-2mil}. 



\begin{comment}

%%% V1

We focus here on three high-level challenges for web-scale
applications that point to the need to address energy-proportionality,
connection scalability, and quality-of-service for web-scale
applications.

First, at the highest level, web-scale applications ideally deliver
the highest possible quality of service in the most
energy-proportional manner~\cite{DBLP:journals/computer/BarrosoH07}.
In practice, these two goals are often at odds with each other:
techniques that can improve quality-of-service, and in particular
reduce end-to-end latency, often consume much more energy.  For
example, low-power states save substantial amounts of energy, at the
cost of an increased latency measured in XXX when transitioning back
into a normal operating system.

Second, the nodes of web-scale applications are often characterized by
extremely high fan-out and high fan-in requirements.  An application
server may rely on tens of thousands of downstream services, and a
downstream service may serve hundreds of thousands of
clients~\cite{missing}.  At the extreme end of the spectrum,
notification and communication services must scale to the hundreds of
millions of Internet clients~\cite{DBLP:conf/sosp/AdyaCMP11}.
Unfortunately, existing implementations of the TCP/IP networking stack
in current systems were never designed to scale to such sizes.  As a
consequence, Facebook pragmatically chose to forgo the TCP protocol in
favor of connection-less UDP datagrams~\cite{nishtala2013scaling} for
its memcached implementation.  WhatsApp put significant engineering
efforts to tun FreeBSD to scale to 2 million concurrent
connections~\cite{whatsapp-2mil}.

Third, these high fan-in and fan-out applications suffer from the
\emph{long tail} problem, with the latency of the application
determined by the latency of slowest-responding
node~\cite{DBLP:journals/cacm/DeanB13}.  To lessen the impact of the
long tail, datacenter operators often overprovision servers to ensure
that they can operate at low utilization.

Collectively, the second and third challenge have been proposed as the
\emph{C10M problem}~\cite{theC10Mproblem}: for a given server, scale
to 10 million concurrent TCP connections, saturate 10 GbE interfaces,
and deliver 10~\microsecond latency (mean), with an additional latency
jitter of not more than an extra 10~\microsecond.  The conventional
wisdom is that such a solution must bypass the operating system
entirely.  For example, mTCP~\cite{jeong2014mtcp} is a user-level TCP
stack with the highest known scalability for short-lived connections.

\end{comment}



\subsection{XXX LEGACY - The Dataplane Alternative}
\label{sec:motivation:dp}


The solution involves potential tradeoffs in both hardware and
software.  On the hardware side, middlebox services can be deployed
either on commodity Xeon-class servers or virtual machines, or on
processors specifically designed for the networking market.  For
example, Cavium's OCTEON III family of processors offers multiple
100Gbps of external connectivity and 48 64-bit MIPS cores into a
single System-on-Chip~\cite{cavium-octeon}.  This provides increases
energy-efficiencies and densities, at the expense of generality and
volume of the solution.  

On the software side, the typical architecture of a middlebox solution
is designed around the following principles: 

\paragraph{Run to completion.}  Packets are processed ``to
completion'' by the dataplane, eliminating most or all forms of
context switching, system calls, and internal queuing.  In contrast,
the classic kernel construction model generally executes the device
driver code as the result of an interrupt, requires a separate system
call for each segment of a flow received or sent.  The MegaPipe
project~\cite{han2012megapipe} eliminates this latter bottleneck, with
significant benefits.  Furthermore, the POSIX model divides the roles
of the protocol processing stack (which pulls packets from the driver,
de-multiplex them into socket queues, and implements the L3/L4
protocols) and the applications, which asynchronously pull the socket
payload from these queues.  Processing is similar on the transmission
side; in fact, both directions typically require a memory copy.

\paragraph{Coherence-free execution.} 
Different cores operate totally independently
of each other.  In particular common case operations should execute in
a coherence-free manner, i.e., without any cache misses due to the
cache-coherency protocol or any atomic operations that require a
similar access to the cache-coherency layered. This is a much stronger
form of decoupling that the use of locks or of wait-free
synchronization techniques, which have an inherent cache-coherency
traffic cost; 

\paragraph{Embeded deployment model.} 

The software stack is designed to run a single
application, which is directly embedded within the dataplane.  Like
most embedded systems, the application controls all resources, and the
software stack is deployed and operated as a single blob with no
externally-visible distinction between the components.




\subsection{Misc. - TODO.}

Connection scalability is also a challenge for Internet-facing servers
such as notification services~\cite{DBLP:conf/sosp/AdyaCMP11} and
instant-messaging services.  Indeed, such servers should ideally
support much larger connection counts that for intra-datacenter
communication, as connection capacity becomes the primary metric, and
low-latency and even throughput become secondary.  For example,
WhatsApp Inc. (now part of Facebook) supports up to 2 million
concurrent connections on FreeBSD, as of 2012~\cite{whatsapp-2mil}.





\subsection{Stuff to cite somehow}
\begin{itemize}


\item DPDK
\item IXGBE
\item some kind of user-mode zero-copy network driver, but I think it is proprietary. \url{http://www.ntop.org/products/pf_ring/}
\item same for FreeBSD \url{https://www.usenix.org/system/files/conference/atc12/atc12-final186.pdf}
\item mTCP - highly scalable userspace multicore TCP kernel~\cite{jeong2014mtcp}.  Site at \url{http://shader.kaist.edu/mtcp/}
\item Megapipe~\cite{han2012megapipe},
\item Affinity-accept~\cite{DBLP:conf/eurosys/PesterevSZM12}
\item Netmap~\cite{rizzo2012netmap}.
\item SO\_RESUEPORT (socket option).  
\item What's App 2milion socket post~\cite{whatsapp-2mil}
\item Dune~\cite{belay2012dune} ...
\end{itemize}

\subsection{Names and Concepts of IX}

\begin{itemize}
\item {\bf cpu}. A hardware thread, labelled x/y with x the core and y the HT
\end{itemize}


\subsection{To put in related work}

\paragraph*{Exokernel}   Exokernels~\cite{DBLP:conf/sosp/EnglerKO95} safely expose hardware to library operating systems.  We propose


\paragraph*{Akkaris}.  
Akkaris~\cite{peter2013arrakis} is the closest system.  It is
apparently built on the barrelfish multikernel~\cite{DBLP:conf/sosp/BaumannBDHIPRSS09} and uses VT-x
like Dune~\cite{belay2012dune}.

\paragraph*{Megapipe} 
Megapipe~\cite{han2012megapipe} proposes a high-performance socket
alternative for TCP/IP based applications for scalable network
applications.  Our API is inspired by it. This is similar to
Netmap~\cite{rizzo2012netmap}, which also ping shared buffers, but is
focused on libpcap applications.


\paragraph{mTCP}

MTCP is a user-level stakc (NSDI 2012) that outperform Megapipe and
Linux (w/ SO\_REUSEPORT) by large factors.  The design calls for a
userlevel TCP stack that runs in the same address space as the
application. The threading model calls for a 1:1 correspondance
between application threads and TCP threads, ideally running on the
two different hyperthreads of the same core.  This gives TCP threads
to run timers with high precision.  To offset the high cost of IPC,
mTCP relies on extensive batching of commands before waking the
distinct thread.  There is no protection between application and TCP
thread; a malicious (or owned) application can easily corrupt the TCP
protocol blocks and header space.  mTCP is built on top of the PSIO
engin, which provides an event-driven packet IO interface.

In constast, IX runs the TCP stack (in kernel model) and the
application on the same threads of control; an IX application can
therefore take advantage of all hyperthreads of the CPU (instead of
only half).  Furthermore, IX runs without any locks, lock-free data
structures, or atomic instructions, whereas mTCP relies on lock-free
data structures (which themsevles require atomic instructions).

mTCP does not expose the dual-thread model directly to
applications. Instead, it hides it behind a nearly-identical POSIX
variant (epoll --> mepoll).  IX exposes an event-driven API directly
to applications, via the libevent framework.

MTCP has some great design ideas that IX could/should leverage:

\begin{itemize}

\item Efficient timer management for high-socket count.  One bucket
  per remaining milisecond for near-term events, and an ordered list
  for longer-term events (TCP cleanup events).

\item They claim optimization for
short-lived connections, including the prioritization of SYN and ACK
packets on TX (which seems like a hack)

\end{itemize}

The evaluation of MTCP includes micro-benchmarks as well as basic
applications (ab + lighttpd, SSLshader).  Fig 7 shows the connection
setup/taredown throughput based on the number of cores, showing linear
speedup up to the measured 8 cores.  At 8 cores, they can setup, send
a 64B message and close a connection 150,000 times per second, using 8
cores (i.e. 16 threads).  They can actually accept/close 300,000
connections per second (Fig 8).

SSLsharer runs with up to 16K connections; they measure median and 95
percentile latency for response time.

They use the Jain index on a benchmark that downloads a 10MiB file
repeatedely across 8K connections to measure fairness.  8K is a small
number of connections.  Table 4 looks at the connection count and
thrhougput of WebReplay across 4x10GbE connections.  The workload
creates up to 15K connections per second and has up to 25k concurrent
connection (the text also mentions having up to 270,000 conurrent
connections).




\paragraph*{Kernel-bypass approaches}  Solarflare Openonload~\cite{openonload}, ...

\paragraph*{Containers}  Linux containers, BSD jail, ...


\paragraph*{Application-specific kernels}  
Exokernels had a bunch of them.
SplashOS~\cite{DBLP:journals/tocs/BugnionDGR97} proposed this in the
context of hypervisors (Disco).
Libra~\cite{DBLP:conf/vee/AmmonsABSGKKRHW07} was a JVM that ran
directly on top of Xen.  It Relied on a separate Linux VM to provide
networking and storage (like Azul).  The
Unikernel~\cite{DBLP:conf/asplos/MadhavapeddyMRSSGSHC13} (ASPLOS 13)
lays out the case for BaremetalOS because it lays out the case for
single app operating systems.  The Unikernel ported the Ocaml runtime
to run on top of Xen hypervisor (like libra).  Actually makes a case
for reduced latency jitter, at least in terms of thread scheduling -
I/O performance basically sucks though because still not taking
advantage of HW (uses Xen PV-NIC).

\section{Techniques and Concepts}

\paragraph*{The C10M problem} As stated by Robert Graham(?)~\cite{theC10Mproblem}.


\paragraph*{Scheduler activations} 
If apps are able to yield cores, and if apps are able to
asynchronously take advantage of new cores, scheduler
activations~\cite{DBLP:journals/tocs/AndersonBLL92} seem like a
perfect mechanism to make those cores available to other processes. 
The idea never really made it into POSIX, but we can use here because the API is different. 

\paragraph*{Ballooning}
Ballooing was introduced for memory
management~\cite{DBLP:conf/osdi/Waldspurger02}.  The balloon keeps
track of the amount of relinquished resources.  CPU ballooning could
be used to (i) start a max core pool, and (ii) dynamically resize it
according to the SLA and the global load.

\paragraph*{Virtual memory performance}  
Direct segment mapping~\cite{DBLP:conf/isca/BasuGCHS13} proposes to
replace the traditional TLB with a dedicated direct segment.
Drepper~\cite{DBLP:journals/queue/Drepper08} measured additional cost
of running workloads in virtualized domains due to two-dimensional
page walking.

\paragraph*{Latency distribution}  
Dean and Barroso~\cite{DBLP:journals/cacm/DeanB13} describe the
problems associated with tail latency in web-scale services.
Dynamo~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07} talked about the
importance to evaluate key-value stores based on their 99.9\% latency.


\paragraph*{Commutativity and scalability} 
The scalable commutativy rule~\cite{DBLP:conf/sosp/ClementsKZMK13}
states that only commutative APIs can be implemented in a truly
scalable manner; our API and our implementation should support that.
RadixVM~\cite{DBLP:conf/eurosys/ClementsKZ13} provides scalable
address spaces for multithreaded applications; we address this as well.

\paragraph*{Congestion management}
Network congestion is a key cause of poor tail-latency performance in
datacenter networks~\cite{DBLP:conf/sigcomm/AlizadehGMPPPSS10}.
DCTCP~\cite{DBLP:conf/sigcomm/AlizadehGMPPPSS10} relies on in-network
ECN support to eliminate up to 90\% of buffering in the network.
\ix's approach to packet buffering effectively extends the network
routing latyer to an extra hop on each end-point.  That extra hop could
be used to signal congestion back to the source by marking the ECN bit
before processing the packet in \ix's TCP stack.



\paragraph{The case against user-level networking}
Margo's original paper (same title)~\cite{magoutis2004case}

\paragraph*{Latency measurements}  

\section{Workloads}

\paragraph*{Memcached} 
Scaling memcached at Facebook~\cite{nishtala2013scaling} required
unnatural tricks such as relying on UDP and on in-network connection
proxies to compensate for the lack of scoket scalability of Linux.

Memcached is so important that folks are prototyping it in
FPGA~\cite{DBLP:conf/fpga/ChalamalasettiLWARM13}.  The authors's main evaluation is
that they can get comparable performance to an industry server with
10x the Performance/Watt, measured in KOPS/S/Watt (Table 3. memcached
``gets'' with 128B size data). To beat their numbers, we need to beat
30 Kops/sec/W to win.  Can we?  The comparison is a case of hardware
specialization vs. software specialization (of the OS).



\paragraph*{ngnx}
