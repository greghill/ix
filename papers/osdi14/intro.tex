
\section{Introduction}
\label{sec:intro}

%\edb{Not updated in a while....}

\christos{State of the world: bring up apps from search to social nets
to ecommerce. Large-scale and centered around networking}
Web-scale applications are redefining the requirements of system
software.  A single application can consist of hundreds of software
components deployed on tens of thousands of machines per datacenter,
and on multiple datacenters.  For example, Amazon routinely accesses
150 distinct services to render a single
page~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07}.  

\christos{requirements paragraph: high packet rate, low tail latency,
  high connection count and connection churn, elasticity,
  protection. We should list them all even if we don't do much with
  some in this paper (e.g., elasticity)}

\christos{1-2 paragraphs on how mainstream approaches fail many 
of these requirements. A trimmed down version of what is shown below. }
At massive scale, these interactions constrain applications.  For
example, latency considerations force Facebook to restrict the number
of sequential data accesses to fewer than 150 per rendered web
page~\cite{rumble2011s}.  Also at Facebook, connection scalability
limitations have led the deployment of a UDP-based memcached service
tier~\cite{nishtala2013scaling} for \texttt{get} operations, which
therefore forgoes all of the benefits of the standard TCP-based model.
To ensure the reliable application of \texttt{put} operations, the
system relies on in-network TCP-proxies to aggregate requests, which
introduces complexity and latency in the architecture.

In a recent keynote, \edb{too personal?}Google's Luiz Barroso identified three additional challenges
for applications that must process large amounts of data in little
time: energy-proportionality, tail-tolerance, and microsecond
computing~\cite{luiz-isscc}.
Energy-proportionality~\cite{DBLP:journals/computer/BarrosoH07}
requires systems to perform with a high degree of energy-efficiency,
typically measured in transactions / Watt, across all typical levels
of load on the system, and not only at saturation. Tail
tolerance~\cite{DBLP:journals/cacm/DeanB13} describes the emerging
systems-building discipline that aims to deliver predictable response
times in a distributed system.  Finally, the microsecond computing
challenge describes the need to minimize the latency of interactions
within a datacenter: even though existing transmission and switching
technologies allow for microsecond-level messaging between any two
nodes in a datacenter, a combination of protocol processing, interrupt
delivery, and operating system overheads generally increase the
latency by two orders of magnitude.  Although each challenge is
non-trivial in itself, these challenges must be handled
simultaneously.  Unfortunately, techniques that improve one metric are
often at odds with another metric.  For example, dynamic voltage
scaling techniques improve energy-proportionality for batch
applications~\cite{DBLP:conf/asplos/DelimitrouK14}, but at the
expense of latency.  Similarly, batching techniques such as interrupt
coalescing improve performance at the expense of average and tail
latency~\cite{missing}.

The conventional wisdom is that these problems are rooted in a
mismatch between existing commodity operating systems, existing
network protocols, and the unique requirements of web-scale
applications.  Consequently, the proposed solutions generally involve
either: 
(i) improving the operating system's implementation and
interface~\cite{DBLP:conf/eurosys/PesterevSZM12,han2012megapipe}; 
(ii) replacing network-level connections with
datagrams~\cite{nishtala2013scaling}; 
(iii) offloading connection and protocol processing to a specialized hardware adapter, e.g, the
RAMcloud~\cite{DBLP:conf/sosp/OngaroRSOR11} key-value store relies on
RDMA~\cite{rdma-user-manual} to achieve 5~\microsecond latency; (iv)
bypassing the operating system and implementing the protocol stack in
user-space~\cite{jeong2014mtcp}; (v) merging the application logic
directly within a software data plane, as is commonly done in
middleboxes such as firewalls~\cite{missing},
load-balancers~\cite{missing}, and software
routers~\cite{DBLP:journals/tocs/KohlerMCJK00,DBLP:conf/sosp/DobrescuEACFIKMR09};
or even (vi) replacing the general-purpose hardware with a specialized
FPGA implementation dedicated to serving a single, simple workload
such as memcached~\cite{DBLP:conf/fpga/ChalamalasettiLWARM13}.

\christos{This is where we bring up the design principles of IX:
  separation data/control plane; native zero-copy event-oriented API; 
data-plane architecture that runs to completion; design for multi-core
and multi-queue. Probably two paragraphs}
In this paper, we propose an architecture designed specifically for
event-driven, web-scale applications.  Like a conventional commodity
operating system, the kernel  ---including the entire network protocol-processing stack--- is protected from the application.  Unlike a
conventional operating system, the networking stack is organized as a
dataplane that minimizes overheads and latency by processing flow
fragments to completion, interleaving (kernel-mode) networking
protocol processing with (user-mode) application execution.  
This model is similar to the use of coroutines in applications\cite{missing}.

We introduce \ix, a specialized system software solution built for
event-driven applications, including applications built for the
libevent framework~\cite{provos2003libevent}.  \ix is designed to meet the
challenge of the C10M problem~\cite{theC10Mproblem}: for a given
server, scale to 10 million concurrent TCP connections, saturate one
or multiple 10 GbE interfaces, and deliver 10~\microsecond latency
(mean), with an additional latency jitter of not more than an extra
10~\microsecond.

\ix addresses the C10M problem through a careful separation of the
control plane and dataplane functions.  The control plane consists of a
vanilla Linux deployment running the Dune
framework~\cite{belay2012dune}.  The dataplane is a special-purpose
library operating system that directly controls hardware I/O queues,
runs the networking stack, and generates the event callbacks of the
application.  Each dataplane instance exclusively controls a set of
hardware I/O queues, and runs a single application, e.g. a web sever
listening to \texttt{:80} and \texttt{:443}.  Unlike kernel-bypass
approaches and middlebox designs, \ix provides memory protection.
Like these designs however, \ix offers a zero-copy solution both
directions, eliminating all packet copy overheads.  Finally, the \ix
API and companion userlevel library meets the commutativity
rule~\cite{DBLP:conf/sosp/ClementsKZMK13}. Furthermore, our
implementation has a symmetrical design in which hardware queues are partitioned among all hyperthreads. 
The hyperthreads each interleave execution in kernel- and userspace, and executes in a
\emph{coherence-free} manner end-to-end in the common case.

\christos{lots of good numbers} Our evaluation of \ix shows that, on comparable hardware, it
outperforms mTCP~\cite{jeong2014mtcp}, the current state-of-the-art userlevel TCP stack by
3x on all microbenchmarks, and yet provides additional security and
tail-latency benefits.

\christos{I prefer to enumerate the main design principles and the
  list impressive numbers towards the goals rather than come up with
  another contribution list that is repetative}
The primary contributions of this paper are as follows:

\begin{itemize}

\item  the use of Linux and Dune as a mechanism to separate the control-plane from the dataplane in web-scale applications;

\item the design and implementation of \ix, a library operating system
  organized as a protected dataplane with zero-copy specifically
  designed to support event-driven applications;

\item an evaluation of \ix that shows that we meet the C10M challenge;
  despite the conventional wisdom, this is achieved through a
  specialized kernel (organized as dataplane);

\item the evaluation of \ix that shows that it outperforms the
  state-of-the-art user-level stack by \edb{3x?} or more for all
  micro-benchmarks and real-world applications such as memcached;

\item XXX

\end{itemize}


The rest of this paper is organized as follows: \edb{TODO}





