
\section{Introduction}
\label{sec:intro}

% \christos{We should stress IX's stregnts. Apart from separation of
%   CP-DP, we should mention a) we show how to architect high perf I/O
%   stacks (TCP and beyond), b) strong security model, c) compatibility
%   with Linux}

\edb{Datacenter\sout{Web-scale}} applications such as search, social networking, 
e-commerce platforms, \edb{and other web-scale applications},  are redefining the requirements \edb{for\sout{on}} systems
software. A single application can consist of hundreds of software
services, deployed on thousands of servers, creating a need for
networking stacks that provide more than high streaming performance.
The new requirements include high packet rates for short messages,
microsecond-level responses to remote requests with tight tail latency
guarantees, and support for high connection counts and
churn~\cite{Atikoglu:2012:WAL,DBLP:journals/cacm/DeanB13,DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}.
It is also important to have a strong protection model and be elastic
in resource usage, allowing other applications to use any idling
resources in a shared
cluster. %~\cite{DBLP:journals/computer/BarrosoH07}.

The conventional wisdom is that there is a basic mismatch between
these requirements and existing networking stacks in commodity
operating systems. Consequently, some systems bypass the kernel and
implement the networking stack in
user-space~\cite{jeong2014mtcp,DBLP:conf/cloud/KapoorPTVV12,sandstorm,openonload,DBLP:conf/sigcomm/ThekkathNML93}.
While kernel bypass eliminates context switch overheads, on its own it
does not eliminate the difficult tradeoffs between high packet rates
and low latency (see \S\ref{sec:eval:netpipe}). Moreover, user-level
networking suffers from lack of protection. Application bugs and
crashes can corrupt the networking stack and impact other workloads.
Other systems go a step further by also replacing TCP/IP with RDMA in
order to offload network processing to specialized
adapters~\cite{dragojevic14farm,DBLP:conf/icpp/JoseSLZHWIOWSP11,mitchell:rdma,DBLP:conf/sosp/OngaroRSOR11}.
However, such adapters must be present at both ends of the connection\edb{ and RDMA does not scale outside the datacenter}.


We propose \ix, an operating system designed to break the $4$-way
tradeoff between high throughput, low latency, strong protection, and
resource efficiency. Its architecture builds upon the lessons from
high performance middleboxes, such as firewalls, load-balancers, and
software routers~\cite{routebricks,click}. \ix separates the control
plane, which is responsible for system configuration and coarse-grain
resource provisioning between applications, from the dataplanes, which
run the networking stack and application logic\edb{ in two distinct protection levels}. \ix uses hardware
virtualization %~\cite{DBLP:journals/computer/UhligNRSMABKLS05}
to provide three-way protection between the control plane, the
networking stack, and the application. In our implementation, the
control plane is the full Linux kernel and the dataplanes run as
protected, library-based operating systems on dedicated hardware
threads.

The \ix dataplane allows for networking stacks that optimize for both
bandwidth and latency. It is designed around a native, zero-copy API
that supports processing of bounded batches of packets to
completion. Each dataplane executes all network processing stages
for a batch of packets in \edb{the \ix kernel}\sout{kernel mode}, followed by the associated
application processing in user mode. This approach amortizes API
overheads and improves both instruction and data locality. We set the
batch size adaptively based on load. The \ix dataplane also optimizes
for multi-core scalability.  The network adapters (NICs) perform
flow-consistent hashing of incoming traffic to distinct queues. Each
dataplane instance exclusively controls a set of these queues and runs
the networking stack and a single application without the need for
synchronization or coherence traffic in the common case operation. The
\ix API departs from the POSIX API, and its design is guided by the commutativity
rule~\cite{DBLP:conf/sosp/ClementsKZMK13}.  However, the \edb{\texttt{libix}\sout{\ix}}
user-level library includes an event-based API nearly identical to the
popular \texttt{libevent} library~\cite{provos2003libevent}, providing
compatibility with a wide range of existing applications.

We compare \ix with a TCP/IP dataplane against Linux
3.16.1 and mTCP, a state-of-the-art user-level TCP/IP
stack~\cite{jeong2014mtcp}.  On a 10GbE experiment using short messages, \ix outperforms Linux and mTCP by up to
10$\times$ and 1.9$\times$ respectively for throughput. \ix
further scales to a 4x10GbE configuration using a single multi-core socket.
The unloaded uni-directional latency for two IX servers is
5.7\microsecond, which is 4$\times$ better than between standard
Linux kernels and an order of magnitude better than mTCP, as both
trade-off latency for throughput.  Our evaluation with memcached, a
widely deployed key-value store, shows that \ix improves upon Linux by
up to 4.7$\times$ in terms of throughput at a given 99th
percentile latency bound, as it can reduce kernel time, due
essentially to network processing, from $>80\%$ with Linux to
$<10\%$ with \ix.

\ix demonstrates that, by revisiting the networking APIs and taking
advantage of modern NICs and multi-core chips, we can design systems
that achieve high throughput \underline{and} low latency
\underline{and} robust protection \underline{and} resource
efficiency. It also shows that, by separating the small subset of
performance-critical I/O functions from the rest of the kernel, we can
architect radically different I/O systems and achieve large
performance gains, while retaining compatibility with the huge set of
APIs and services provided by a modern OS like Linux.

The rest of the paper is organized as follows. \S\ref{sec:motivation}
motivates the need for a new OS architecture. \S\ref{sec:design} and
\S\ref{sec:impl} present the design principles and implementation of
\ix.  \S\ref{sec:eval} presents the quantitative
evaluation. \S\ref{sec:disc} and \S\ref{sec:related} discuss open
issues and related work.



