
\section{Introduction}
\label{sec:intro}

%\edb{Not updated in a while....}

Web-scale applications such as search, social networking, and
e-commerce platforms, are redefining the requirements on system
software. A single application can consist of hundreds of software
services, deployed on thousands of servers. For example, Amazon
routinely accesses 150 distinct services to render each page request
from external users~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07}. Such
applications need TCP/IP networking stack implementations that go well
beyond the classical requirements of high streaming performance and
moderate connection scalability. The new requirements include high
packet rates for the short messages~\cite{Atikoglu:2012:WAL,other?},
microsecond-level reponses to remote requests with tight tail latency
guarantees~\cite{DBLP:journals/cacm/DeanB13}, and support for hundreds
of thousands of connections with high connection
churn~\cite{nishtala2013scaling,other?}. It is also desirable to be
elastic in resource usage, allowing other applications to use the
available cores and memory in a shared
datacenter~\cite{nishtala2013scaling} or enabling cost reduction
through power management~\cite{DBLP:journals/computer/BarrosoH07}.


% \christos{1-2 paragraphs on how mainstream approaches fail many 
% of these requirements. A trimmed down version of what is shown below. }
% At massive scale, these interactions constrain applications.  For
% example, latency considerations force Facebook to restrict the number
% of sequential data accesses to fewer than 150 per rendered web
% page~\cite{rumble2011s}.  Also at Facebook, connection scalability
% limitations have led the deployment of a UDP-based memcached service
% tier~\cite{nishtala2013scaling} for \texttt{get} operations, which
% therefore forgoes all of the benefits of the standard TCP-based model.
% To ensure the reliable application of \texttt{put} operations, the
% system relies on in-network TCP-proxies to aggregate requests, which
% introduces complexity and latency in the architecture.

% In a recent keynote, \edb{too personal?}Google's Luiz Barroso identified three additional challenges
% for applications that must process large amounts of data in little
% time: energy-proportionality, tail-tolerance, and microsecond
% computing~\cite{luiz-isscc}.
% Energy-proportionality~\cite{DBLP:journals/computer/BarrosoH07}
% requires systems to perform with a high degree of energy-efficiency,
% typically measured in transactions / Watt, across all typical levels
% of load on the system, and not only at saturation. Tail
% tolerance~\cite{DBLP:journals/cacm/DeanB13} describes the emerging
% systems-building discipline that aims to deliver predictable response
% times in a distributed system.  Finally, the microsecond computing
% challenge describes the need to minimize the latency of interactions
% within a datacenter: even though existing transmission and switching
% technologies allow for microsecond-level messaging between any two
% nodes in a datacenter, a combination of protocol processing, interrupt
% delivery, and operating system overheads generally increase the
% latency by two orders of magnitude.  Although each challenge is
% non-trivial in itself, these challenges must be handled
% simultaneously.  Unfortunately, techniques that improve one metric are
% often at odds with another metric.  For example, dynamic voltage
% scaling techniques improve energy-proportionality for batch
% applications~\cite{DBLP:conf/asplos/DelimitrouK14}, but at the
% expense of latency.  Similarly, batching techniques such as interrupt
% coalescing improve performance at the expense of average and tail
% latency~\cite{missing}.

The conventional wisdom is that these problems are rooted in a
mismatch between existing commodity operating systems, existing
network protocols, and the unique requirements of web-scale
applications.  Consequently, the proposed solutions generally involve
either: 
(i) improving the operating system's implementation and
interface~\cite{DBLP:conf/eurosys/PesterevSZM12,han2012megapipe}; 
(ii) replacing network-level connections with
datagrams~\cite{nishtala2013scaling}; 
(iii) offloading connection and protocol processing to a specialized hardware adapter, e.g, the
RAMcloud~\cite{DBLP:conf/sosp/OngaroRSOR11} key-value store relies on
RDMA~\cite{rdma-user-manual} to achieve 5~\microsecond latency; (iv)
bypassing the operating system and implementing the protocol stack in
user-space~\cite{jeong2014mtcp}; (v) merging the application logic
directly within a software data plane, as is commonly done in
middleboxes such as firewalls~\cite{missing},
load-balancers~\cite{missing}, and software
routers~\cite{DBLP:journals/tocs/KohlerMCJK00,DBLP:conf/sosp/DobrescuEACFIKMR09};
or even (vi) replacing the general-purpose hardware with a specialized
FPGA implementation dedicated to serving a single, simple workload
such as memcached~\cite{DBLP:conf/fpga/ChalamalasettiLWARM13}.

\christos{This is where we bring up the design principles of IX:
  separation data/control plane; native zero-copy event-oriented API; 
data-plane architecture that runs to completion; design for multi-core
and multi-queue. Probably two paragraphs}
In this paper, we propose an architecture designed specifically for
event-driven, web-scale applications.  Like a conventional commodity
operating system, the kernel  ---including the entire network protocol-processing stack--- is protected from the application.  Unlike a
conventional operating system, the networking stack is organized as a
dataplane that minimizes overheads and latency by processing flow
fragments to completion, interleaving (kernel-mode) networking
protocol processing with (user-mode) application execution.  
This model is similar to the use of coroutines in applications\cite{missing}.

We introduce \ix, a specialized system software solution built for
event-driven applications, including applications built for the
libevent framework~\cite{provos2003libevent}.  \ix is designed to meet the
challenge of the C10M problem~\cite{theC10Mproblem}: for a given
server, scale to 10 million concurrent TCP connections, saturate one
or multiple 10 GbE interfaces, and deliver 10~\microsecond latency
(mean), with an additional latency jitter of not more than an extra
10~\microsecond.

\ix addresses the C10M problem through a careful separation of the
control plane and dataplane functions.  The control plane consists of a
vanilla Linux deployment running the Dune
framework~\cite{belay2012dune}.  The dataplane is a special-purpose
library operating system that directly controls hardware I/O queues,
runs the networking stack, and generates the event callbacks of the
application.  Each dataplane instance exclusively controls a set of
hardware I/O queues, and runs a single application, e.g. a web sever
listening to \texttt{:80} and \texttt{:443}.  Unlike kernel-bypass
approaches and middlebox designs, \ix provides memory protection.
Like these designs however, \ix offers a zero-copy solution both
directions, eliminating all packet copy overheads.  Finally, the \ix
API and companion userlevel library meets the commutativity
rule~\cite{DBLP:conf/sosp/ClementsKZMK13}. Furthermore, our
implementation has a symmetrical design in which hardware queues are partitioned among all hyperthreads. 
The hyperthreads each interleave execution in kernel- and userspace, and executes in a
\emph{coherence-free} manner end-to-end in the common case.

\christos{lots of good numbers} Our evaluation of \ix shows that, on comparable hardware, it
outperforms mTCP~\cite{jeong2014mtcp}, the current state-of-the-art userlevel TCP stack by
3x on all microbenchmarks, and yet provides additional security and
tail-latency benefits.

\christos{I prefer to enumerate the main design principles and the
  list impressive numbers towards the goals rather than come up with
  another contribution list that is repetative}
The primary contributions of this paper are as follows:

\begin{itemize}

\item  the use of Linux and Dune as a mechanism to separate the control-plane from the dataplane in web-scale applications;

\item the design and implementation of \ix, a library operating system
  organized as a protected dataplane with zero-copy specifically
  designed to support event-driven applications;

\item an evaluation of \ix that shows that we meet the C10M challenge;
  despite the conventional wisdom, this is achieved through a
  specialized kernel (organized as dataplane);

\item the evaluation of \ix that shows that it outperforms the
  state-of-the-art user-level stack by \edb{3x?} or more for all
  micro-benchmarks and real-world applications such as memcached;

\item XXX

\end{itemize}


The rest of this paper is organized as follows: \edb{TODO}





