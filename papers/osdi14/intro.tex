
\section{Introduction}
\label{sec:intro}


Web-scale applications are redefining the requirements of system
software.  A single application can consist of hundreds of software
components deployed on tens of thousands of machines per datacenter,
and on multiple datacenters.  For example, Amazon routinely accesses
150 distinct services to render a single
page~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07}.  

At massive scale, these interactions constrain applications.  For
example, latency considerations force Facebook to restrict the number
of sequential data accesses to fewer than 150 per rendered web
page~\cite{rumble2011s}.  Also at Facebook, connection scalability
limitations have led the deployment of a UDP-based memcached service
tier~\cite{nishtala2013scaling} for \texttt{get} operations, which
therefore forgoes all of the benefits of the standard TCP-based model.
To ensure the reliable application of \texttt{put} operations, the
system relies on in-network TCP-proxies to aggregate requests, which
introduces complexity and latency in the architecture.

In a recent keynote, Google's Luiz Barroso identified three additional challenges
for applications that must process large amounts of data in little
time: energy-proportionality, tail-tolerance, and microsecond
computing~\cite{luiz-isscc}.
Energy-proportionality~\cite{DBLP:journals/computer/BarrosoH07}
requires systems to perform with a high degree of energy-efficiency,
typically measured in transactions / Watt, across all typical levels
of load on the system, and not only at saturation. Tail
tolerance~\cite{DBLP:journals/cacm/DeanB13} describes the emerging
systems-building discipline that aims to deliver predictable response
times in a distributed system.  Finally, the microsecond computing
challenge describes the need to minimize the latency of interactions
within a datacenter: even though existing transmission and switching
technologies allow for microsecond-level messaging between any two
nodes in a datacenter, a combination of protocol processing, interrupt
delivery, and operating system overheads generally increase the
latency by two orders of magnitude.  Although each challenge is
non-trivial in itself, these challenges must be handled
simultaneously.  Unfortunately, techniques that improve one metric are
often at odds with another metric.  For example, dynamic voltage
scaling techniques improve energy-proportionality for batch
applications~\cite{DBLP:conf/asplos/DelimitrouK14}, but at the
expense of latency.  Similarly, batching techniques such as interrupt
coalescing improve performance at the expense of average and tail
latency~\cite{missing}.

The conventional wisdom is that these problems are rooted in a
mismatch between existing commodity operating systems, existing
network protocols, and the unique requirements of web-scale
applications.  Consequently, the proposed solutions generally involve
either: 
(i) improving the operating system's implementation and
interface~\cite{DBLP:conf/eurosys/PesterevSZM12,han2012megapipe}; 
(ii) replacing network-level connections with
datagrams~\cite{nishtala2013scaling}; 
(iii) offloading connection and protocol processing to a specialized hardware adapter, e.g, the
RAMcloud~\cite{DBLP:conf/sosp/OngaroRSOR11} key-value store relies on
RDMA~\cite{rdma-user-manual} to achieve 5~\microsecond latency; (iv)
bypassing the operating system and implementing the protocol stack in
user-space~\cite{jeong2014mtcp}; (v) merging the application logic
directly within a software data plane, as is commonly done in
middleboxes such as firewalls~\cite{missing},
load-balancers~\cite{missing}, and software
routers~\cite{DBLP:journals/tocs/KohlerMCJK00,DBLP:conf/sosp/DobrescuEACFIKMR09};
or even (vi) replacing the general-purpose hardware with a specialized
FPGA implementation dedicated to serving a single, simple workload
such as memcached~\cite{DBLP:conf/fpga/ChalamalasettiLWARM13}.

In this paper, we propose an architecture designed specifically for
event-driven, web-scale applications.  Like a conventional commodity
operating system, the kernel, including the entire network protocol
processing stack, are protected from the application.  Unlike a
conventional operating system, the networking stack is organized as
dataplane that minimizes overheads and latency by processing flow
fragments to completion, interleaving (kernel-mode) networking
protocol processing with (user-mode) application execution.

We introduce \ix, a specialized system software solution built for
event-driven applications, including applications built for the
libevent framework~\cite{provos2003libevent}.  \ix is designed to meet the
challenge of the C10 problem~\cite{theC10Mproblem}: for a given
server, scale to 10 million concurrent TCP connections, saturate one
or multiple 10 GbE interfaces, and deliver 10~\microsecond latency
(mean), with an additional latency jitter of not more than an extra
10~\microsecond.

\ix addresses the C10M problem through a careful separation of the
control plane and dataplane functions.  The control plane consists of a
vanilla Linux deployment running the Dune
framework~\cite{belay2012dune}.  The dataplane is a special-purpose
library operating system that directly controls hardware I/O queues,
runs the networking stack, and schedules the event callbacks of the
application.  Each dataplane instance exclusively controls a set of
hardware I/O queues, and runs a single application, e.g. a web sever
listening to \texttt{:80} and \texttt{:443}.  Unlike kernel-bypass
approaches and middlebox designs, \ix provides memory protection.
Like these designs however, \ix offers a zero-copy solution both
directions, eliminating all packet copy overheads.  Finally, the \ix
API and companion userlevel library meets the commutativity
rule~\cite{DBLP:conf/sosp/ClementsKZMK13}. Furthermore, our
implementation has a symmetrical design in which each hyperthread
interleaves execution in kernel- and userspace, and executes in a
\emph{coherence-free} manner end-to-end in the common case.

Our evaluation of \ix shows that, on comparable hardware, it
outperforms mTCP~\cite{jeong2014mtcp}, the current state-of-the-art userlevel TCP stack by
3x on all microbenchmarks, and yet provides additional security and
tail-latency benefits.

\edb{MORE, MORE, MORE}

The primary contributions of this paper are as follows:

\begin{itemize}

\item  the use of Linux and Dune as a mechanism to separate the control-plane from the dataplane in web-scale applications;

\item the design and implementation of \ix, a library operating system
  organized as a protected dataplane with zero-copy specifically
  designed to support event-driven applications;

\item an evaluation of \ix that shows that we meet the C10M challenge;
  despite the conventional wisdom, this is achieved through a
  specialized kernel (organized as dataplane);

\item the evaluation of \ix that shows that it outperforms the
  state-of-the-art user-level stack by \edb{3x?} or more for all
  micro-benchmarks and real-world applications such as memcached;

\item XXX

\end{itemize}


The rest of this paper is organized as follows: \edb{TODO}





\begin{comment}

%%% V1

We focus here on three high-level challenges for web-scale
applications that point to the need to address energy-proportionality,
connection scalability, and quality-of-service for web-scale
applications.

First, at the highest level, web-scale applications ideally deliver
the highest possible quality of service in the most
energy-proportional manner~\cite{DBLP:journals/computer/BarrosoH07}.
In practice, these two goals are often at odds with each other:
techniques that can improve quality-of-service, and in particular
reduce end-to-end latency, often consume much more energy.  For
example, low-power states save substantial amounts of energy, at the
cost of an increased latency measured in XXX when transitioning back
into a normal operating system.

Second, the nodes of web-scale applications are often characterized by
extremely high fan-out and high fan-in requirements.  An application
server may rely on tens of thousands of downstream services, and a
downstream service may serve hundreds of thousands of
clients~\cite{missing}.  At the extreme end of the spectrum,
notification and communication services must scale to the hundreds of
millions of Internet clients~\cite{DBLP:conf/sosp/AdyaCMP11}.
Unfortunately, existing implementations of the TCP/IP networking stack
in current systems were never designed to scale to such sizes.  As a
consequence, Facebook pragmatically chose to forgo the TCP protocol in
favor of connection-less UDP datagrams~\cite{nishtala2013scaling} for
its memcached implementation.  WhatsApp put significant engineering
efforts to tun FreeBSD to scale to 2 million concurrent
connections~\cite{whatsapp-2mil}.

Third, these high fan-in and fan-out applications suffer from the
\emph{long tail} problem, with the latency of the application
determined by the latency of slowest-responding
node~\cite{DBLP:journals/cacm/DeanB13}.  To lessen the impact of the
long tail, datacenter operators often overprovision servers to ensure
that they can operate at low utilization.

Collectively, the second and third challenge have been proposed as the
\emph{C10M problem}~\cite{theC10Mproblem}: for a given server, scale
to 10 million concurrent TCP connections, saturate 10 GbE interfaces,
and deliver 10~\microsecond latency (mean), with an additional latency
jitter of not more than an extra 10~\microsecond.  The conventional
wisdom is that such a solution must bypass the operating system
entirely.  For example, mTCP~\cite{jeong2014mtcp} is a user-level TCP
stack with the highest known scalability for short-lived connections.

\end{comment}
