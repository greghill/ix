
\section{Introduction}
\label{sec:intro}


% \dm{The intro is very buzzword-heavy, which makes a fundamental
%   contribution (architecture for super low-overhead networking) sound
%   really artifactual.  The term ``web-scale'' does not do us any
%   favors.  Yes right now the web and social networking are hot topics,
%   but these should be presented as examples of services that can
%   benefit from sub-microsecond receive paths.  Our contribution is not
%   to fix twitter's or Amazon's problems in 2014.  It is a fundamental
%   re-thinking of how to design network APIs for modern NIC hardware,
%   which could enable new types of system down the line.}

Web-scale applications such as search, social networking, and
e-commerce platforms, are redefining the requirements on system
software. A single application can consist of hundreds of software
services, deployed on thousands of servers. For example, Amazon
accesses 150 distinct services to render each page requested by
users~\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07}. Such applications need
TCP/IP networking stacks that go well beyond the classical
requirements of high streaming performance and moderate connection
scalability. The new requirements include high packet rates for short
messages~\cite{Atikoglu:2012:WAL}, microsecond-level responses to
remote requests with tight tail latency
guarantees~\cite{DBLP:journals/cacm/DeanB13}, and support for hundreds
of thousands of connections with high connection
churn~\cite{nishtala2013scaling}. It is also desirable to be elastic
in resource usage, allowing other applications to use any available
resources in a shared
cluster~\cite{DBLP:journals/computer/BarrosoH0,Leverich:RHSU:2014}.
% cores and memory in a shared cluster~\cite{Leverich:RHSU:2014} or
% enabling cost reduction through power
% management~\cite{DBLP:journals/computer/BarrosoH07}.


% \christos{1-2 paragraphs on how mainstream approaches fail many 
% of these requirements. A trimmed down version of what is shown below. }
% At massive scale, these interactions constrain applications.  For
% example, latency considerations force Facebook to restrict the number
% of sequential data accesses to fewer than 150 per rendered web
% page~\cite{rumble2011s}.  Also at Facebook, connection scalability
% limitations have led the deployment of a UDP-based memcached service
% tier~\cite{nishtala2013scaling} for \texttt{get} operations, which
% therefore forgoes all of the benefits of the standard TCP-based model.
% To ensure the reliable application of \texttt{put} operations, the
% system relies on in-network TCP-proxies to aggregate requests, which
% introduces complexity and latency in the architecture.

% In a recent keynote, \edb{too personal?}Google's Luiz Barroso identified three additional challenges
% for applications that must process large amounts of data in little
% time: energy-proportionality, tail-tolerance, and microsecond
% computing~\cite{luiz-isscc}.
% Energy-proportionality~\cite{DBLP:journals/computer/BarrosoH07}
% requires systems to perform with a high degree of energy-efficiency,
% typically measured in transactions / Watt, across all typical levels
% of load on the system, and not only at saturation. Tail
% tolerance~\cite{DBLP:journals/cacm/DeanB13} describes the emerging
% systems-building discipline that aims to deliver predictable response
% times in a distributed system.  Finally, the microsecond computing
% challenge describes the need to minimize the latency of interactions
% within a datacenter: even though existing transmission and switching
% technologies allow for microsecond-level messaging between any two
% nodes in a datacenter, a combination of protocol processing, interrupt
% delivery, and operating system overheads generally increase the
% latency by two orders of magnitude.  Although each challenge is
% non-trivial in itself, these challenges must be handled
% simultaneously.  Unfortunately, techniques that improve one metric are
% often at odds with another metric.  For example, dynamic voltage
% scaling techniques improve energy-proportionality for batch
% applications~\cite{DBLP:conf/asplos/DelimitrouK14}, but at the
% expense of latency.  Similarly, batching techniques such as interrupt
% coalescing improve performance at the expense of average and tail
% latency~\cite{missing}.

% \dm{Bad to rely on ``conventional wisdom'' without citation, but we
%   can reasonably quantify such overheads ourselves to state a fact.
%   E.g., ``Currently the only option for meeting XXX performance
%   requirement is to bypass the kernel[cite], with the following
%   disadvantages\ldots''} 
The conventional wisdom is that there is a fundamental mismatch
between the requirements of web-scale workloads and existing
implementations of TCP/IP in commodity operating
systems. Consequently, many proposed solutions bypass the OS and
implement the networking stack in
user-space~\cite{jeong2014mtcp,Kapoor:2012:CPL,openonload,marinos2013network,Thekkath:1993:INP}. Some
systems go one step further by also replacing TCP/IP with RDMA in
order to offload protocol processing to Infiniband
adapters~\cite{DBLP:conf/sosp/OngaroRSOR11,Jose:2011:MDH,mitchell:rdma,dragojevic14farm}. While
kernel bypass eliminates context switch overheads, on its own it does
not eliminate the difficult tradeoffs between high packet rates and
low latency or the challenges of managing large connections
counts. Moreover, user-level networking suffers from lack of
protection. Application bugs and crashes can corrupt the networking
stack and impact other workloads.

% Christos: I have decided to postpone the discussion of all other
% alternatives in section 2. Simplifies the message in the intro
% The conventional wisdom is that these problems are rooted in a
% mismatch between existing commodity operating systems, existing
% network protocols, and the unique requirements of web-scale
% applications.  Consequently, the proposed solutions generally involve
% either: (i) improving the operating system's implementation and
% interface~\cite{DBLP:conf/eurosys/PesterevSZM12,han2012megapipe}; (ii)
% replacing network-level connections with
% datagrams~\cite{nishtala2013scaling}; (iii) offloading connection and
% protocol processing to a specialized hardware adapter, e.g, the
% RAMcloud~\cite{DBLP:conf/sosp/OngaroRSOR11} key-value store relies on
% RDMA~\cite{rdma-user-manual} to achieve 5~\microsecond latency; (iv)
% bypassing the operating system and implementing the protocol stack in
% user-space~\cite{jeong2014mtcp}; (v) merging the application logic
% directly within a software data plane, as is commonly done in
% middleboxes such as firewalls~\cite{missing},
% load-balancers~\cite{missing}, and software
% routers~\cite{DBLP:journals/tocs/KohlerMCJK00,DBLP:conf/sosp/DobrescuEACFIKMR09};
% or even (vi) replacing the general-purpose hardware with a specialized
% FPGA implementation dedicated to serving a single, simple workload
% such as memcached~\cite{DBLP:conf/fpga/ChalamalasettiLWARM13}.

% \christos{This is where we bring up the design principles of IX:
%   separation data/control plane; native zero-copy event-oriented API; 
% data-plane architecture that runs to completion; design for multi-core
% and multi-queue. Probably two paragraphs}

\dm{Again, this makes it sound narrow.  Phrase in a more fundamental
  way.  Point is that currently people must achieve a delicate $n$-way
  balance between throughput, latency, protection/robustness,
  complexity, number of machines/power consumption, etc.  \ix shows it
  doesn't have to be this way; we can have our cake and eat it, too,
  if we architect better systems around improved APIs.} 

We propose \ix, an operating system designed specifically to meet the
requirements of event-driven, web-scale applications.  Its
architecture builds upon the lessons from high performance
middleboxes, such as firewalls, load-balancers, and software
routers~\cite{DBLP:journals/tocs/KohlerMCJK00,DBLP:conf/sosp/DobrescuEACFIKMR09}. \ix
separates the control plane, which is responsible for basic kernel
functionality such as provisioning and scheduling, from the dataplanes
which run the networking stack and application logic. However, \ix
uses hardware
virtualization~\cite{DBLP:journals/computer/UhligNRSMABKLS05} to
isolate the dataplanes from the applications they run and offer the
same protection model as commodity operating systems. In our
implementation, the control plane runs on a full Linux kernel and the
dataplanes use Dune to run \ix as protected, library-based operating
systems on dedicated hardware
threads~\cite{belay2012dune}. % \adam{new:} Dune makes this
% possible by directly and safely exposing hardware protection
% mechanisms to user-level.

\ix provides a native, zero-copy API that explicitly exposes network
flow-control to applications.  This API enables dataplanes to optimize
for both bandwidth and latency by processing a bounded batch of
packets to completion.  Each dataplane executes all the pipeline
stages for TCP/IP processing for the batch in kernel mode, followed by
the associated application processing in user mode. This approach
amortizes API overheads and improves both instruction and data
locality.  \ix is also optimized for synchronization and coherence
free execution on multi-core systems. It uses multi-queue network
adapters (NICs) and receive-side scaling (RSS)~\cite{url:rss} to
implement flow-consistent hashing of incoming traffic to distinct
hardware queues. Each dataplane instance exclusively controls a set of
these queues and runs the networking stack and a single application,
such as a webserver or a distributed caching server. Moreover, the \ix
API and companion user-level library meet the commutativity
rule~\cite{DBLP:conf/sosp/ClementsKZMK13}, eliminating multi-core
synchronization in the common case operation. The \ix user-level
library includes an interface nearly identical to the popular
\texttt{libevent} library~\cite{provos2003libevent}, providing
compatibility between \ix and a wide range of existing applications.

\dm{The above paragraph does a reasonable job of describing what's
  new.  But it might be worth another paragraph addressing why now
  Basically there's a fundamental question of granularity here.  E.g.,
  page sizes have been 4--8K since machines had less than a Megabyte
  of DRAM\@.  Schedulers have been designed under the premise of more
  applications than CPUs.  Network APIs were designed when packet
  inter-arrival times were many times the system call/interrupt
  latency.}

\dm{Another point to add is that the breadth of OS APIs has made it
  virtually impossible to deploy clean-slate operating systems,
  despite possibly huge performance gains from radically different IO
  architectures.  Fortunately, the performance-critical IO functions
  are a small subset of the garbage can of system calls required for
  setup, initialization, and configuration.  Hence, a big contribution
  is showing how we can completely rearchitect the IO path while
  retaining a high degree of source code compatibility and remaining
  compatible with existing system configuration and management tool.}

% \ix addresses the C10M problem through a careful separation of the
% control plane and dataplane functions.  The control plane consists of a
% vanilla Linux deployment running the Dune
% framework~\cite{belay2012dune}. 

% The dataplane is a special-purpose library operating system that
% directly controls hardware I/O queues, runs the networking stack, and
% generates the event callbacks of the application.  Each dataplane
% instance exclusively controls a set of hardware I/O queues, and runs a
% single application, e.g. a web sever listening to \texttt{:80} and
% \texttt{:443}.  Unlike kernel-bypass approaches and middlebox designs,
% \ix provides memory protection.  Like these designs however, \ix
% offers a zero-copy solution both directions, eliminating all packet
% copy overheads.


% We introduce \ix, a specialized system software solution built for
% event-driven applications, including applications built for the
% libevent framework~\cite{provos2003libevent}.  \ix is designed to meet the
% challenge of the C10M problem~\cite{theC10Mproblem}: for a given
% server, scale to 10 million concurrent TCP connections, saturate one
% or multiple 10 GbE interfaces, and deliver 10~\microsecond latency
% (mean), with an additional latency jitter of not more than an extra
% 10~\microsecond.

% The primary contributions of the \ix design are: (i) the combined use
% of commodity OS and virtualization hardware to separate control from
% data plane in a protected networking stack; (ii) a run-to-completion
% execution model with cooperating, untrusted applications; (iii) a
% bounded, adaptive batching mechanism that provides both low-latency
% and high throughput; (iv) an implementation carefully optimized for
% multi-core scalability. 

We compare \ix against Linux 3.11.10 and mTCP, a state-of-the-art
user-level TCP stack~\cite{jeong2014mtcp}.  \ix outperforms Linux and
mTCP by up to 13x and 2.5x respectively for throughput. \ix even
scales to 4x10GbE configuration using a single multi-core socket.  The
uni-directional latency for two IX servers is 6.9\microsecond, while
Linux and mTCP lead to latencies of 24\microsecond and 95\microsecond
respectively.
% On networking
% microbenchmarks that are either latency-sensitive, require a high TCP
% connection churn, or manage a large number of concurrent connections,
% \ix outperforms Linux by an order of magnitude, and mTCP by up to
% 2.5x.  
Our evaluation with memcached, a massively deployed key-value store,
shows that \ix improves upon Linux by up to 2.7x in terms of
throughput at a given 95th percentile latency bound, as it can reduce
kernel mode processing from $>80\%$ with Linux to $<33\%$ with \ix.

\ix demonstrates that, by revisiting the networking APIs and taking
advantage of modern NICs and multi-core chips, we can design systems
that achieve high throughput \underline{and} low latency
\underline{and} high connection counts \underline{and} robust
protection. It also shows that, by separating the the small subset of
performance-critical I/O functions from the rest of the kernel, we can
design systems with radically different I/O architectures and achieve
large performance gains, while retaining compatibility with the huge
set of APIs and services provided by a modern OS like Linux.


% \christos{I prefer to enumerate the main design principles and the
%   list impressive numbers towards the goals rather than come up with
%   another contribution list that is repetative}
% The primary contributions of this paper are as follows:

% \begin{itemize}

% \item  the use of Linux and Dune as a mechanism to separate the control-plane from the dataplane in web-scale applications;

% \item the design and implementation of \ix, a library operating system
%   organized as a protected dataplane with zero-copy specifically
%   designed to support event-driven applications;

% \item an evaluation of \ix that shows that we meet the C10M challenge;
%   despite the conventional wisdom, this is achieved through a
%   specialized kernel (organized as dataplane);

% \item the evaluation of \ix that shows that it outperforms the
%   state-of-the-art userlevel stack by \edb{3x?} or more for all
%   micro-benchmarks and real-world applications such as memcached;

% \item XXX

% \end{itemize} 

\christos{Eliminate if we don't have space} The rest of the paper is 
organized as follows. \S \ref{sec:motivation} motivates the need
for a new OS architecture for web-scale applications. \S\ref{sec:design} and \S\ref{sec:impl} present the design principles and
implementation of \ix. \S\ref{sec:eval} presents the
quantitative evaluation.\S\ref{sec:disc} and \S\ref{sec:related} discuss future and related work.






