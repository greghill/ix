\section{\ix Design Approach}
\label{sec:design}

The first two requirements of \S\ref{sec:motivation:challenges} --
microsecond latency and high packet rates -- are not unique to
web-scale applications.  These requirements have been addressed in the
design of middleboxes such as firewalls, load-balancers, and software
routers~\cite{routebricks,click}, by integrating the networking stack
and the application into a single \emph{dataplane}. The two remaining
requirements -- protection and resource efficiency -- are not
addressed in middleboxes because they are single-purpose systems, not
exposed directly to users.

Dataplanes differ from traditional OS designs in two fundamental
ways. First, they are designed to \emph{run each packet to
  completion}. All network protocol and application processing for a
packet is done before moving on to the next packet.  In contrast, a
commodity OS decouples protocol processing from the application itself
in order to provide scheduling and flow control flexibility.  For
example, a commodity OS relies on device and soft interrupts to
context switch from applications to protocol processing. Similarly,
the kernel's stack will generate a TCP \texttt{ACK} and slide its
receive window even when the application is not consuming data, up to
an extent. Second, dataplanes optimize for \emph{synchronization-free
  operation} in order to scale well on many cores. Network flows are
distributed into distinct queues via receive-side scaling
(RSS)~\cite{url:rss} and the common case packet processing requires no
synchronization or coherence traffic between cores.

\ix extends the dataplane architecture to efficiently support
untrusted applications and satisfy simultaneously all requirements in
\S\ref{sec:motivation:challenges}. Its design is based on the
following key principles: 

\christos{this list must be aligned with the list in 2.2. Ie we should
  be able to explain how each approach helps with the
  requirements. This is critical}

\myparagraph{Separation and protection of control and data plane:} Our
design separates the control function of the kernel, responsible for
resource configuration, provisioning, scheduling, and monitoring, from
the dataplane, which runs the networking stack and application logic.
Like a conventional OS, the control plane multiplexes and schedules
resources among dataplanes, but in a coarse-grained manner in space
and time. Entire cores are dedicated to dataplanes, memory is
allocated at large page granularity, and NIC queues are assigned to
dataplane cores. The control plane is also responsible for elastically
adjusting the allocation of these resources between dataplanes.

The separation of control and data plane allows us to consider
radically different I/O APIs and implementations in the dataplane,
while permitting other OS functionality, such as file system support,
to be passed through to the control plane for compatibilility.
Similar to the Exokernel~\cite{DBLP:conf/sosp/EnglerKO95}, each data
plane runs a single application in a single address space. However, we
use modern virtualization hardware~\cite{dune} to provide three-way
isolation between the control plane, the dataplane, and untrusted user
code. Dataplanes have capabilities similar to guest OSes in
virtualized systems.  They can manage their own address translations,
on top of the address space provided by the control plane, and can
protect the network stack from untrusted application logic through use
of privilege ring hardware. Moreover, dataplanes are given direct
passthrough access to NIC queues through memory mapped I/O.

% \christos{we should bring up HW virtualization here, likely as a
%   independent part of the approach or merged with the one above. It is
%   a big part of the protection story. The discussion on CP/DP
%   seperation must be linked to elasticity too}


\myparagraph{Run to completion with adaptive batching:} Our dataplane
design runs to completion all pipeline stages needed to receive and
transmit a packet, interleaving protocol processing (kernel mode) and
application logic (user mode) at well defined transition
points. Hence, there is no need for intermediate buffering between
pipeline stages or between application logic and the TCP/IP stack.
Unlike previous work that applied a similar approach to eliminate
receive livelocks on congestion periods~\cite{receive-livelock}, \ix
applies run to completion during all load conditions. Thus, we are
able to use polling and avoid interrupt overhead in the common
case. However, we still rely on interrupts as a mechanism to regain
control, for example, if a dataplane is slow to respond.  Run to
completion improves both message throughput and latency because each
processing phase tends to access many of the same cache lines, leading
to better data cache locality.

The \ix dataplane also makes extensive use of batching.  Previous
systems applied batching at the system call
boundary~\cite{DBLP:conf/osdi/HanMCR12, DBLP:conf/osdi/SoaresS10} and
at the network API and hardware queue level\cite{jeong2014mtcp}.  We
apply batching in every processing stage in the network stack,
including but not limited to system calls and queues. Moreover, we use
batching \emph{adaptively} as follows: (i) we never wait to batch
requests and batching only occurs in the presence of congestion; (ii)
we set an upper bound on the number of batched packets. Using batching
only during congestion allows us to minimize the impact on latency,
while bounding the batch size prevents the live set from exceeding
cache capacities and helps to prevent transmit queue
starvation. Batching improves the message throughput because it
amortizes system call transition overheads % (although these costs are
% relatively small on modern hardware)
and increases instructions per
cycle through improved instruction cache locality, prefetching
effectiveness, and branch prediction accuracy. When applied
adaptively, batching also decreasing latency because these same
efficiencies reduce head-of-line blocking.

The combination of bounded batch sizes and run to completion means
that queues for incoming packets can build up only at the NIC edge,
before packet processing starts in the dataplane. 
% before packets are processed by the dataplane. \ana{Emphasize that
% we are not wasting resources on packets that might have to be
% dropped; drop early at the NIC if can't handle.  Citation for Mogul
% and Ramakrishnan could go here.}
The networking stack sends acknowledgments to peers only as fast as
the application can process them. Any slowdown in the
application-processing rate quickly leads to shrinking windows in
peers. The dataplane can also monitor queue depths at the NIC edge and
signal the control plane to allocate additional resources for the
dataplane (more hardware threads, increased clock frequency), notify
peers explicitly about congestion (e.g., via
ECN~\cite{ramakrishnan2001addition}), and make policy decisions for
congestion management (e.g., via
RED~\cite{DBLP:journals/ton/FloydJ93}).

\input{figs/float-cp-dp}

\myparagraph{Native, zero-copy API with explicit flow control:} We do
not expose or emulate the POSIX API for networking.  Instead, the
dataplane and the application communicate asynchronously with each
other via messages stored in
memory~\cite{DBLP:conf/osdi/HanMCR12,DBLP:journals/cacm/Rizzo12}.  The
API meets the commutativity rule~\cite{DBLP:conf/sosp/ClementsKZMK13}
and allows for a true zero-copy operation in both directions. The
dataplane and application cooperatively manage the message buffer
pool. Incoming packets are mapped read-only into the application,
which may hold onto message buffers and return them to the dataplane
at a later point.  The application sends to the dataplane
scatter/gather lists of memory locations for transmission but, since
contents are not copied, the application must keep the content
immutable until the peer acknowledges reception. The dataplane implements
all flow control mechanisms and may trim transmission requests that
exceed the available size of the sliding window.  Essentially, the API
directly, but safely, exposes flow control to applications.


\myparagraph{Flow consistent, synchronization-free processing:} We use
multi-queue NICs with RSS support to provide flow-consistent hashing
of incoming traffic to distinct hardware queues. Each queue is
assigned to a single dataplane and is served by a single hardware
thread, eliminating the need for synchronization and coherence traffic
between cores in the networking stack. Similarly, memory management is
organized in distinct pools for each hardware thread. The absence of a
POSIX sockeet API eliminates the issue of the shared file descriptor
namespace in multithreaded
applications~\cite{DBLP:conf/sosp/ClementsKZMK13}. Overall, the \ix
dataplane design scales well with the increasing number of cores in
modern servers, which improves both packet rate and latency. This
approach does not restrict the memory model for
applications. Application logic can take advantage of coherent, shared
memory to exchange information and synchronize between cores.




