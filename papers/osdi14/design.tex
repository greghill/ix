\section{\ix Design Approach}
\label{sec:design}

% We now present the fundamental design principles of a dataplane
% architecture designed to run untrusted, event-driven applicatxions, and
% designed to address the specific scalability challenges of today's
% web-scale applications.

The first three requirements of \S\ref{sec:motivation:challenges} --
high packet rate, microsecond latency, and connection scalability---
are not unique to event-driven, web-scale applications.  These
requirements are reasonably well understood and have been addressed in
the design of middleboxes such as firewalls~\cite{missing},
load-balancers~\cite{missing}, and software
routers~\cite{DBLP:journals/tocs/KohlerMCJK00,DBLP:conf/sosp/DobrescuEACFIKMR09};
by integrating the networking stack and the application into a single
\emph{dataplane}. The two remaining requirements, resource elasticity
and protection, are not addressed for such middleboxes because they
are single-purpose systems, not exposed directly to users. 

% As for the other two remaining aspects -- resource elasticity and
% security and protection --, middleboxes provide fewer insights: since
% each middlebox typically performs a single function, resource
% elasticity and proportionality is less of a direct concern.  They are
% traditionally deployed and maintained using an embedded paradigm in
% which the entire stack -- operating system, applications and utilities
% -- is packaged as single blob.  As such, the security and protection
% model is not directly exposed to users.

Dataplanes differ from a traditional OS designs in two fundamental
ways. First, they are designed to \emph{run each packet to
  completion}. All protocol and application processing for a packet is
done before moving on to the next packet.  In contrast, a commodity OS is designed
with protocol processing decoupled from the application itself in
order to enable flexibility in terms of resource scheduling and flow
control. For example, a commodity OS relies on device and soft interrupts to context switch from application processing to protocol processing; 
similarly, the OS' networking stack will generate a TCP \texttt{ACK} and
slide its receive window even though the application is not consuming
data, up to some extent. Second, dataplanes are designed to operate in
a \emph{flow-consistent, coherence-free} manner.
Network flows are distributed into distinct queues via receive-side
scaling (RSS)~\cite{DBLP:journals/computer/RegnierMIIMHNCF04}, so that 
the common case processing of a packet requires no synchronization or cache coherence interactions
between cores.

% Flow-consistency distributes flows
% into distinct queues based on a L4-hash; this is routinely supported
% in network CPUs used in middleboxes (e.g., ~\cite{cavium-octeon}) as
% well as commodity NIC via Receive Side Scaling (RSS)~\cite{missing}.

Building upon the lessons from middleboxes, we design \ix to answer
the following question: {\it can the dataplane architecture be
  efficiently extended to support untrusted, event-driven applications
  and satisfy simultaneously the five requirements of
  \S\ref{sec:motivation:challenges}?}  The answer relies on the
following key design principles:


\myparagraph{Separation and protection of control and data plane:} 
% Software dataplanes are designed to operate on flows, not to
% provision resources or manage them in an elastic and proportional
% manner.
Our design separates the control function, responsible for resource
configuration, provisioning, scheduling, and monitoring, from the
dataplane, which runs the networking stack and application logic.
Like a conventional OS, our control plane multiplexes and schedules
resources among dataplanes but in an coarse-grain manner in space and
time: cores are dedicated to applications until revoked, memory is
allocated is superpage granularity and pinned; memory hardware
queues are exclusively assigned to a single dataplane until explicitly revoked. 
Each dataplane runs a single application in a single address
space.  This is similar to the Exokernel approach of using library
operating systems~\cite{DBLP:conf/sosp/EnglerKO95}, but with the
additional protection between applications and the control plane using
the ubiquitous available hardware virtualization features in modern
servers~\cite{DBLP:journals/computer/UhligNRSMABKLS05,belay2012dune}.


\myparagraph{Native zero-copy, event-oriented API:} The kernel does
not expose or emulate the POSIX API for networking.  Instead, kernel
and application communicate asynchronously with each other via
messages stored in memory~\cite{rizzo2012netmap,han2012megapipe}.  The
API, i.e., the set of define messages, meets the commutativity rule~\cite{DBLP:conf/sosp/ClementsKZMK13}.
The API allows for a true zero-copy
operation in both receive and transfer directions by having the
dataplane and the application cooperatively manage the message buffer
pool.  Incoming packets are mapped read-only into the
application. Applications may hold onto message buffers and return
them to the kernel at a later point.

\myparagraph{Explicit flow control:} The API supports a zero-copy,
non-blocking API that directly, but safely, exposes flow control to
applications. The application may send to the dataplane scatter/gather
lists of memory locations for transmission.  The kernel trims requests
that exceed the available size of the sliding window. As the contents
are not copied, the application must further keep the content
immutable until the peer later acknowledges reception.  Although
the kernel implements all flow control mechanisms, it does not
introduce additional buffering or abstraction mechanisms, allowing
applications to make appropriate policy decisions.


\myparagraph{Run to completion with adaptive batching:} Our dataplane
runs to completion all pipeline stages needed to receive or transmit a
packet, intermixing protocol processing and application logic as
needed. Hence, there is no need for intermediate buffering between
pipeline stages or between the application logic and network protocol
stack. Batch processing minimizes instruction overheads and increases
cache locality.  Unlike previous proposals that batch only at the API
level in order to amortize system call
overhead~\cite{jeong2014mtcp,han2012megapipe}, we execute every
pipeline stage on a small batch of packets or commands in order to
fully amortize instruction caching effects and the overhead of PCIe
transfers.  To minimize the impact on latency, and improve utilization
on the outgoing wire, we adaptively batch and only in the presence of
congestion. We experimentally find that bounding batches to 128
packets is sufficient to amortize all noticeable per-packet overheads.

\input{figs/float-queues-cores}


\myparagraph{Flow consistent, coherence-free processing:} We use multi-queue
NICs with RSS support to provide flow-consistent hashing of incoming
traffic to distinct hardware queues. Each queue is served by a single
hardware thread all the way to the application layer, eliminating the
need for synchronization and cache coherence traffic between
cores. Similarly, memory management is organized in distinct pools for
each hardware thread. The absence of a socket layer eliminates the
issue of the shared file descriptor namespace in multithreaded applications~\cite{DBLP:conf/sosp/ClementsKZMK13}. Hence,
our design scales well with the increasing number of cores in modern
servers. Our approach does not restrict the memory model for
applications. Application logic can take advantage of coherent, shared
memory to exchange information and synchronize between cores. 


% \christos{seems like an implementation detail to me}
%The approach extends to link-aggregation bonds to scale to multiple links.


\myparagraph{Buffering at the NIC edge:} The consequence of adaptive
batching is the buildup of queues at the NIC edge before the packets
are processed by the dataplane.  Because of the dataplane's design,
congestion occurs only at the NIC edge.  In effect, the NIC edge acts
like the last-hop buffer in the network.  This has an additional
benefit in terms of flow control, as the networking stack sends
acknowledgments to peers only as fast as the application can process
them.  Congestion in the NIC edge therefore leads to shrinking windows
in peers, and flow control.


Like switches and routers, the NIC edge can
monitor queue depths to detect congestion, signal the control plane to
allocate additional resources (more hardware threads, increase clock
frequency) for the dataplane, notify explicitly flow sources of
congestion (e.g., via ECN~\cite{ramakrishnan2001addition}), and finally make
policy decisions when it is necessary to manage congestion (e.g., via RED~\cite{DBLP:journals/ton/FloydJ93}).


