\section{\ix Design Approach}
\label{sec:design}

The first three requirements of \S\ref{sec:motivation:challenges} --
high packet rate, microsecond latency, and connection scalability--
are not unique to web-scale applications.  These requirements have
been addressed in the design of middleboxes such as firewalls,
load-balancers, and software
routers~\cite{routebricks,click};
by integrating the networking stack and the application into a single
\emph{dataplane}. The two remaining requirements -- resource
elasticity and protection -- are not addressed in middleboxes because
they are single-purpose systems, not exposed directly to users.

Dataplanes differ from traditional OS designs in two fundamental
ways. First, they are designed to \emph{run each packet to
  completion}. All network protocol and application processing for a
packet is done before moving on to the next packet.  In contrast, a
commodity OS decouples protocol processing from the application itself
in order to provide scheduling and flow control flexibility.  For
example, a commodity OS relies on device and soft interrupts to
context switch from application to protocol processing. Similarly, the
kernel's networking stack will generate a TCP \texttt{ACK} and slide
its receive window even when the application is not consuming data, up
to an extent. Second, dataplanes are designed to operate in a
\emph{flow-consistent, coherence-free} manner.  Network flows are
distributed into distinct queues via receive-side scaling
(RSS)~\cite{url:rss} and the common case packet processing requires no
synchronization or coherence traffic between cores.

Building upon the lessons from middleboxes, we design \ix to answer
the following question: {\it \ana{how} can the dataplane architecture be
  efficiently extended to support untrusted, event-driven applications
  and satisfy simultaneously the five requirements of
  \S\ref{sec:motivation:challenges}?}  The answer relies on the
following key design principles:

\christos{this list must be aligned with the list in 2.2. Ie we should
  be able to explain how each approach helps with the
  requirements. This is critical}

\myparagraph{Separation and protection of control and data plane:} 
Our design separates the control function of the kernel, responsible
for resource configuration, provisioning, scheduling, and monitoring,
from the dataplane, which runs the networking stack and application
logic.  Like a conventional OS, the control plane multiplexes and
schedules resources among dataplanes, but in a coarse-grained manner
in space and time: cores are dedicated to applications until revoked,
memory is allocated at large page granularity of physical memory;
hardware queues are exclusively assigned to a single dataplane until
explicitly revoked.  The separation allows us to consider radically
different I/O APIs and implementations in the dataplane, while
retaining full compatibility with a modern OS like Linux.  Similar to
Exokernel~\cite{DBLP:conf/sosp/EnglerKO95}, each dataplane runs a
single application in a single address space. 

\christos{we should bring up HW virtualization here, likely as a
  independent part of the approach or merged with the one above. It is
  a big part of the protection story. The discussion on CP/DP
  seperation must be linked to elasticity too}

\myparagraph{Native zero-copy API with explicit flow control:} We do
not expose or emulate the POSIX API for networking.  Instead, the
dataplane and the application communicate asynchronously with each
other via messages stored in
memory~\cite{DBLP:conf/osdi/HanMCR12,DBLP:journals/cacm/Rizzo12}.  The API meets the
commutativity rule~\cite{DBLP:conf/sosp/ClementsKZMK13} and allows for
a true zero-copy operation in both directions. The dataplane and
application cooperatively manage the message buffer pool. Incoming
packets are mapped read-only into the application, which may hold onto
message buffers and return them to the dataplane at a later point.
The application sends to the dataplane scatter/gather lists of memory
locations for transmission but, since contents are not copied, the
application must keep the content immutable until the peer
acknowledges reception. The kernel implements all flow control
mechanisms and may trim transmission requests that exceed the
available size of the sliding window.  Essentially, the API directly,
but safely, exposes flow control to applications.


\myparagraph{Run to completion with adaptive batching:} Our dataplane
runs to completion all pipeline stages needed to receive or transmit a
packet, interleaving protocol processing (kernel mode) and application
logic (user mode) as needed. Hence, there is no need for intermediate
buffering between pipeline stages or between the application logic and
the TCP/IP stack. Run to completion has been previously proposed to
address receive livelock within the OS~\cite{receive-livelock}. We
extend it to both kernel and user-level, which eliminates interrupts
and improves data locality throughout the common case packet
processing.

Batch processing minimizes instruction overheads and increases
instruction locality. Previous proposals batch only at the API level
in order to amortize system call
overhead~\cite{DBLP:conf/osdi/HanMCR12,jeong2014mtcp,DBLP:conf/osdi/SoaresS10}. We
execute every pipeline stage on a small batch of packets or commands
to fully amortize instruction caching effects and the overhead of PCIe
transfers. To minimize the impact on latency, we perform adaptive
batching as follows: (i) we never wait to batch requests and batching
only occurs in the presence of congestion; (ii) we set an upper bound
on batches of \data{128}{XXX -- RETHINK} packets, a value experimentally determined to
effectively amortize startup overheads.

The combination of adaptive batch processing and zero copy means that
queues for incoming packets can build up only at the NIC edge, before
packets are processed by the dataplane. \ana{Emphasize that we are not
  wasting resources on packets that might have to be dropped; drop
  early at the NIC if can't handle.  Citation for Mogul and
  Ramakrishnan could go here.} The networking stack sends
acknowledgments to peers only as fast as the application can process
them. Any slowdown in the application-processing rate quickly leads to
shrinking windows in peers. The dataplane also monitors queue depths
at the NIC edge and signals the control plane to allocate additional
resources for the dataplane (more hardware threads, increased clock
frequency), notify peers explicitly about congestion (e.g., via
ECN~\cite{ramakrishnan2001addition}), and make policy decisions for
congestion management (e.g., via
RED~\cite{DBLP:journals/ton/FloydJ93}).

\input{figs/float-cp-dp}


\myparagraph{Flow consistent, coherence-free processing:} We use
multi-queue NICs with RSS support to provide flow-consistent hashing
of incoming traffic to distinct hardware queues. \data{Each queue is served
by a single hardware thread}{XXX REDO} all the way to the application layer,
eliminating the need for synchronization and cache coherence traffic
between cores. Similarly, memory management is organized in distinct
pools for each hardware thread. The absence of a socket layer
eliminates the issue of the shared file descriptor namespace in
multithreaded
applications~\cite{DBLP:conf/sosp/ClementsKZMK13}. Hence, our design
scales well with the increasing number of cores in modern servers. Our
approach does not restrict the memory model for
applications. Application logic can take advantage of coherent, shared
memory to exchange information and synchronize between cores.




