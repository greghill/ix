
\subsection{Memcached Performance}
\label{sec:eval:memcached}

\input{figs/float-connscaling}
\input{figs/float-mutilate}

Finally, we evaluated the performance benefits of the \ix prected
dataplane design with \texttt{memcached}, a massively deployed,
in-memory key-value store built on top of the \texttt{libevent}
framework~\cite{url:memcached}, . It is frequently used as a
high-throughput, low-latency caching tier in front of persistent
database servers. \texttt{memcached} is a networking bound
application, with threads spending over 80\% of execution time in
kernel mode for network processing~\cite{Leverich:RHSU:2014}. It is a
particularly difficult application to scale, in particular because the
common deployments involve high connection counts for
\texttt{memcached} servers and small-sized requests and
replies~\cite{nishtala2013scaling,Atikoglu:2012:WAL}

We use the \texttt{mutilate} load-generator to place a selected load
on the server in terms of requests per second (RPS) and measure
response latency~\cite{url:mutilate}. \texttt{mutilate} coordinates a
large number of client threads across multiple machines to generate
the desired RPS load, while a sepeparate unloaded client measures
latency by issuing one request at the time.  We configure
\texttt{mutilate} to generate load representative of two workloads
from Facebook~\cite{Atikoglu:2012:WAL}: the ETC workload that
represents that highest capacity deployment in Facebook, has 20B - 70B
keys, 1B-1KB values, and 90\% GET requests; and the USR workload that
represents deployment with most GET requests in Facebook, has short
keys ($<$20B), 2B values, and 99\% GET requests. In USR, almost all
traffic involves minimum-sized TCP packets. Each request is issued 

For all experiments, we report 95th percentile latency as a function
of the achieved throuhput. This graph provides insights the full range
of system behaviors. Most commercial \texttt{memcached} use such a
latency-throughput graph to provision each server so that the 95th
percentile latency does not exceed 200 to 500\microsecond. We report
only 95ht percentile latency as the average or median latency is
irrelevant for web-scale services~\cite{DBLP:journals/cacm/DeanB13}.
We carefully tune the Linux baseline setup according to the guidlines
in \cite{Leverich:RHSU:2014}. Specifically, we pin memcached threads,
configure interrupt-distribution based on thread-affinity, and tune
interrupt moderation thresholds. We believe that our baseline Linux
numbers are as tuned as possible for this hardware using the
open-source version of memcached. \christos{add version number}

\christos{Should we discuss the porting process to IX?}

\christos{ guestimate} Fig.~\cite{fig:mutilate} shows the
throughput-latency curves for the two \texttt{memcached} workloads for
Linux and \ix. In both cases, we use \christos{X} client machines and
a total of \christos{Y} connections to the memcached server. The
server uses 16 hyperthreads for memcached. Comparing the knee of the
Linux and \ix, \ix achieves nearly a 2x improvement throughput (QPS)
improvement at the same 95th percentil latency. \christos{say
  something about each workload once graphs finalized}. The primary
advantage of \ix is that it drops the time spend in the kernel for
TCP/IP processing from 80\% of total time with Linux roughly
\christos{check} 33\%. \christos{anything else worth discussing?}


%\input{eval-mtier}
