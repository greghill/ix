
\subsection{Memcached Performance}
\label{sec:eval:memcached}

\input{figs/float-connscaling}
\input{figs/float-mutilate}

Finally, we evaluated the performance benefits of the \ix protected
dataplane design with \texttt{memcached}, a massively deployed,
in-memory key-value store built on top of the \texttt{libevent}
framework~\cite{url:memcached}. It is frequently used as a
high-throughput, low-latency caching tier in front of persistent
database servers. \texttt{memcached} is a network-bound application,
with threads spending over 80\% of execution time in kernel mode for
network processing~\cite{DBLP:conf/eurosys/LeverichK14}. It is a difficult
application to scale, in particular because the common deployments
involve high connection counts for \texttt{memcached} servers and
small-sized requests and
replies~\cite{DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13,Atikoglu:2012:WAL}

We use the \texttt{mutilate} load-generator to place a selected load
on the server in terms of requests per second (RPS) and measure
response latency~\cite{url:mutilate}. \texttt{mutilate} coordinates a
large number of client threads across multiple machines to generate
the desired RPS load, while a separate unloaded client measures
latency by issuing one request at the time.  We configure
\texttt{mutilate} to generate load representative of two workloads
from Facebook~\cite{Atikoglu:2012:WAL}: the ETC workload that
represents that highest capacity deployment in Facebook, has 20B - 70B
keys, 1B-1KB values, and 75\% GET requests; and the USR workload that
represents deployment with most GET requests in Facebook, has short
keys ($<$20B), 2B values, and 99\% GET requests. In USR, almost all
traffic involves minimum-sized TCP packets. Each request is issued
separately (no \texttt{multiget} operations).


For all experiments, we report average and \data{95}{99}th percentile latency as
a function of the achieved throughput as the latter is is the relevant metric
for datacenter applications~\cite{DBLP:journals/cacm/DeanB13}. This
graph provides insights into the full range of system behaviors. Most
commercial \texttt{memcached} deployments use such a
latency-throughput graph to provision each server so that the 95th \edb{or 99th}
percentile latency does not exceed 200\microsecond to 500\microsecond.
We carefully tune the Linux baseline setup according to the guidelines
in \cite{DBLP:conf/eurosys/LeverichK14}. Specifically, we pin memcached threads,
configure interrupt-distribution based on thread-affinity, and tune
interrupt moderation thresholds. We believe that our baseline Linux
numbers are as tuned as possible for this hardware using the
open-source version of \texttt{memcached-1.4.18}. For our benchmark,
we use \data{6}{18} client machines and a total of \data{772}{229 - CHECK} connections to the
memcached server.  We report the results for the server configuration
that provides the best performance: 8 cores with Linux, but only 6
with \ix.

Porting \texttt{memcached} to IX primarily consisted of adapting it to
use our event library. In most cases, the port was straightforward,
replacing Linux and \texttt{libevent} function calls with their
equivalent versions in our API. \edb{We also replaced {\tt pthread} locks with simple spin-locks.}
We did yet not attempt to tune the internal scalability of {\it
  memcached}~\cite{DBLP:conf/nsdi/FanAK13} or to support zero-copy IO operations.  


\input{tbl-mutilate}

Fig.~\ref{fig:mutilate} shows the throughput-latency curves for the
two \texttt{memcached} workloads for Linux and \ix, while
Table~\ref{tbl:mutilate} reports the unloaded latencies and maximum
query throughput that meets a service-level agreement, both measured
at the \data{95}{99}th percentile.  \ix noticeably reduces the unloaded
latencies.  Note that we use Linux clients for these experiments;
running \ix on clients should further reduce latency.

For both workloads, the distribution of CPU time shifts from being
$>80\%$ in the kernel with Linux to \data{$<30\%$}{$<10\%$} with \ix, and the
throughput increases by \data{2.0}{3.5} and \data{2.7}{4.9} for
\texttt{ETC} and \texttt{USR}, respectively.
The difference is due to
the increased lock contention within the application itself, in
particular for \texttt{ETC}, which has a higher write frequency.


%\input{eval-mtier}

