
\subsection{Memcached Performance}
\label{sec:eval:memcached}

\input{figs/float-connscaling}
\input{figs/float-mutilate}

Finally, we evaluated the performance benefits of the \ix protected
dataplane design with \texttt{memcached}, a massively deployed,
in-memory key-value store built on top of the \texttt{libevent}
framework~\cite{url:memcached}, . It is frequently used as a
high-throughput, low-latency caching tier in front of persistent
database servers. \texttt{memcached} is a networking bound
application, with threads spending over 80\% of execution time in
kernel mode for network processing~\cite{Leverich:RHSU:2014}. It is a
particularly difficult application to scale, in particular because the
common deployments involve high connection counts for
\texttt{memcached} servers and small-sized requests and
replies~\cite{nishtala2013scaling,Atikoglu:2012:WAL}

We use the \texttt{mutilate} load-generator to place a selected load
on the server in terms of requests per second (RPS) and measure
response latency~\cite{url:mutilate}. \texttt{mutilate} coordinates a
large number of client threads across multiple machines to generate
the desired RPS load, while a separate unloaded client measures
latency by issuing one request at the time.  We configure
\texttt{mutilate} to generate load representative of two workloads
from Facebook~\cite{Atikoglu:2012:WAL}: the ETC workload that
represents that highest capacity deployment in Facebook, has 20B - 70B
keys, 1B-1KB values, and 90\% GET requests; and the USR workload that
represents deployment with most GET requests in Facebook, has short
keys ($<$20B), 2B values, and 99\% GET requests. In USR, almost all
traffic involves minimum-sized TCP packets. Each request is issued 

For all experiments, we report 95th percentile latency as a function
of the achieved throughput as this is the relevant metric for
datacenter
applications\microsecond~\cite{DBLP:journals/cacm/DeanB13}. This graph
provides insights the full range of system behaviors. Most commercial
\texttt{memcached} use such a latency-throughput graph to provision
each server so that the 95th percentile latency does not exceed 200 to
500.  We carefully tune the Linux baseline setup according to the
guidelines in \cite{Leverich:RHSU:2014}. Specifically, we pin
memcached threads, configure interrupt-distribution based on
thread-affinity, and tune interrupt moderation thresholds. We believe
that our baseline Linux numbers are as tuned as possible for this
hardware using the open-source version of
\texttt{memcached-1.4.18}. For our benchmark, we use 6 client machines
and a total of 772 connections to the memcached server. We report the
results for the configuration that provides the best performance: 
8 sockets with Linux, but only 6 with \ix.

\christos{Should we discuss the porting process to IX?}

\input{tbl-mutilate}

\edb{NEW with table:} Fig.~\ref{fig:mutilate} shows the throughput-latency curves for the
two \texttt{memcached} workloads for Linux and \ix, while
Table~\ref{tbl:mutilate} reports the unloaded latencies and maximum query throughput that meets a service-level agreement of $<1ms$ at the 95th percentile.
\ix noticeably reduces the unloaded latencies, also measured
at the 95th percentile.  We note here that the benchmark
clients are running on Linux, and that running them on \ix should
further reduce that latency. 

\edb{NEW:}For both workloads, the distribution of CPU time shifts from being
$>80\%$ in the kernel with Linux to $<30\%$ with \ix.  Since the
application is unmodified, Amdahl's law would predict a speedup of 3x.
\ix actually increases the throughput of memcached by 1.8 and 2.7
for \texttt{ETC} and \texttt{USR}, respectively.  We explain the
difference by the increased lock contention within the application
itself, in particular for \texttt{ETC}, which has a higher write frequency.


%\input{eval-mtier}

