

\subsection{Memcached Performance}
\label{sec:eval:memcached}




Finally, we evaluated the performance benefits of \ix with
\texttt{memcached}, a widely deployed, in-memory, key-value store built
on top of the \texttt{libevent} framework~\cite{url:memcached}. It is
frequently used as a high-throughput, low-latency caching tier in
front of persistent database servers. \texttt{memcached} is a
network-bound application, with threads spending over 75\% of
execution time in kernel mode for network
processing~\cite{DBLP:conf/eurosys/LeverichK14}. It is a difficult
application to scale because the common deployments involve high
connection counts for \texttt{memcached} servers and small-sized
requests and
replies~\cite{Atikoglu:2012:WAL,DBLP:conf/nsdi/NishtalaFGKLLMPPSSTV13}.



We use the \texttt{mutilate} load-generator to place a selected load
on the server in terms of requests per second (RPS) and measure
response latency~\cite{url:mutilate}. \texttt{mutilate} coordinates a
large number of client threads across multiple machines to generate
the desired RPS load, while a separate unloaded client measures
latency by issuing one request at the time.  We configure
\texttt{mutilate} to generate load representative of two workloads
from Facebook~\cite{Atikoglu:2012:WAL}: the ETC workload that
represents that highest capacity deployment in Facebook, has 20B--70B
keys, 1B--1KB values, and 75\% GET requests; and the USR workload that
represents deployment with most GET requests in Facebook, has short
keys ($<$20B), 2B values, and 99\% GET requests. In USR, almost all
traffic involves minimum-sized TCP packets. Each request is issued
separately (no \texttt{multiget} operations). However, clients
are permitted to pipeline up to four requests per connection
if needed to keep up with their target request rate. We use 23
client machines to generate load for a total of
1,476 connections to the \texttt{memcached} server.

To provide insights into the full range of system behaviors, we report
average and 99th percentile latency as a function of the achieved
throughput. The 99th percentile latency captures tail latency issues and is
the most relevant metric for datacenter
applications~\cite{DBLP:journals/cacm/DeanB13}. Most commercial
\texttt{memcached} deployments provision each server so that the
99th percentile latency does not exceed 200\microsecond to 500\microsecond.
We carefully tune the Linux baseline setup according to the guidelines
in \cite{DBLP:conf/eurosys/LeverichK14}: we pin memcached threads,
configure interrupt-distribution based on thread-affinity, and tune
interrupt moderation thresholds. We believe that our baseline Linux
numbers are as tuned as possible for this hardware using the
open-source version of \texttt{memcached-1.4.18}. We report the
results for the server configuration that provides the best
performance: 8 cores with Linux, but only 6 with \ix.

\input{tbl-mutilate}

Porting \texttt{memcached} to \ix primarily consisted of adapting it
to use our event library. In most cases, the port was straightforward,
replacing Linux and \texttt{libevent} function calls with their
equivalent versions in our API. We did yet not attempt to tune the
internal scalability of \texttt{memcached}~\cite{DBLP:conf/nsdi/FanAK13}
or to support zero-copy I/O operations.


Fig.~\ref{fig:mutilate} shows the throughput-latency curves for the
two \texttt{memcached} workloads for Linux and \ix, while
Table~\ref{tbl:mutilate} reports the unloaded, roundtrip latencies and
maximum request rate that meets a service-level agreement, both
measured at the 99th percentile.  \ix noticeably reduces the unloaded
latencies to roughly half.  Note that we use Linux clients for these
experiments; running \ix on clients should further reduce latency.

At high request rates, the distribution of CPU time shifts from being
\twiddle~75\% in the Linux kernel to $<10\%$ in the \ix dataplane kernel.
This allows \ix
to increase throughput by 3.3$\times$ and 4.3$\times$ for
\texttt{ETC} and \texttt{USR} respectively at a 500\microsecond tail
latency SLA.  The improvement for \texttt{ETC} is lower due to the
increased lock contention within the application itself, in particular
because it has a higher write frequency.  Lock contention
within application code is also the reason that \ix cannot provide
throughput improvements with more than 6 cores.


%\input{eval-mtier}

