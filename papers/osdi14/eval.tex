
\section{Evaluation}
\label{sec:eval}

% \christos{todo list: a) study of adaptive batching (fixed with various
%   sizes vs adaptive), b) some breakdown of processing before/after to
%   show how things change with IX, c) fix FIN/RST issue in our
%   microbenchmark}

We compared \ix to a baseline running the most recent Linux kernel and
to mTCP~\cite{jeong2014mtcp}. Our evaluation uses both networking
microbenchmarks and a widely deployed, event-based application. In all
cases, we use TCP as the networking protocol.

\subsection{Experimental Methodology}
\label{sec:eval:setup}

\input{figs/float-pingpong} 

Our experimental setup consists of a cluster of 18 clients and one
server connected by a Quanta/Cumulus 48x10GbE switch with a Broadcom
Trident+ ASIC.  The client machines are a mix of Xeon E5-2637 @ 3.5
Ghz and Xeon E5-2650 @ 2.6 Ghz. The server is a Xeon E5-2665 @ 2.4 Ghz
with 256 GB of DRAM.  Each client and server socket has 8 cores and 16
hyperthreads. All machines are configured with Intel x520 10GbE NICs
(82599EB chipset). We connect clients to the switch through a single
NIC port, while for the server it depends on the experiment. For
\emph{10GbE} experiments we use a single NIC port, and for
\emph{4x10GbE} experiments we use four NIC ports bonded by the switch
with a L3+L4 hash.

Our baseline configuration in each
machine is an Ubuntu LTS 14.0.4 distribution, updated to the 3.16.1 Linux kernel, the most
recent at time of writing.
% When reporting multi-core scalability, we report results on a
% per-core basis.
We enable hyperthreading when it improves performance. Except for
\S\ref{sec:eval:netpipe}, client machines always run Linux. All power
management features are disabled for all systems in all
experiments. Jumbo frames are never enabled. All Linux workloads
are pinned to hardware threads, to avoid scheduling jitter, and
background tasks are disabled.


The Linux client and server implementations of our benchmarks use the
\texttt{libevent} framework with the \texttt{epoll} system call.  We
downloaded and installed mTCP from the public-domain
release~\cite{url:mtcp}, but had to write the benchmarks ourselves
using the mTCP API.  We run mTCP with the 2.6.36 Linux kernel, as this
is the most recent supported kernel version.  We report only 10GbE
results for mTCP, as it does not support NIC bonding. For \ix, we
bound the maximum batch size to $B=64$ packets per iteration (see
\S\ref{sec:disc}).



\subsection{Latency and Single-flow Bandwidth}
\label{sec:eval:netpipe}

We first evaluated the latency of \ix using NetPIPE, a popular
ping-pong benchmark, using our 10GbE setup.  NetPIPE simply exchanges
a fixed-size message between two servers and helps calibrate the
latency and bandwidth of a single flow~\cite{snell1996netpipe}.  In
all cases, we run the same system (Linux, mTCP, or \ix) on both ends.

Fig.~\ref{fig:pingpong} shows the goodput achieved for different
message sizes.  Two \ix servers have a one-way latency of
5.7\microsecond for 64B messages and achieve goodput of 5Gbps, half of
the maximum, with messages as small as 20KB. In contrast, two Linux
servers have a one-way latency of 24\microsecond and require 385KB
messages to achieve 5Gpbs.  The differences in system architecture
explain the disparity: \ix has a dataplane model that polls queues and
processes packets to completion whereas Linux has an interrupt model,
which wakes up the blocked process.  mTCP uses aggressive batching to
offset the cost of context switching~\cite{jeong2014mtcp}, which comes
  at the expense of higher latency than both \ix and Linux in this
  particular test.

 
\input{figs/float-short-both}

  \subsection{Throughput and Scalability}
\label{sec:eval:short}

Next, we evaluate \ix's throughput and multi-core scalability using
the same benchmark used to evaluate
MegaPipe~\cite{DBLP:conf/osdi/HanMCR12} and
mTCP~\cite{jeong2014mtcp}. Multiple clients connect to a single server
listening on a single port, send a request of size $s$, and wait for
an echo of a message of the same size.  As with NetPIPE, while
receiving the message, the server holds off its echo response until
the message has been entirely received.  Each client performs this
synchronous remote procedure call $n$ times before closing the
connection.  As in ~\cite{jeong2014mtcp}, clients close the connection
using a reset (\texttt{TCP RST}) to avoid exhausting ephemeral ports.
%\dm{This begs the question of what happens with FIN, since that is
%  more realistic\@.  Is the point that \ix has an unfair advantage?}
%\adam{(1) how do we get linux clients to send TCP RST? (2) Does the
%  fact that IX will only close with TCP RST matter and why is this?}


%%
%% RAW DATA - 1024 message per connection
%% IX:  7074246
%% mTCP: 3253521

Fig.~\ref{fig:shortboth} shows the message
rate or goodput for both the 10GbE and the 40GbE configurations as we
vary the number of cores used, the number of roundtrip messages per
connection, and the message size respectively.  For the 10GbE
configuration, the results for Linux and mTCP are consistent with
those published in the mTCP paper~\cite{jeong2014mtcp}.  For all three
tests (core scaling, message count scaling, message size scaling), \ix
scales more aggressively than mTCP and
Linux. Fig.~\ref{fig:short10:mcore} shows that \ix needs only 3 cores
to saturate the 10GbE link whereas mTCP requires all 8 cores. On
Fig.~\ref{fig:short10:roundtrips} for 1024 roundtrips per connection,
\ix delivers 8.8 million messages per second, which is
1.9x the throughput of mTCP and of and 8.8x that of
Linux. With this packet rate, \ix achieves line rate and is limited
only by 10GbE bandwidth.

%% PROOF of line rate:
% 10 * 10 ^ 9 bits  / (12 interframe gap + 7 preamble + 1 start frame delimiter +
%		       14 ethernet hdr + 20 IP hdr + 20 TCP hdr + 64 payload + 4 CRC) bytes
% = 8 802 816.9 requests / second

Fig.~\ref{fig:shortboth} also shows that \ix
scales well beyond 10GbE to a 4x10GbE configuration.
Fig.~\ref{fig:short10:mcore} shows that \ix linearly scales to deliver
3.8 million TCP connections per second on 4x10GbE.
Fig.~\ref{fig:short10:roundtrips} shows a speedup of 2.3x with $n=1$
and of 1.3x with $n=1024$ over 10GbE \ix.  Finally,
Fig.~\ref{fig:short10:size} shows \ix can deliver 8KB messages with a
goodput of 34.5 Gbps, for a wire throughput of
37.9 Gpbs, out of a possible 39.7Gbps.  Overall, \ix makes
it practical to scale protected TCP/IP processing beyond 10GbE, even
with a single socket multi-core server.

\input{figs/float-connscaling}

\subsection{Connection Scalability}

\label{sec:eval:scale}

We also evaluate \ix's scalability when handling a large number of
concurrent connections on the 4x10GbE setup. Each of the 18 client
machines runs $n$ threads, with each thread repeatedly performing a
64B remote procedure call to the server with a variable number of
active connections. % \adam{Should we emphasize that all connections are
% active not just open?}
We experimentally set $n=24$ to maximize
throughput.  We report the maximal throughput in messages per second
for a range of total established connections.

%Connection scaling IX/Linux @ 40Gbe 
%12036405.031689
%915256.419191
%/p
%13.15085

\input{figs/float-mutilate}

Fig.~\ref{fig:connscaling} shows up to 250,000
connections, which is the upper bound we can reach with the available
client machines.  As expected, throughput increases with the degree of
connection concurrency, but then decreases for very large connections
counts due to the increasingly high cost of multiplexing among open
connections.  At the peak, \ix performs 10x better than
Linux, consistent with the results from
Fig.~\ref{fig:short10:roundtrips}.

With 250,000 connections and 4x10GbE, \ix is able to deliver
47\% of its own peak throughput.  We verified that the drop
in throughput is not due to an increase in the instruction count, but
instead can be attributed to the performance of the memory
subsystem. Intel's Data Direct I/O technology, an evolution of
DCA~\cite{DBLP:conf/isca/HuggahalliIT05}, eliminates nearly all cache
misses associated with DMA transfers when given enough time between
polling intervals, resulting in as little as 1.4 L3 cache misses per
message for up to 10,000 concurrent connections.  In contrast, the
workload averages 25 L3 cache misses per message when handling 250,000
concurrent connections.  At high connection counts, the working set of
this workload is dominated by the TCP connection state and does not
fit into the processor's L3 cache.  Nevertheless, we believe that
further optimizations in the size and access pattern of lwIP's TCP/IP
protocol control block structures can substantially reduce this
handicap.
  


\input{eval-memcache}
