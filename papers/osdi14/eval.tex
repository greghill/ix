s
\section{Evaluation}
\label{sec:eval}

We evaluate the scalability of \ix and compare it to the Linux baseline
and to mTCP, the state-of-the-art user-space TCP alternative available
on the same hardware.  We first describe the experimental setup
(\S\ref{sec:eval:setup}).  We then characterize the performance using
a series of micro-benchmark: the popular NetPIPE ping-pong
test~\cite{snell1996netpipe} (\S\ref{sec:eval:netpipe}); the short
message transaction test previously used to evaluate
MegaPipe~\cite{han2012megapipe} and mTCP~\cite{jeong2014mtcp} in
\S\ref{sec:eval:short}; and the scalability of the system for large
persistent connections (\S\ref{sec:eval:scale}).  Finally, we
characterize the performance of the \ix system by running real-world
applications such as memcached and XXX using the
\texttt{libevent-}compatibility library(\S\ref{missing}).


\subsection{Experimental setup}
\label{sec:eval:setup}

\edb{Describes EPFL setup:}
Our experimental setup consists of a cluster of 19 clients and one
server connected by an HP 5820AF-24XG 10GbE switch.  The client
machines are a mix of dual Xenon E5-2637 running at 3.5 Ghz and single Xenon E5-2650 running at 2.6 Ghz with a single, Intel
82599EB 10GbE NIC connecting to the switch.  The server is a dual-Xeon E5-2665
running at 2.4 Ghz with 256 GB of DRAM with four Intel 2P X520 NICs.  Each
socket (client or server) has 8 cores and 16 hyperthreads.  When
reporting multi-core scalability, we report results with
hyperthreading enabled, and both hyperthreads of the cores are used.

For our experiments, the server is connected to
the switch via either a single 10GbE link or via a 4x10GbE bond configured as a L2+L3
hash via LACP~\cite{ieee802.3ad}. Jumbo
frames are never enabled.  Our baseline configuration in each machine
is the Ubuntu 12.04.4 LTS distribution running the 3.11.10 Linux kernel.  Although
the server has two sockets, our experiments pin IRQs\george{not really}, processes, and
the IX dataplane onto a single socket and use memory from that socket;
all NICs are attached to that socket via the PCIe root complex. 

\edb{CONTREVERSIAL: We chose to limit execution to a single NUMA node to simplify the
  analysis because of the asymmetrical nature of our hardware, to
  model the experimental setup after the one used in recent related
  work~\cite{jeong2014mtcp}, and also as we are practically never CPU-bound in our tests}

%\christos{We should organize the eval in the following way: first
%  micro-benchmarks that showcase each of the advantages (high packet
%  rate, low latency/jitter, high connection/churn). The title of the
%  subsection should indicate the aspect evaluated. Then there should
%  a subsection on full app eval (memcache hopefully).}

Whenever both practical and relevant, we compare the performance of an
\ix server with a baseline out-of-the-box Linux configuration
((\texttt{Linux-base}), an optimized configuration of Linux
(\texttt{Linux-opt}), and the performance of mTCP, a
recently-published user-level TCP stack specifically designed for
scalability~\cite{jeong2014mtcp}.  Unless specifically mentioned, all
clients run a baseline Linux configuration. 

The Linux server implementations use the \texttt{libevent} framework
running on XXX threads to serve the clients. The mTCP server
implementation use the native mTCP API running on up to 8 cores
(within each core, one hyperthread is dedicated to processing the
network stack).  We downloaded and installed mTCP
from the public-domain implementation~\cite{url:mtcp}, but re-wrote the
benchmarks as they were not part of the distribution.  To run mTCP, we
switch to a kernel-2.6.36, as this is the most recent supported kernel
version.
Finally, \ix server use
the native \ix API running on up to 8 cores (using symmetrically both
hyperthreads).  For all experiments, we use the same Linux client code, generally also written using \texttt{libevent}.


\subsection{High-bandwidth and low-latency}
\label{sec:eval:netpipe}

\input{figs/float-pingpong}
\input{tbl-pingpong}

We start the evaluation with the popular NetPIPE ping-pong
benchmark~\cite{snell1996netpipe} that exchanges a fixed-size message
between two servers.  The benchmark is used to calibrate the latency
and single-flow bandwidth of a communication link: with minimal
message size (64B), this benchmark determines the one-way latency for
short message communication.  With increasingly larger messages, it
determines how quickly this single ping-pong behavior can utilize 50\%
of the available bandwidth on the link.

Fig.~\ref{fig:pingpong} compares the performance of various
configurations for different message sizes.  As expected, the
bandwidth increases with the message size, and rises to half of the
maximum bandwidth with messages as small as XXXKB for \ix-\ix.  At the
other end of the spectrum, two Linux machines need to exchange
messages of at least XXXKB to reach the same bandwdith.  mTCP behaves
\edb{TODO}.

Table~\ref{tbl:pingpong} lists the average and 99\% latencies for 64B
messages for the same benchmark.  The impact of the \ix dataplane
model is evident: for the hardware, the one-way latency between \ix
systems is as low as 7.3\microsecond on our hardware\footnote{Our
HP switch is ``well-amortized'' by 2014, which likely impacts
  performance; we performed the same benchmark on a low-latency Arista
  switch and measured 5.6\microsecond between a pair of comparable
  (but not identical servers). We have a low-latency switch on order
  that will be used for the results of the camera-ready.}.  In
contrast, the software overhead of Linux add XXX\microsecond over \ix.
mTCP ... \edb{TODO}


\subsection{Handling high TCP connection churn}
\label{sec:eval:short}

\input{figs/float-short}

We then evaluate \ix's multi-core scalability in handling workloads
with extreme connection churn by using the same benchmark used to evaluate
Megapipe~\cite{han2012megapipe} and mTCP~\cite{jeong2014mtcp}:
multiple clients connect to a single server listening on a single
port, send a request of size $s$ and wait for an echo of a message of
the same size.  As with NetPIPE, while accepting the message, the server holds off its
echo response until the message has been entirely received.
Each client performs this synchronous remote procedure
call $n$ times, and then close the connection using a reset
(\texttt{TCP RST}).


\edb{PLACEHOLDER:} Fig.~\ref{fig:short} shows the results for both
10GbE and 4x10GbE hardware configurations.  We first
are able to confirm and reproduce the mTCP performance published in
~\cite{jeong2014mtcp}.  On this benchmark, \ix generally scales XXx
better than mTCP on a per core basis and XXx better than Linux.  In
Fig.~\ref{fig:short:mcore}, we see that \ix can saturate the single
10Gbpe wire using only 3 cores; in contrast mTCP requires all 8
cores. \ix can further scale to nearly XX\% of the wire limit on a
4x10GbE configuration, and using a single socket.
Fig.~\ref{fig:short:roundtrips}, which uses 8 cores, similarly shows
that \ix can nearly saturate the wire on the single-link
configuration, but remains CPU-bound on the 4x10 configuration.
Finally, Fig.~\ref{fig:short:size} shows that only \ix can saturate
4x10Gbps with 4KB messages, and only \ix and mTCP can saturate the
single-link configuration.


\subsection{Connection Scalability}

\input{figs/float-connscaling}
\label{sec:eval:scale}

We now evaluate \ix's scalability when handling a large number of
concurrent connections. In this benchmark, each client machine runs
$n$ threads, with each thread independently performing a 64B remote
procedure call to the server.  Each RPC randomly chooses among $m$
open sockets to perform the RPC.  In our setup with 18 clients, the
server must multiplex among $18 \times n \times m$ open connections.
We experimentally set $n \leq 24$ to maximize throughput.  The 19th
client performs RPC at a low rate on a single connection to measure
latency.  We report the throughput in message per second and the
round-trip latency for varying number of total established connections.

Fig.~\ref{fig:connscaling} shows the results.  As expected, throughput
increases with the degree of concurrency, but then decreases with
increasing number of connections due to the increasingly high cost of
multiplexing among open connections.  At the peak, \ix performs XXx
better than Linux, consistent with the results from
Fig.~\ref{fig:short:roundtrips}.  With 100,000 connections, \ix is
able to deliver XX\% of its peak throughput, whereas Linux only
delivers XXX\% of its own peak throughput\footnote{Unexplained
  performance issues with our client configuration setup limit our
  experiments to 100,000 connections at the time of submission. \ix is
  designed to scale to millions of concurrent connections.}


\subsection{Application performance -- memcache}
\label{sec:eval:memcache}

\input{figs/float-mutilate}

We now validate the performance benefits of the protected dataplane
design on memcached -- a real,world, widely deployed, in-memory
key-value store built on top of the libevent framework.  We use the
mutilate~\cite{url:mutilate} load-generator and tool to measure
average and 99\% latency as a function of the throughput delivered by
the server~\cite{Leverich:RHSU:2014}.

Fig.~\ref{fig:mutilate} reports the latencies as a function of the
number of queries per second.  The peak QPS is measured using \ix at
XXX, which maps to a bandwidth leaving the server of XXX Gbps, or XX\%
of the theoretical wire rate of the machine.  We report the nubmers
for Linux and \ix for XXX and YYY established, concurrent
connections served by the 18 clients.

Our results show that \edb{TODO}.....

\subsection{Web serving performance}


\todo Benchmark: lightttpd -- used by Affinity-Accept and mTCP.  

\todo ngnx: an actually used webserver.


%%%%%
%%%%% very optimistic
%%%%%

%\input{eval-mtier}
