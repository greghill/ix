
\section{Evaluation}
\label{sec:eval}

We evaluate the scalability of \ix and compare it the Linux baseline
and to mTCP, the state-of-the-art user-space TCP alternative available
on the same hardware.  We first describe the experimental setup
(\S\ref{sec:eval:setup}).  We then characterize the performance using
a series of micro-benchmark: the popular NetPIPE ping-pong
test~\cite{snell1996netpipe} in \S\ref{sec:eval:netpipe}; the short
message transaction test previously used to evaluate
MegaPipe~\cite{han2012megapipe} and mTCP~\cite{jeong2014mtcp} in
\S\ref{sec:eval:short}; and the scalability of the system for large
persistent connections in \S\ref{sec:eval:scale}.  Finally, we
characterize the performance of the \ix system by running real-world
applications such as memcached and XXX using the
\texttt{libevent-}compatibility library.


\subsection{Experimental setup}
\label{sec:eval:setup}

\edb{Describes EPFL setup:}

Our experimental setup consists of a cluster of 19 clients and one
server connected by an HP 5820AF-24XG low-latency 10GbE switch.  The client
machines are a mix of dual Xenon E5-2637 running at 3.5 Ghz and single Xenon E5-2650 running at 2.6 Ghz with a single, Intel
82599EB 10GbE NIC connecting to the switch.  The server is a dual-Xeon E5-2665
running at 2.4 Ghz with 256 GB of DRAM with four Intel 2P X520 NICs.  Each
socket (client or server) has 8 cores and 16 hyperthreads.  When
reporting multi-core scalability, we report results with
hyperthreading enabled, and both hyperthreads of the cores are used.

The server is connected to
the switch via a 4x10GbE bond configured in as a L2+L3
hash~\cite{missing} in the switch via LACP~\cite{ieee802.3ad}.  Jumbo
frames are never enabled.  Our baseline configuration in each machine
is the Ubuntu 12.04.4 LTS distribution running the 3.11 Linux kernel.  Although
the server has two sockets, our experiments pin IRQs, processes, and
the IX dataplane onto a single socket and use memory from that socket;
all NICs are attached to that socket via the PCIe root complex.

\christos{We should organize the eval in the following way: first
  micro-benchmarks that showcase each of the advantages (high packet
  rate, low latency/jitter, high connection/churn). The title of the
  subsection should indicate the aspect evaluated. Then there should
  a subsection on full app eval (memcache hopefully).}

\subsection{Single stream ping-pong (NetPIPE)}
\label{sec:eval:netpipe}

\input{figs/float-pingpong}
\input{tbl-pingpong}

Fig.~\ref{fig:pingpong} illustrates the importance of system software
design and configuration tradeoffs using the NetPIPE ping-pong
benchmark~\cite{snell1996netpipe} that exchanges a fixed-size message
between two servers.  Although the application is trivial, it provides
a good proxy for the effective performance of synchronous remote
procedure calls in uncontended situations.  We compare the performance
of (i) an out-of-the-box Ubuntu 12.04.4 LTS configuration
(\texttt{Linux-base}) running an unmodified NetPIPE v3.7.2; (ii) a
tuned configuration of the same kernel (\texttt{Linux-opt})) with the
same application; (iii) between two mTCP-based equivalent
applications; (iv) a hybrid version in which a Linux client talking to
an \ix server; and finally (v) between two \ix systems.  For the same
application and hardware configuration (described in
\S\ref{sec:eval:setup}), the differences are dramatic.  We see that
\ix-\ix achieves 50\% of the asymptotic bandwdith with messages as small
as \george{XXX bytes}.  In constrast, both Linux configurations remain
latency-bound.  Although mTCP, ....


Table~\ref{tbl:pingpong} lists the average and 99\% percentile half
round-trip latencies for small messages (64B), using the same netpipe
benchmark.  \edb{TODO - COMMENTARY.}

\subsection{Handling short TCP transactions}
\label{sec:eval:short}

\input{figs/float-short}

We first evaluate \ix's scalability with a benchmark that stresses
connection churn in server, as used to evaluate
Megapipe~\cite{han2012megapipe} and mTCP~\cite{jeong2014mtcp}:
multiple clients connect to a single server listeningo on a single
port, send a request of size $s$ and wait for an echo of a message of
the same size.  As with netpipe, while accepting the message, the server holds off its
echo response until the message has been entirely received.
Each client performs this synchronous remote procedure
call $n$ times, and then close the connection using a reset
(\texttt{TCP RST}).

Fig.~\ref{fig:short} shows the results. XXX


subsection{Connection Scalability}

\input{figs/float-connscaling}
\label{sec:eval:scale}

We then evaluate \ix's scalabilty when handling a large number of concurrent connections. 

XXX


\subsection{TODO}

\todo Compare apples-to-apples with Megapipe, and mTCP in terms of microbenchmarks.

\todo Baseline comparison of state-of-the art systems include:  Linux (some recent version, with SO\_REUSEPORT); Megapipe (if possible), mTCP (if possible). 

\todo Microbenchmark: short TCP transactions (echo server, as defined in megapipe).   Goal is to beat mTCP (and therefore all others) hands down.

\todo Benchmark: memcached - compare with FB results (?)

\todo Benchmark: lightttpd -- used by Affinity-Accept and mTCP.  

\todo ngnx: an actually used webserver.

