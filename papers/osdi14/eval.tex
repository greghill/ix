
\section{Evaluation}
\label{sec:eval}

We evaluate the scalability of \ix and compare it to the Linux
baseline running a 3.11.10 kernel, and whenever possible to mTCP.  We
selected mTCP as it is the state-of-the-art user-space TCP alternative
available on the same hardware~\cite{jeong2014mtcp}.  We first
describe the experimental setup (\S\ref{sec:eval:setup}).  We then
characterize the performance using a series of micro-benchmark against
a server with 10GbE of connectivity: the popular NetPIPE ping-pong
test~\cite{snell1996netpipe} (\S\ref{sec:eval:netpipe}) and the short
message transaction test previously used to evaluate MegaPipe and mTCP
in~\cite{han2012megapipe,jeong2014mtcp} (\S\ref{sec:eval:short}).  We
then scale the latter test to a server with 4x10GbE of connectivity
and use that infrastructure to characterize \ix's scalability in handling large persistent connections
(\S\ref{sec:eval:scale}).  Finally, we characterize the performance of
the \ix system by running memcached -- a real-world massively deployed
application (\S\ref{sec:eval:memcached}).


\subsection{Experimental setup}
\label{sec:eval:setup}

Our experimental setup is complicated by the fact that we wish to
simulanteously compare with mTCP and scale to bandwidth beyond 10GbE.
Unfortunately, our 4x10GbE setup uses more recent hardware that is
sufficiently different in terms of CPU and NIC that porting mTCP and
getting good performance became a challenge\footnote{We will unify our testing infrastructure by the deadline.}

consists of a cluster of 19 clients and one
server connected by an HP 5820AF-24XG 10GbE switch.  The client
machines are a mix of dual Xenon E5-2637 running at 3.5 Ghz and single Xenon E5-2650 running at 2.6 Ghz with a single, Intel
82599EB 10GbE NIC connecting to the switch.  The server is a dual-Xeon E5-2665
running at 2.4 Ghz with 256 GB of DRAM with four Intel 2P X520 NICs.  Each
socket (client or server) has 8 cores and 16 hyperthreads.  When
reporting multi-core scalability, we report results with
hyperthreading enabled, and both hyperthreads of the cores are used.

For our experiments, the server is connected to the switch via either
a single 10GbE link or via a 4x10GbE bond configured as a L2+L3 hash.
Jumbo frames are never enabled.  Our
baseline configuration in each machine is the Ubuntu 12.04.4 LTS
distribution running the 3.11.10 Linux kernel.  Although the server
has two sockets, our experiments use only the CPU and memory resources
of the socket to which the NIC are attached.  
\edb{CONTREVERSIAL: We made this deliberate decision to eliminate NUMA
  imbalances from our experiments, and to most closely reproduce the
  setup used in ~\cite{jeong2014mtcp}}

%\christos{We should organize the eval in the following way: first
%  micro-benchmarks that showcase each of the advantages (high packet
%  rate, low latency/jitter, high connection/churn). The title of the
%  subsection should indicate the aspect evaluated. Then there should
%  a subsection on full app eval (memcache hopefully).}

\input{figs/float-pingpong}
%\input{tbl-pingpong}

\input{figs/float-short10}
\input{figs/float-short40}

Whenever both practical and relevant, we compare the performance of an
\ix server with both the baseline Linux configuration and with mTCP, a
recently-published user-level TCP stack specifically designed for
scalability~\cite{jeong2014mtcp}.  Unless specifically mentioned, all
clients run the same Linux configuration. 

The Linux server implementations use the \texttt{libevent} framework
running on XXX threads to serve the clients, and the \texttt{epoll} system call. The mTCP server
implementation use the native mTCP API running on up to 8 cores
(within each core, one hyperthread is dedicated to processing the
network stack).  We downloaded and installed mTCP
from the public-domain implementation~\cite{url:mtcp}, but re-wrote the
benchmarks as they were not part of the distribution.  To run mTCP, we
switch to a kernel-2.6.36, as this is the most recent supported kernel
version.
Finally, \ix server use
the native \ix API running on up to 8 cores (using symmetrically both
hyperthreads).  For all experiments, we use the same Linux client code, generally also written using \texttt{libevent}.


\subsection{High-bandwidth and low-latency}
\label{sec:eval:netpipe}


We start the evaluation with the popular NetPIPE ping-pong
benchmark~\cite{snell1996netpipe} that exchanges a fixed-size message
between two servers.  The benchmark is used to calibrate the latency
and single-flow bandwidth of a communication link: with minimal
message size (64B), this benchmark determines the one-way latency for
short message communication.  With increasingly larger messages, it
determines how quickly this single ping-pong behavior can utilize 50\%
of the available bandwidth on the link.


Fig.~\ref{fig:pingpong} compares the performance of various
configurations for different message sizes.  As expected, the
bandwidth increases with the message size, and rises to half of the
maximum bandwidth with messages as small as 32KB for \ix-\ix.  In
constrast, both Linux-Linux and mTCP-mTCP lag and only reach the
half-bandwidth mark with messages sizes of 384KB and 1MB,
respectively.  

The difference is also noticeable with small message sizes, in
particular with 64B payloads.  The one-way latency is 6.8\microsecond
between two machines running \ix, 24\microsecond if they are running
Linux, and 87\microsecond if they are running mTCP.  The
difference in system architecture help explain such a dramatic
difference: \ix has a dataplane model that polls queues and runs them
to completion; Linux has an interrupt model, which wakes up the
blocked process.  The mTCP authors mention high context switching
overheads between the various threads in their architecture, and the
need for aggressive batching~\cite{jeong2014mtcp}, which helps explain
why it is such a challenging benchark for that system\footnote{Our HP
  switch is well-amortized by now, which likely impacts performance;
  we performed the same benchmark on a current, low-latency Arista
  switch and measured XXX 5.6\microsecond between a pair of comparable
  (but not identical) servers both running \ix. We have a low-latency
  switch on order that will be used for the results of the
  camera-ready.}.

\subsection{Handling high TCP connection churn}
\label{sec:eval:short}


We then evaluate \ix's multi-core scalability in handling workloads
with extreme connection churn by using the same benchmark used to evaluate
Megapipe~\cite{han2012megapipe} and mTCP~\cite{jeong2014mtcp}:
multiple clients connect to a single server listening on a single
port, send a request of size $s$ and wait for an echo of a message of
the same size.  As with NetPIPE, while accepting the message, the server holds off its
echo response until the message has been entirely received.
Each client performs this synchronous remote procedure
call $n$ times, and then close the connection using a reset
(\texttt{TCP RST}).


\edb{PLACEHOLDER:} Fig.~\ref{fig:short} shows the results for both
10GbE and 4x10GbE hardware configurations.  We first
are able to confirm and reproduce the mTCP performance published in
~\cite{jeong2014mtcp}.  On this benchmark, \ix generally scales XXx
better than mTCP on a per core basis and XXx better than Linux.  In
Fig.~\ref{fig:short:mcore}, we see that \ix can saturate the single
10Gbpe wire using only 3 cores; in contrast mTCP requires all 8
cores. \ix can further scale to nearly XX\% of the wire limit on a
4x10GbE configuration, and using a single socket.
Fig.~\ref{fig:short:roundtrips}, which uses 8 cores, similarly shows
that \ix can nearly saturate the wire on the single-link
configuration, but remains CPU-bound on the 4x10 configuration.
Finally, Fig.~\ref{fig:short:size} shows that only \ix can saturate
4x10Gbps with 4KB messages, and only \ix and mTCP can saturate the
single-link configuration.


\subsection{Connection Scalability}
\input{figs/float-connscaling}

\label{sec:eval:scale}

We now evaluate \ix's scalability when handling a large number of
concurrent connections. In this benchmark, each client machine runs
$n$ threads, with each thread independently performing a 64B remote
procedure call to the server.  Each RPC randomly chooses among $m$
open sockets to perform the RPC.  In our setup with 19 clients, the
server must multiplex among $19 \times n \times m$ open connections.
We experimentally set $n \leq 24$ to maximize throughput.  We report
the maximal throughput in messages per second for varying number of
total established connections.

Fig.~\ref{fig:connscaling} shows the results.  As expected, throughput
increases with the degree of concurrency, but then decreases with
increasing number of connections due to the increasingly high cost of
multiplexing among open connections.  At the peak, \ix performs XXx
better than Linux, consistent with the results from
Fig.~\ref{fig:short:roundtrips}.  With 100,000 connections, \ix is
able to deliver XX\% of its peak throughput, whereas Linux only
delivers XXX\% of its own peak throughput\footnote{Unexplained
  performance issues with our client configuration setup limit our
  experiments to 100,000 connections at the time of submission. \ix is
  designed to scale to millions of concurrent connections.}


\subsection{Application performance -- memcache}
\label{sec:eval:memcached}

\input{figs/float-mutilate}

We now validate the performance benefits of the protected dataplane
design on memcached -- a real world, massively deployed, in-memory
key-value store built on top of the libevent framework.  We use the
mutilate~\cite{url:mutilate} load-generator and tool to measure
average and 99\% latency as a function of the throughput delivered by
the server~\cite{Leverich:RHSU:2014}.

Fig.~\ref{fig:mutilate} reports the latencies as a function of the
number of queries per second.  The peak QPS is measured using \ix at
XXX, which maps to a bandwidth leaving the server of XXX Gbps, or XX\%
of the theoretical wire rate of the machine.  We report the numbers
for Linux and \ix for XXX and YYY established, concurrent
connections served by the 18 clients.

Our results show that \edb{TODO}.....

\subsection{Web serving performance}


\todo Benchmark: lightttpd -- used by Affinity-Accept and mTCP.  

\todo ngnx: an actually used webserver.


%%%%%
%%%%% very optimistic
%%%%%

%\input{eval-mtier}
