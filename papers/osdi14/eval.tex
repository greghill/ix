
\section{Evaluation}
\label{sec:eval}

We compared \ix to a Linux baseline running a 3.11.10 kernel and to
mTCP~\cite{jeong2014mtcp}. Our evaluation uses both networking
microbenchmarks and a real-world, massively deployed, event-based
application.

\subsection{Experimental Methodology}
\label{sec:eval:setup}

We use two experimental setups because we wish to simultaneously
compare with mTCP and scale to bandwidth beyond 10GbE.  Unfortunately,
our 4x10GbE setup uses recent hardware not supported by mTCP out of
the box\footnote{We were able to port mTCP to the 4x10GbE setup, but
  could not reach the expected performance.  We will attempt to unify
  the setup by the camera-ready deadline.}.  Linux and \ix run on both
setups.

\input{figs/float-pingpong} \myparagraph{10GbE setup:} This setup
consists of a cluster of 28 Xeon E5-2630 @ 2.3 Ghz machines with Intel
82599EB 10GbE and Solarflare SFC9000 10GbE NICs, configured with 64 GB
of RAM each. They are connected with an Arista 7050S-64 switch. One
machine with an Intel NIC is used as the server, while the rest are
used as clients. The server socket has 6 cores and 12 hyperthreads.

\input{figs/float-short-both}

\myparagraph{4x10GbE setup:} This setup consists of a cluster of 18
clients and one server connected by an HP 5820AF-24XG 10GbE switch.
The client machines are a mix of Xeon E5-2637 @ 3.5 Ghz and Xeon
E5-2650 @ 2.6 Ghz.  Each client has a single 82599EB 10GbE NIC
connecting to the switch.  The server is a Xeon E5-2665 @ 2.4 Ghz with
256 GB of DRAM and four Intel 2P X520 NICs.  Each client or server
socket has 8 cores and 16 hyperthreads.  The server is connected
to the switch via a 4x10GbE bond configured as a L2+L3 hash.

\myparagraph{Common elements:} Our baseline configuration in each
machine is the Ubuntu 12.04.4 LTS distribution running the 3.11.10
Linux kernel.  When reporting multi-core scalability, we report
results on a per-core basis. We enable hyperthreading when it improves performance. Except for
\S\ref{sec:eval:netpipe}, client machines always run Linux. All power
management features are disabled for all systems in all
experiments. Jumbo frames are never enabled.


The Linux client and server implementations of our benchmarks use the
\texttt{libevent} framework with the \texttt{epoll} system call.  We
downloaded and installed mTCP from the public-domain
release~\cite{url:mtcp}, but had to write the benchmarks ourselves
using the mTCP API.  To run mTCP, we switch to the 2.6.36 kernel, as
this is the most recent supported kernel version.


\subsection{Latency and Single-flow Bandwidth}
\label{sec:eval:netpipe}

We first evaluated the latency of \ix using NetPIPE, a popular
ping-pong benchmark, using our 10GbE setup.  NetPIPE simply exchanges
a fixed-size message between two servers and helps calibrate the
latency and bandwidth of a single-flow~\cite{snell1996netpipe}.  In
all cases, the two servers use the same system (Linux, mTCP, or \ix).


Fig.~\ref{fig:pingpong} shows the goodput achieved for different
message sizes.  Two \ix servers achieve goodput of 5Gbps (half of the
maximum) with messages as small as 20KB. In contrast, Linux requires
384KB messages to achieve 5Gbps, while mTCP requires messages larger
than 500KB. The goodput difference between the three systems is most
dramatic for small messages. Using 64B messages, the one-way unloaded
latency is 6.9\microsecond between two servers running \ix,
21\microsecond if they are running Linux, and 95\microsecond if they
are running mTCP.  The differences in system architecture explain the
dramatic gap. \ix has a dataplane model that polls queues and
processes packets to completion. Linux has an interrupt model, which
wakes up the blocked process. mTCP uses aggressive batching to offset
the cost of context switching~\cite{jeong2014mtcp}.



\subsection{Scalability and High Connection Churn}
\label{sec:eval:short}



Next, we evaluate \ix's multi-core scalability in handling workloads
with extreme connection churn by using the same benchmark used to
evaluate MegaPipe~\cite{DBLP:conf/osdi/HanMCR12} and
mTCP~\cite{jeong2014mtcp}. Multiple clients connect to a single server
listening on a single port, send a request of size $s$ and wait for an
echo of a message of the same size.  As with NetPIPE, while accepting
the message, the server holds off its echo response until the message
has been entirely received.  Each client performs this synchronous
remote procedure call $n$ times, and then closes the connection using a
reset (\texttt{TCP RST}).  \dm{This begs the question of what happens
  with FIN, since that is more realistic\@.  Is the point that \ix has
an unfair advantage?}


%%
%% RAW DATA - 1024 message per connection
%% IX:  7074246
%% mTCP: 3253521

\myparagraph{10GbE results:}
Figs.~\ref{fig:short10:mcore}-\ref{fig:short10:size} show the results
on the 10GbE setup.  For Linux and mTCP, our results are consistent
with the ones published in the mTCP paper~\cite{jeong2014mtcp}.  For
all three tests (core scaling, message count scaling, message size
scaling), \ix scales more aggressively and is quickly able to saturate
the hardware resources.  When reaching saturation, it is never
CPU-bound, and appears to be limited by the NIC packet per second
hardware limit (Fig.~\ref{fig:short10:mcore}) or Ethernet wire rates
limitations (Fig.~\ref{fig:short10:size}).  By contrast, mTCP, can
only come close to matching \ix on Fig.~\ref{fig:short10:mcore} with
all 6 cores, and delivers 46\% of the throughput of \ix on
Fig.~\ref{fig:short10:roundtrips}.  Linux is never able to match \ix
or mTCP on any configuration.

%%
%%  see data in figs/data/connection-scaling-kstats.tex
%%

\myparagraph{40GbE results:}
Figs.~\ref{fig:short40:mcore}-\ref{fig:short40:size} show the results
on the 4x10GbE setup for Linux and \ix, as well as an \ix
configuration that uses only one of the four
links. Fig.~\ref{fig:short40:mcore} shows that \ix linearly scales and
delivers 3.9MTPS on 4x10GbE with a single multi-core socket.
Fig.~\ref{fig:short40:roundtrips} shows that going to 4x10GbE also
improves throughput, but only by 30\%: the 10GbE configuration is IO
bound and within 90\% of the wire rate, resulting in small batches or
\twiddle 3 packets processed at every iteration through the \ix
pipeline.  At 4x10GbE, the workload becomes CPU bound and \ix
automatically adapts to larger pipeline batches of \twiddle 113
packets, which amortizes transitions and improves locality.  Finally,
Fig.\ref{fig:short40:size} shows that \ix can deliver 8KB messages
with a goodput of 35Gbps, for a wire throughput of 38Gpbs (out of a
possible 39.7Gbps).  Overall, \ix makes it practical to scale
protected TCP/IP processing beyond 10GbE, even with a single socket
multi-core server.


\subsection{Connection Scalability}

\label{sec:eval:scale}

We also evaluate \ix's scalability when handling a large number of
concurrent connections on the 4x10GbE setup. In this benchmark, each client machine runs
$n$ threads, with each thread independently performing a 64B remote
procedure call to the server.  Each RPC randomly chooses among $m$
open sockets to perform the RPC.  In our setup with 18 clients, the
server must multiplex among $18 \times n \times m$ open connections.
We experimentally set $n \leq 24$ to maximize throughput.  We report
the maximal throughput in messages per second for varying number of
total established connections.

%Connection scaling IX/Linux @ 40Gbe 
%12036405.031689
%915256.419191
%/p
%13.15085


Fig.~\ref{fig:connscaling} shows up to to 100,000 connections, which
is the upper bound we can reach with the available client machines.
As expected, throughput increases with the degree of concurrency, but
then decreases for very large connections counts due to the
increasingly high cost of multiplexing among open connections.  At the
peak, \ix performs 13x better than Linux, consistent with the results
from Fig.~\ref{fig:short40:roundtrips}.  With 100,000 connections and
4x10GbE, \ix is able to deliver 53\% of its own peak throughput.  The
performance drop is not due to an increase in instruction counts, but
rather primarily because the size of the TCP/IP protocol control block
structures, which alone exceed the size of the processor's L3
cache. We believe we can improve further on the complexity and memory
footprint of our lwIP-based stack in future work. 

\input{eval-memcache}
