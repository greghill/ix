
\section{Evaluation}
\label{sec:eval}

We evaluate the scalability of \ix and compare it to the Linux
baseline running a 3.11.10 kernel, and to mTCP. We selected mTCP as it
is the state-of-the-art user-space TCP alternative available on the
same hardware~\cite{jeong2014mtcp}.  We first describe the
experimental setup (\S\ref{sec:eval:setup}).  We then characterize the
performance using a series of micro-benchmark against a server with
10GbE of connectivity: the popular NetPIPE ping-pong
test~\cite{snell1996netpipe} (\S\ref{sec:eval:netpipe}) and the short
message transaction test previously used to evaluate MegaPipe and mTCP
in~\cite{han2012megapipe,jeong2014mtcp} (\S\ref{sec:eval:short}).  We
then scale the latter test to a server with 4x10GbE of connectivity
and use that infrastructure to characterize \ix's scalability in
handling large persistent connections (\S\ref{sec:eval:scale}).
Finally, we characterize the performance of the \ix system by running
memcached -- a real-world massively deployed application
(\S\ref{sec:eval:memcached}).


\subsection{Experimental setup}
\label{sec:eval:setup}

Our experimental setup is complicated by the fact that we wish to
simulanteously compare with mTCP and scale to bandwidth beyond 10GbE.
Unfortunately, our 4x10GbE setup uses more recent hardware that does
not run mTCP out of the box.  We therefore used a \emph{10GbE setup}
for \S\ref{sec:eval:netpipe} and \S\ref{sec:eval:short} and includes
mTCP results. The \emph{4x10GbE setup} is used for
\S\ref{sec:eval:short40}, \S\ref{sec:eval:scale} and
\S\ref{sec:eval:memcached} and compares \ix and Linux~\footnote{We
  were able to port mTCP to the 4x10GbE setup, but could not reproduce
  the expected performance.  We will attempt to unify the setup by the
  camera-ready deadline.}

%\george{possible false hope that mTCP can use 4 NICs. I am not sure about that.}}.

 
\myparagraph{10GbE setup:} \edb{PLACEHOLDER:} This setup consists of a
cluster of XX dual-Xeon XXX@XXXGhz used as clients and one dual-Xeon
XXX@XXXGhz machine uses as the server connected by an Arista XXX
switch via Intel 82599EB 10GbE NICs.  The server socket has 6 cores and
12 hyperthreads.

\myparagraph{4x10GbE setup:} This setup consists of a cluster of 19
clients and one server connected by an HP 5820AF-24XG 10GbE switch.
The client machines are a mix of dual Xenon E5-2637 @ 3.5 Ghz
and single Xenon E5-2650 @ 2.6 Ghz.  Each client has a single
82599EB 10GbE NIC connecting to the switch.  The server is a dual-Xeon
E5-2665 running at 2.4 Ghz with 256 GB of DRAM with four Intel 2P X520
NICs.  Each socket (client or server) has 8 cores and 16 hyperthreads.
The server is connected to the switch via a 4x10GbE bond configured as a L2+L3 hash.

\myparagraph{Common elements:} Jumbo frames are never enabled.  Our
baseline configuration in each machine is the Ubuntu 12.04.4 LTS
distribution running the 3.11.10 Linux kernel.  When reporting
multi-core scalability, we report results on a per-core basis with
hyperthreading enabled.  Except for \S\ref{sec:eval:netpipe}, client
machines always run Linux. Although the servers have two sockets, our
experiments use only the CPU and memory resources of the socket to
which the NIC are attached.
\edb{CONTREVERSIAL: We made this deliberate decision to eliminate NUMA
  imbalances from our experiments, and to most closely reproduce the
  setup used in ~\cite{jeong2014mtcp}}


\input{figs/float-pingpong}
%\input{tbl-pingpong}

\input{figs/float-short10}
\input{figs/float-short40}

As for the benchmarks themselves: the Linux client and server
implementations of our benchmarks use the \texttt{libevent} framework
with the \texttt{epoll} system call.  We downloaded and installed mTCP
from the public-domain implementation~\cite{url:mtcp}, but had to
write the benchmarks using the mTCP API.  To run mTCP, we switch to the
2.6.36 kernel, as this is the most recent supported kernel version.

\subsection{High-bandwidth and low-latency}
\label{sec:eval:netpipe}


We start the evaluation with the popular ping-pong benchmark as
specified by NetPIPE~\cite{snell1996netpipe}. The benchmark simply
exchanges a fixed-size message between two servers and helps
calibrate the latency and bandwdith of a single-flow.

\edb{NEED FINAL LAT NUMBERS.} Fig.~\ref{fig:pingpong} compares the performance of various
configurations for different message sizes.  As expected, the
bandwidth increases with the message size. Two \ix machines can
pinp-pong at half 5Gbps (half the maximum) with messages as small as
32KB.  The latency differences, measured with 64B messages, are
striking.  The one-way latency is 6.8\microsecond between two machines
running \ix, 30\microsecond if they are running Linux, and
91\microsecond if they are running mTCP.  The difference in system
architecture help explain such a dramatic difference: \ix has a
dataplane model that polls queues and runs them to completion; Linux
has an interrupt model, which wakes up the blocked process.  The mTCP
authors mention high context switching overheads between the various
threads in their architecture, and the need for aggressive
batching~\cite{jeong2014mtcp}, which helps explain why it is such a
challenging benchmark for that system.


\subsection{Handling high TCP connection churn}
\label{sec:eval:short}

We then evaluate \ix's multi-core scalability in handling workloads
with extreme connection churn by using the same benchmark used to evaluate
Megapipe~\cite{han2012megapipe} and mTCP~\cite{jeong2014mtcp}:
multiple clients connect to a single server listening on a single
port, send a request of size $s$ and wait for an echo of a message of
the same size.  As with NetPIPE, while accepting the message, the server holds off its
echo response until the message has been entirely received.
Each client performs this synchronous remote procedure
call $n$ times, and then close the connection using a reset
(\texttt{TCP RST}).

Fig.~\ref{fig:short10} shows the results on the 10GbE setup.  For
Linux and mTCP, our results match the ones published in
~\cite{jeong2014mtcp}.  For all three tests, \ix scales much better
and is quickly able to sature the hardware resources.  When reaching
saturation, it is never CPU-bound, and appears to be limited by the
NIC packet per second hardware limit and Ethernet wire rates limitations.  In constast, mTCP needs
all 6 cores to match the rate on Fig.~\ref{fig:short10:mcore}, and
delivers 40\% of the throughput of \ix on
Fig.~\ref{fig:short10:roundtrips}.  Linux is never able to match \ix
or mTCP on any configuration.


\subsection{High connection chun at 4x10GbE}
\label{sec:eval:short40}

Fig.~\ref{fig:short40} shows the results on the 4x10GbE setup, and
compare them with a configuration that uses only one of the four
links.  The result for Linux are nearly identical, so only the 4x10
configuration is shown.  Fig.~\ref{fig:short40:mcore} shows that \ix
linearly scales and can nearly quadruple throughput.
Fig.~\ref{fig:short40:roundtrips} also shows scaling, although here
also appears limited by a hardware limitation, which we suspect to be
due to high rate of short DMA requests on the PCIe bus.
Fig.\ref{fig:short40:size} shows that it can nearly saturate 4x10GbE
of traffic even with short messgages.

\subsection{Connection Scalability}
\input{figs/float-connscaling}

\label{sec:eval:scale}

We now evaluate \ix's scalability when handling a large number of
concurrent connections. In this benchmark, each client machine runs
$n$ threads, with each thread independently performing a 64B remote
procedure call to the server.  Each RPC randomly chooses among $m$
open sockets to perform the RPC.  In our setup with 19 clients, the
server must multiplex among $19 \times n \times m$ open connections.
We experimentally set $n \leq 24$ to maximize throughput.  We report
the maximal throughput in messages per second for varying number of
total established connections. 

%\george{need to verify that this is indeed the setup}

Fig.~\ref{fig:connscaling} shows the results.  As expected, throughput
increases with the degree of concurrency, but then decreases with
increasing number of connections due to the increasingly high cost of
multiplexing among open connections.  At the peak, \ix performs XXx
better than Linux, consistent with the results from
Fig.~\ref{fig:short40:roundtrips}.  With 100,000 connections, \ix is
able to deliver XX\% of its peak throughput, whereas Linux only
delivers XXX\% of its own peak throughput\footnote{Unexplained
  performance issues with our client configuration setup limit our
  experiments to 100,000 connections at the time of submission. \ix is
  designed to scale to millions of concurrent connections.}


\subsection{Application performance -- memcache}
\label{sec:eval:memcached}

\input{figs/float-mutilate}

We now validate the performance benefits of the protected dataplane
design on memcached -- a real world, massively deployed, in-memory
key-value store built on top of the libevent framework.  We use the
mutilate~\cite{url:mutilate} load-generator and tool to measure
average and 99\% latency as a function of the throughput delivered by
the server~\cite{Leverich:RHSU:2014}.

Fig.~\ref{fig:mutilate} reports the latencies as a function of the
number of queries per second.  The peak QPS is measured using \ix at
XXX, which maps to a bandwidth leaving the server of XXX Gbps, or XX\%
of the theoretical wire rate of the machine.  We report the numbers
for Linux and \ix for XXX and YYY established, concurrent
connections served by the 18 clients.

Our results show that \edb{TODO}.....


%%%%%
%%%%% very optimistic
%%%%%

%\input{eval-mtier}
