
\section{Evaluation}
\label{sec:eval}

\christos{todo list: a) study of adaptive batching (fixed with various
  sizes vs adaptive), b) some breakdown of processing before/after to
  show how things change with IX, c) fix FIN/RST issue in our
  microbenchmark}

We compared \ix to a baseline running the most recent Linux kernel and to
mTCP~\cite{jeong2014mtcp}. Our evaluation uses both networking
microbenchmarks and a real-world, massively deployed, event-based
application.

\subsection{Experimental Methodology}
\label{sec:eval:setup}


%We use two experimental setups because we wish to simultaneously
%compare with mTCP and scale to bandwidth beyond 10GbE.  Unfortunately,
%our 4x10GbE setup uses recent hardware not supported by mTCP out of
%the box\footnote{We were able to port mTCP to the 4x10GbE setup, but
%  could not reach the expected performance.  We will attempt to unify
%  the setup by the camera-ready deadline.}.  Linux and \ix run on both
%setups.

\input{figs/float-pingpong} 

%\myparagraph{10GbE setup:} This setup
%consists of a cluster of 28 Xeon E5-2630 @ 2.3 Ghz machines with Intel
%82599EB 10GbE and Solarflare SFC9000 10GbE NICs, configured with 64 GB
%of RAM each. They are connected with an Arista 7050S-64 switch. One
%machine with an Intel NIC is used as the server, while the rest are
%used as clients. The server socket has 6 cores and 12 hyperthreads.

\input{figs/float-short-both}

%\myparagraph{4x10GbE setup:} 

The experimental setup consists of a cluster of \data{18}{XXX} clients
and one server connected by \edb{\sout{an HP 5820AF-24XG 10GbE switch}} 
a Quanta/Cumulus 48x10GbE switch with a Broadcom Trident+ ASIC.
The client machines are a mix of Xeon E5-2637 @ 3.5 Ghz and Xeon
E5-2650 @ 2.6 Ghz.  Each client has a single 82599EB 10GbE NIC
connecting to the switch.  The server is a Xeon E5-2665 @ 2.4 Ghz with
256 GB of DRAM and five Intel x520 NIC ports.  Each client or server
socket has 8 cores and 16 hyperthreads.  The server is connected
to the switch both via a single 10GbE interface and a 4x10GbE bond configured as a L2+L3 hash.  The former is used for all of our \emph{10GbE} experiments, and the latter for our \emph{4x10GbE} experiments. 

Our baseline configuration in each
machine is an Ubuntu LTS 14.0.4 distribution running the 3.16.1 (most-recent) Linux kernel.
When reporting multi-core scalability, we report
results on a per-core basis. We enable hyperthreading when it improves performance. Except for
\S\ref{sec:eval:netpipe}, client machines always run Linux. All power
management features are disabled for all systems in all
experiments. Jumbo frames are never enabled.


The Linux client and server implementations of our benchmarks use the
\texttt{libevent} framework with the \texttt{epoll} system call.  We
downloaded and installed mTCP from the public-domain
release~\cite{url:mtcp}, but had to write the benchmarks ourselves
using the mTCP API.  To run mTCP, we switch to the 2.6.36 kernel, as
this is the most recent supported kernel version.  We report only
10GbE results for mTCP as it does not support device bonds.


\subsection{Latency and Single-flow Bandwidth}
\label{sec:eval:netpipe}

We first evaluated the latency of \ix using NetPIPE, a popular
ping-pong benchmark, using our 10GbE setup.  NetPIPE simply exchanges
a fixed-size message between two servers and helps calibrate the
latency and bandwidth of a single-flow~\cite{snell1996netpipe}.  In
all cases, the two servers use the same system (Linux, mTCP, or \ix).


Fig.~\ref{fig:pingpong} shows the goodput achieved for different
message sizes.  Two \ix servers achieve goodput of 5Gbps (half of the
maximum) with messages as small as 20KB. In contrast, Linux requires
384KB messages to achieve 5Gbps, while mTCP requires messages larger
than 500KB. The goodput difference between the three systems is most
dramatic for small messages. Using 64B messages, the one-way unloaded
latency is \data{6.9}{5.7}\microsecond between two servers running \ix,
\data{21}{24}\microsecond if they are running Linux, and \data{95}{127-XXX}\microsecond if they
are running mTCP.  The differences in system architecture explain the
dramatic gap. \ix has a dataplane model that polls queues and
processes packets to completion. Linux has an interrupt model, which
wakes up the blocked process. mTCP uses aggressive batching to offset
the cost of context switching~\cite{jeong2014mtcp}, \edb{at the expense of low latency.}




\subsection{Scalability and High Connection Churn}
\label{sec:eval:short}



Next, we evaluate \ix's multi-core scalability in handling workloads
with extreme connection churn by using the same benchmark used to
evaluate MegaPipe~\cite{DBLP:conf/osdi/HanMCR12} and
mTCP~\cite{jeong2014mtcp}. Multiple clients connect to a single server
listening on a single port, send a request of size $s$ and wait for an
echo of a message of the same size.  As with NetPIPE, while accepting
the message, the server holds off its echo response until the message
has been entirely received.  Each client performs this synchronous
remote procedure call $n$ times, and then closes the connection using a
reset (\texttt{TCP RST}).  \dm{This begs the question of what happens
  with FIN, since that is more realistic\@.  Is the point that \ix has
an unfair advantage?}


%%
%% RAW DATA - 1024 message per connection
%% IX:  7074246
%% mTCP: 3253521

\myparagraph{10GbE results:}
Figs.~\ref{fig:short10:mcore}-\ref{fig:short10:size} show the results
on the 10GbE setup.  For Linux and mTCP, our results are consistent
with the ones published in the mTCP paper~\cite{jeong2014mtcp}.  For
all three tests (core scaling, message count scaling, message size
scaling), \ix scales more aggressively and is quickly able to saturate
the hardware resources.  When reaching saturation, it is never
CPU-bound, and appears to be limited by the NIC packet per second
hardware limit (Fig.~\ref{fig:short10:mcore}) or Ethernet wire rates
limitations (Fig.~\ref{fig:short10:size}).  By contrast, mTCP, can
only come close to matching \ix on Fig.~\ref{fig:short10:mcore} with
all \data{6}{XXX} cores, and delivers \data{46}{XXX}\% of the throughput of \ix on
Fig.~\ref{fig:short10:roundtrips}.  Linux is never able to match \ix
or mTCP on any configuration \edb{CHECK}.

%%
%%  see data in figs/data/connection-scaling-kstats.tex
%%

\myparagraph{4x10GbE results:}
Figs.~\ref{fig:short40:mcore}-\ref{fig:short40:size} show the results
on the 4x10GbE setup for Linux and \ix, as well as an \ix
configuration that uses only one of the four
links. Fig.~\ref{fig:short40:mcore} shows that \ix linearly scales and
delivers 3.9MTPS on 4x10GbE with a single multi-core socket.
Fig.~\ref{fig:short40:roundtrips} shows that going to 4x10GbE also
improves throughput, but only by 30\%: the 10GbE configuration is IO
bound and within 90\% of the wire rate, resulting in small batches or
\twiddle 3 packets processed at every iteration through the \ix
pipeline.  At 4x10GbE, the workload becomes CPU bound and \ix
automatically adapts to larger pipeline batches of \twiddle 113
packets, which amortizes transitions and improves locality.  Finally,
Fig.\ref{fig:short40:size} shows that \ix can deliver 8KB messages
with a goodput of 35Gbps, for a wire throughput of 38Gpbs (out of a
possible 39.7Gbps).  Overall, \ix makes it practical to scale
protected TCP/IP processing beyond 10GbE, even with a single socket
multi-core server.


\subsection{Connection Scalability}

\label{sec:eval:scale}

We also evaluate \ix's scalability when handling a large number of
concurrent connections on the 4x10GbE setup. In this benchmark, the 18 client machine each run
$n$ threads, with each thread repeatetly performing a 64B remote
procedure call to the server for all of its open connections.
We experimentally set $n=24$ to maximize throughput.  We report
the maximal throughput in messages per second for varying number of
total established connections.

%Connection scaling IX/Linux @ 40Gbe 
%12036405.031689
%915256.419191
%/p
%13.15085


Fig.~\ref{fig:connscaling} shows up to to \data{100,000}{250,000} connections, which
is the upper bound we can reach with the available client machines.
As expected, throughput increases with the degree of concurrency, but
then decreases for very large connections counts due to the
increasingly high cost of multiplexing among open connections.  At the
peak, \ix performs \data{13x}{XXX} better than Linux, consistent with the results
from Fig.~\ref{fig:short40:roundtrips}.  With 250,000 connections and
4x10GbE, \ix is able to deliver  \data{53}{XXX}\% of its own peak throughput.

Fig.~\ref{fig:connscaling-hw} provides insights into \ix's connection
scalability using hardware performance counters for the 4x10GbE setup:
even though the average instruction count is nearly constant for all
data points, the performance, as measured in \emph{cycles-per-msg}
(excluding the idle loop) clearly shows that the workload has an
efficiency sweep spot between 1000 and 10,000 concurrent connections.
Within the sweet spot, \ix's adaptive batching mechanism processes at
least 10 packets per iteration. Intel's x520 direct cache access
technology~\cite{DBLP:conf/isca/HuggahalliIT05} eliminates nearly all
cache misses associated with DMA, resulting in as little as 1.4 L3
cache misses per message within the sweet spot.

On the left of the sweet spot, with fewer than 1000 connections, this
microbenchmark does not have enough concurrency to satisfy the 16
elastic threads with the server clearly overprovisioned: the average
batch size is as low as XXX, and the L3 cache behavior is poor, likely
because of prefetching interference and false sharing on the
descriptor rings between the NIC and the driver.

On the right side of the sweet spot, with more than 10,000
connections, the server is CPU-bound, and the average batch size
matches the configured bound.  The performance gradually worsens as
the working set size scales with the number of outstanding
connections, as each connection requires a distinct protocol control
block (in the kernel) and connection context data structure (in user
space).  At peak, the workload averages 25 L3 cache misses per
message.  Clearly, the working set of this workload, which is
dominated by the TCP connection state, cannot fit into the processor's
L3 cache.  Nevertheless, we believe that further optimizations in the
size and access pattern of lwIP's TCP/IP protocol control block
structures could substantially reduce the impact of the memory
subsystem.


\input{eval-memcache}
