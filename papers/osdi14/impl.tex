
\section{\ix Implementation}
\label{sec:impl}

\input{tbl-api}

\christos{We should stress that the same pattern/architecture can be
  used to implement any high performance stack, not just TCP/IP}

The current \ix prototype uses the VT-x virtualization features
available on all x86-64
servers~\cite{DBLP:journals/computer/UhligNRSMABKLS05}. However, it
can be ported to any architecture with virtualization support, such as
ARM, SPARC, and Power. 
\edb{The prototype leverages the RSS and multi-queue features of
  Intel's \texttt{ixgbe} NIC via Intel's DPDK toolkit~\cite{intel:dpdk}, but is
  designed to easily support additional drivers.}

\edb{NEW}Fig.~\ref{fig:ix} presents the \ix architecture, including the
separation of control and dataplane (\S\ref{sec:impl:overview}), and
the interleaving of protocol processing and application within the
dataplane (\S\ref{sec:impl:api}).


\subsection{Overview}
\label{sec:impl:overview}


Fig.~\ref{fig:cp-dp} illustrates the control plane, a number of dataplanes,
and applications. The hardware environment is a multi-core server with
one or more multi-queue NICs with RSS support.

The \ix control plane consists of the full Linux kernel and
\texttt{IXCP}, a user-level program. The Linux kernel initializes PCIe
devices, such as the NICs, and provides the basic mechanisms for
resource allocation to the dataplanes, including cores, memory, and
network queues. Equally important, Linux provides system calls and
services that are necessary for compatibility with a wide range of
applications, such as file system and signal support. \texttt{IXCP}
monitors resource usage and dataplane performance and implements
resource allocation policies. The development of efficient allocation
policies involves understanding difficult tradeoffs between dataplane
performance, energy proportionality, and resource sharing between
co-located applications as their load varies over
time~\cite{DBLP:conf/eurosys/LeverichK14}. We leave the design of such
policies to future work and focus this paper on the \ix dataplane
architecture.

We run the Linux kernel in VMX root ring 0, the mode typically used to
run hypervisors in virtualized
systems~\cite{DBLP:journals/computer/UhligNRSMABKLS05}. We use the
Dune system within Linux to enable dataplanes to run as app-specific
OSes in the VMX non-root ring 0, the mode typically used to run guest
kernels in virtualized systems~\cite{dune}.
Finally, applications run VMX non-root ring 3. This approach provides
dataplanes with direct access to hardware features, such as page
tables and exceptions, and provides full protection between the
control plane, dataplanes, and untrusted user code. Moreover,
dataplanes have direct passthrough access to  NICs.


Each \ix dataplane supports a single, multithreaded
application. For instance, Fig.~\ref{fig:cp-dp} shows one dataplane
for a multi-threaded \texttt{memcached} server and another dataplane
for a multi-threaded \texttt{nginx} server. The control plane allocates
resources to each dataplane in a coarse-grained manner. Core allocation
is controlled through real-time priorities and \texttt{cpusets};
memory is allocated in large pages; each NIC hardware queue is
assigned to a single dataplane. This approach avoids the overheads and
unpredictability of fine-grained time multiplexing of resources between
demanding applications~\cite{DBLP:conf/eurosys/LeverichK14}.

The \ix dataplane operates as a single address-space OS and supports
two thread types within the shared, user-level address space: (i)
\emph{elastic threads} which interact with the \ix dataplane to
initiate and consume network I/O and (ii) \emph{background threads}.
Both elastic and background threads can issue arbitrary POSIX system
calls that are intermediated and validated for security by the
dataplane before being forwarded to the Linux kernel.  Elastic
threads are expected to \emph{not} issue blocking calls because of the
adverse impact on network behavior and performance. Similarly, each
elastic thread makes exclusive use of a core or hyperthread allocated
to the dataplane in order to achieve high performance with predictable
latency. In contrast, multiple background threads may timeshare an
allocated hyperthread. For example, if an application were
allocated four hyperthreads, it could use all of them as elastic
threads to serve external requests or it could temporarily transition
to three elastic threads and use one background thread to execute
tasks such as garbage collection. When the control plane revokes or
allocates an additional hyperthread using a protocol similar to the
one used in Exokernel~\cite{DBLP:conf/sosp/EnglerKO95}, the dataplane
must adjust its number of elastic threads.

\christos{We should stress that queues are a central concept in our
  work.  we should add some discussion about elasticity/load balancing
  here. Should we discuss how traffic is seperated between queues in
  this section?}

\subsection{The Dataplane Kernel}
\label{sec:impl:dpkernel}

Our dataplane runs in kernel space, as a virtualization guest on
top of Linux~\cite{dune}. It differs from typical kernels
in that it is specialized for high performance I/O and runs only
a single application (similar to a library OS but with memory
isolation). However, our dataplane is still required to provide
many common kernel-level services.

For memory management, we are willing to accept some memory fragmentation in order
to reduce complexity and improve efficiency. All hot-path data objects are allocated from
per-core memory pools. Each memory pool is structured as arrays
of identically sized objects, provisioned in page-sized blocks. Free objects
are tracked with a simple free list, and allocation routines are
inlined directly into calling functions. Mbufs (our storage object
for network packets) are stored as contiguous chunks of bookkeeping
data and MTU sized buffers, rounded to the nearest
kilobyte, and are used for both receiving and transmitting packets.

The dataplane also manages its own virtual address translations, supported
through nested paging. In constrast to contemporary OSes, we use exclusively
large (2MB) pages. We found that large pages are an optimal granularity,
given their potential to reduce address translation overhead~\cite{DBLP:conf/isca/BasuGCHS13, dune}
and the growing abundance of physical memory resources.. Our dataplane maintains
only a single address space; kernel pages are protected with
supervisor bits. We deliberately chose not to support swappable memory
in order to avoid adding unecessary performance variability.

We provide a hierarchical timing wheel~\cite{DBLP:conf/sosp/VargheseL87} subsystem
for managing network timeouts, such as TCP retransmissions. It is optimized
for the common case where most timers are canceled before they expire. We support
extremely high resolution timeouts (as low as 16 \microsecond), which has been
shown to improve performing in the presence of TCP incast congestion~\cite{DBLP:conf/sigcomm/VasudevanPSKAGGM09}.

The dataplane \edb{currently} consists of \data{37K}{XXX} SLOC~\cite{url:sloccount}.  We
leveraged existing codebases in its development: 43\% is derived from
the DPDK variant of the Intel NIC device driver~\cite{intel:dpdk},
\data{23\%}{XXX} from the lwIP TCP/IP stack~\cite{dunkels2001design}, and \data{16\%}{XXX}
from the Dune library.  All three code bases are highly modified for
\ix. The rest is approximately \data{7K}{XXX} SLOC of new code. We chose lwIP as a
starting point for TCP/IP processing because of its modularity and its
maturity as a RFC-compliant, feature-rich networking stack. We
implemented RFC-compliant support for UDP, ARP, and ICMP.
Since lwIP was optimized for memory efficiency in embedded
environments, we had to radically change its internal data structures
for multi-core scalability and fine-grained timer management. We did not
yet optimize the lwIP code for performance. Hence, there is room for
improvement in the results shown in \S\ref{sec:eval}.

\subsection{Dataplane API and Operation}
\label{sec:impl:api}

Elastic threads interact with the \ix dataplane through three
asynchronous, non-blocking mechanisms summarized in
Table~\ref{tbl:api}: they issue \emph{batched systems calls} to the
dataplane; they consume \emph{event conditions} generated by the
dataplane; and they have direct, but safe, access to
(\emph{mbuf}s) containing incoming payloads.  The latter allows
for zero-copy access to incoming network traffic.  The application
can hold on to mbufs until it asks the dataplane to
release them via the \texttt{recv\_done} batched system call.

Both batched system calls and event conditions are passed through
arrays of shared memory, managed by the user and the kernel
respectively.  We provide an unbatched system call
(\texttt{run\_io}) that yields control to the kernel and initiates a new run
to completion cycle. As part of the
cycle, the kernel overwrites the array of batched system call requests with
corresponding return codes and populates the array of event conditions.
The handles defined in Table~\ref{tbl:api} are kernel-level flow identifiers. Each handle is
associated with a cookie~\cite{DBLP:conf/osdi/HanMCR12}, an opaque value
provided by the user at connection establishment to enable efficient
user-level state lookup. %without an additional table lookup.

\ix differs from POSIX sockets in that it directly exposes flow
control conditions to the application: the \texttt{sendv} system call
does not return the number of bytes buffered. Instead, it returns the
number of bytes that were accepted and sent by the TCP stack, as
constrained by correct TCP sliding window operation. Then, when the
receiver acknowledges the bytes, a \texttt{sent} event
condition informs the application that it is possible to send more
data. Thus, send window sizing policy is determined entirely by the
application.  By contrast, conventional OSes buffer send data beyond
raw TCP constraints and apply flow control policy inside the kernel
~\cite{dynamicwindow}.

One hazard of working with batched system calls is that when one
request depends on the outcome of another request in the batch, it can
lead to unexpected results. For example, if two separate requests are made to
write to a TCP connection, then the first could fail (for example if the
kernel runs out of memory) while the second could succeed, resulting in incorrect
data ordering. While not always required, we found that the
simple heuristic of issuing at most one system call per TCP connection in a given
batch greatly simplified error handling. Our \texttt{sendv} system
call also supports multiple buffers, so it is
usually possible to coalesce several writes into a single request.

We built a user-level library, called \emph{libix}, that abstracts away
the complexity of our low-level API. It provides a compatible programming
model for legacy applications and significantly simplifies the
development of new applications. Libix includes a functionally
compatible interface to \texttt{libevent} and non-blocking POSIX
socket operations. It also includes new interfaces for zero copy read
and write operations that are more efficient, at the expense of
requiring changes to existing applications. 


%\input{figs/float-pipeline} 

Fig.~\ref{fig:dataplane} illustrates the run-to-completion
operation of elastic threads in the \ix dataplane. \edb{REDO?}Each elastic thread
is associated with a specific set of receive queues from one or more
NICs. Each NIC uses RSS to implement flow-consistent hashing and
distribute incoming packets to queues. The queues are mapped in the
server's main memory and the NIC is given a set of buffer descriptors
that allow it to transfer incoming packets to memory using DMA\@.  The
elastic thread (1) polls the receive queues it is responsible for and
potentially posts fresh buffer descriptors to the NIC for use with
future incoming packets. The elastic thread then (2) processes a bounded
number of packets through the TCP/IP networking stack, thereby
generating event conditions. Next, the elastic thread (3) passes
control to the user-space applications, which consumes all event
conditions. Assuming that the incoming packets include remote
requests, the application processes these requests and responds with a
batch of system calls. Upon return of control from user-space, the
elastic thread (4) processes all system calls, and in particular the
ones that direct outgoing TCP/IP traffic. It also (5) runs all kernel
timers in order to ensure compliant TCP behavior. \edb{Finally (6)}, it places
outgoing Ethernet frames in the NIC's descriptor rings for
transmissio and it notifies the NIC to initiate a DMA transfer
for these frames by updating the transmit ring's tail register.  This
process repeats in a loop until there is no network activity. In this
case, we enter a quiescent state which involves either power-efficient
polling or yielding the hardware thread \george{the verb yield reminds me of a scheduler and i don't know if we want people to think that a dataplane and the control plane are analogous to a thread and the kernel scheduler} to the control plane.



\subsection{Multi-core Scalability}
\label{sec:impl:cohfree}

The \ix dataplane is optimized for multi-core scalability as elastic
threads operate in a synchronization and coherence free manner in the
common case. This is a stronger requirement than lock-free
synchronization, which requires expensive atomic instructions even
when a single thread uses a particular lock in the common
case~\cite{DBLP:conf/sosp/DavidGT13}.  This was made possible 
through a set of conscious design and implementation tradeoffs. 

First, the \ix API is commutative. Events are processed independently
on each elastic thread. Handles identify flows but cannot be exchanged
between elastic threads. There is no file descriptor namespace.
According to the commutativity rule, system call implementations can
only be coherence-free if the API itself is
commutative~\cite{DBLP:conf/sosp/ClementsKZMK13}.

Second, the API implementation is carefully optimized.  Each elastic
thread manages its own memory pools, hardware queues, event condition
array, and batched system call array. No synchronization is required
to access any of them. The implementation of event conditions and
batched system calls benefits directly from the explicit, cooperative
control flow transfers between \ix and the application by the elastic
thread.  Since there is no concurrent execution by producer and
consumer, event conditions and batched system calls are implemented
without synchronization primitives based on
atomics.

Third, the use of flow-consistent hashing at the NICs ensures that
each elastic thread operates on a disjoint subset of incoming TCP
flows. Hence, no synchronization or coherence occurs during the
processing of incoming requests for a server application. For client
applications with outbound connections, we need to ensure that the
reply is assigned to the same elastic thread that made the
request. Since we cannot reverse the Toeplitz hash used by RSS~\cite{url:rss}, we
simply probe the ephemeral port range to find a port number that
would lead to the desired behavior. Note that this implies that two
elastic threads in a client cannot share a flow to a server.


\ix does have a small number of shared structures, including some that
require synchronization on updates.  For example, the ARP table is
shared by all elastic threads and is protected by RCU
locks~\cite{mckenney1998read}, Hence, the common case reads are
coherency-free but the rare updates are not.
%
\dm{Can you add a sentence about what constitutes a quiescent period
  for RCU garbage collection, given that this is a very non-Linux
  API?}


\ix also requires
synchronization when the control plane reallocates resources between
dataplanes.  For instance, when a core is revoked from a dataplane,
the corresponding incoming queues must be assigned to another elastic
thread. Such events are rare because resource allocation happens in a
coarse-grain manner. Finally, the application code may include
inter-thread communication and synchronization. While using \ix does
not eliminate the need to develop scalable application code, it
ensures that there are no scaling bottlenecks in the system and
protocol processing code. 

\subsection{Security Model}
\label{sec:impl:coop}

\christos{We need to clean up this section, make a bigger deal out of
  this, and explain why this is the right way to architect IO stacks/libOS}

The \ix API and implementation leads to a cooperative flow control
model between application code and the network-processing stack.  But
unlike user-level stacks that also expose networking behavior to user
code, \dm{Can you say something more specific than networking
  behavior?  Maybe something like ``unrestricted raw network
  access''?}  the \ix protection model makes few assumptions about
application behavior. A malicious or misbehaving application can only
hurt itself. It cannot corrupt the networking stack or affect other
applications.

The \ix dataplane and the application collaboratively manage
memory. To enable zero-copy operation, a buffer used for an incoming
packet is passed read-only to the application, enabling zero-copy
operation. Applications that hold on to message buffers for extensive
periods of time must bound their use of this shared resource.  In the
transmit direction, zero-copy operation implies that the application
must not modify outgoing data
until reception is
acknowledged by the peer.

Since elastic threads in \ix execute both the network stack and
application code, a long running application can block further network
processing for a set of queues. This behavior in no way affects other
applications or dataplanes. We use a timeout interrupt to detect
elastic threads that spend excessive time in user mode (e.g., in
excess of 10ms). We mark such applications as non-responsive and
notify the control
plane.

Note that all application code in \ix runs in user-mode, while the
dataplane code is in protected ring 0. Applications cannot access
dataplane memory, except for read-only message buffers.  No sequence
of batched system calls or other user-level actions can be used to
violate correct adherence to TCP and other network specifications.
Furthermore, the dataplane can be used to enforce network security
policies (e.g., iptables or Amazon Security
Groups~\cite{url:amazon-sg}) or to implement the network
virtualization functions typically done in a
hypervisor~\cite{nsdi:nsx}. Our security model is as strong as
conventional network stacks in commodity OSes and is missing from all
the recently proposed user-level networking stacks.

Our current implementation does not use an IOMMU due to their
performance overheads~\cite{iommu:overhead}. The \ix kernel is trusted
code that has access to descriptor rings with host-physical addresses.
This decision is not fundamental to our design and does not affect the
security model provided to applications. 


\dm{This raised a couple of questions for me.  First, is the
  immutability of transmit buffers enforced by memory protection, or
  just by convention.  In other words, can you cause TCP segments to
  be sent with bad checksums or otherwise shoot yourself in the foot
  by writing to buffers in the send queue?  Second, why are message
  buffers read-only (or does this exclude the actual payload part).
  For example, if I'm implementing an HTTP proxy, can I make small
  modifications to a message buffer I just received and then
  retransmit it?  Or is the point that one would use scatter gather IO
  for such surgical edits?}

\christos{adam wants another implementation section or subsection with
  more code details -- add outline}
