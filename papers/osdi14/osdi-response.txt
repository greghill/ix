
We thank the reviewers, and will take all editorial comments to
heart. We agree that the writing can be much improved in form and
substance.

Design

 -[#B]Batching to completion: our system is unique in that it batches
  system calls adaptively based on congestion, and therefore without
  impacting latency in low-utilization situations.  IX is also unique
  in batching all steps of the processing pipeline together rather
  than a single phase (such as the system call interface), which
  eliminates intermediate buffers and improves cache behavior.

 -[#A,#C] The IX design partitions NIC queues and CPU cores between IX
  instances.  IX is not designed to run within a VM
  which is actually ok as most large website (Google, Facebook, ...)
  don't use VMs for webscale workloads.

 -[#C]RSS only dictates the behavior of RX packets; TX packets can be
  sent on any queue.  For incoming connections, the RSS hash
  determines the core, which is then used by the entire pipeline.  For
  outgoing connections, the ephemeral port is selected so that the RSS
  hash points to the CPU core that opened the connection.

Comparison 

 - [#A]netmap is only a raw Ethernet packet system, and therefore can't
   run natively any TCP-based workload.

 - [#E]We compare thoroughly with mTCP on our setup; the limitation
   described in the footnote is specific to our 4x10GbE cluster, which
   ended up requiring an extensive source-code level patch to mTCP
   (post-submission).

 - [#E]The Megapipe source code is not readily available on the web,
   so our comparison with Megapipe is "transitive" in the submission:
   the mTCP paper [36] shows that mTCP clearly outperforms megapipe, and IX outperforms mTCP.

 - [#A] We'd be very interested in comparing our system against
   Arrakis as details emerge.

Evaluation:

 - [#A]The half-ticks in Fig 4(a) and 4(b) correspond to
   hyperthreads. We enabled and used hyperthreads only when it
   improves performance. With mTCP, the two hyperthreads of a core are always
   used asymmetrically, as specified by the authors.

 - [#C] We have measured the 99th latency for memcached and will add
   it to the paper.  IX compares even more favorably to Linux on the
   99th than on the 95th.

 - [#D]We made an aggressive effort to tun Linux in our comparison,
   e.g. pinning threads to core, killing background tasks, adjusting
   interrupt stearing, applying custom parameters to the NIC driver,
   and running a very recent kernel (further suggestions welcomed).
   This has a huge impact over untuned results, which we did chose not
   to publish.


 - [#E]We are actively looking at behavior passed a single core.  Our
   initial focus was to deliver line rate (10 and 4x10GbE) using a
   single socket.  Also, note that memcache has severe limitations
   scaling even within a socket.

Networking behavior:

 - [#A]TSO and RSC cannot impact short-connection behavior. When
   streaming, IX achieves line-rate without TSO because of IX's
   optimized stack.  Also, Intel's implementations of RSC does not
   scale to large flow count and adds latency.

 - [#B]ECN/RED traffic engineering: Yes, IX's adaptive,
   run-to-completion model ensures that receive queues build up only
   at the NIC edge, before processing.  Step (1) of Fig 2 can be
   modelled as a software router from which batches of packets are
   delivered.  This papers focuses on system comparison using
   "standard" networking protocols.


