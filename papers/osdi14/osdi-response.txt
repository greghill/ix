
We thank the reviewers, and will take all comments to heart. 
We agree that the writing can be much improved in form and
substance.


Design

 -[#B]Batching to completion: our system is unique in that it batches
  adaptively based on congestion, and therefore without impacting latency
  at low load.  IX is also unique in batching all steps of the RX/TX processing 
  pipelines together rather than a single phase (such as the system call 
  interface), which eliminates intermediate buffers and improves cache behavior.

 - ADD STATEMENT ABOUT CONTRIBUTIONS | WHY AN EVOLUTIONARY DESIGN NOT ENOUGH

 -[#A,#C] Our design partitions NIC queues and cores between IX
  instances.  IX is not designed to run within a VM which is 
  actually ok as most large website (Google, Facebook, Twitter,â€¦)
  don't use VMs for webscale workloads.

 -[#C]RSS only dictates the behavior of RX packets; TX packets can be
  sent on any queue.  For incoming connections, the RSS hash
  determines the core, which is then used by the entire pipeline.  For
  outgoing connections, the ephemeral port is selected so that the RSS
  hash points to the CPU core that opened the connection.

 - ADD STATEMENT ABOUT PROTECTION


Comparison 

 - [#A]netmap is a raw Ethernet packet system, and therefore can't
   directly run a TCP-based workload.

 - [#E]We compare thoroughly with mTCP on our setup; the limitation
   described in the footnote is specific to our 4x10GbE cluster, which
   ended up requiring an extensive source-code level patch to mTCP
   (post-submission).

 - [#E]The Megapipe source code is not readily available on the web,
   so our comparison with Megapipe is "transitive": [36] shows that mTCP clearly 
   outperforms megapipe, and IX outperforms mTCP.

 - [#A] We'd be very interested in comparing our system against
   Arrakis as details emerge.

Evaluation:

 - [#A]The half-ticks in Fig 4(a) and 4(b) correspond to
   hyperthreads. We enabled hyperthreads only when it
   improves performance. With mTCP, the two hyperthreads of a core are always
   used asymmetrically, as specified by the authors.

 - [#C] We have measured the 99th latency for memcached and will add
   it to the paper.  IX compares even more favorably to Linux on the
   99th than on the 95th.

 - [#D]We aggressively tuned Linux e.g. pinning threads to core, 
   killing background tasks, adjusting interrupt steering, applying 
   custom parameters to the NIC driver, and running a very recent kernel 
   (further suggestions welcomed). This has a huge impact over untuned 
   results, which we did chose not to publish. ADD SPECIFIC COMMENT ON NUMBERS

 - [#E]We are actively looking at behavior of multiple sockets. Our
   initial focus was to deliver line rate (10 and 4x10GbE) using a
   single socket.  

Networking behavior:

 - [#A]TSO and RSC cannot impact short-connection behavior. When
   streaming, IX achieves line-rate without TSO because of IX's
   optimized stack.  Also, Intel's implementations of RSC does not
   scale to large flow count and adds latency.

 - [#B]ECN/RED traffic engineering: Yes, IX's adaptive,
   run-to-completion model ensures that receive queues build up only
   at the NIC edge, before processing.  Step (1) of Fig 2 can be
   modeled as a software router from which batches of packets are
   delivered.  This papers focuses on system comparison using
   "standard" networking protocols.


