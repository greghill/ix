
\section{Discussion}
\label{sec:disc}


\myparagraph{What makes \ix fast:} The results in \S\ref{sec:eval}
show that a networking stack can be implemented in a protected OS
kernel and still deliver wire-rate performance for most benchmarks.
The tight coupling of the dataplane architecture, using only a minimal
amount of batching to amortize transition costs, causes application
logic to be scheduled at the right time, which is essential for
latency-sensitive workloads.  Therefore, the benefits of \ix go beyond
just minimizing kernel overheads. The lack of intermediate buffers
allows for efficient, application-specific implementations of I/O
abstractions such the \texttt{libix} event library. The zero-copy
approach helps even when the user-level libraries add a level of
copying, as it is the case for the \texttt{libevent} compatible
interfaces in \texttt{libix}.  The extra copy occurs much closer to
the actual use, thereby increasing cache locality.  Finally, we
carefully tuned \ix for multi-core scalability, eliminating constructs
that introduce synchronization or coherence traffic.



\myparagraph{Subtleties of adaptive batching:} Batching is commonly
understood to trade off higher latency at low loads for better
throughput at high loads.  \ix uses adaptive, bounded batching to
actually improve on both metrics.  Fig.~\ref{fig:batch-mutilate}
compares the latency vs. throughput on the \texttt{USR}
\texttt{memcached} workload of Fig.~\ref{fig:mutilate} for different
upper bounds $B$ to the batch size.  At low load, $B$ does not impact
tail latency, as adaptive batching does not delay processing of
pending packets.  At higher load, larger values of $B$ improve
throughput, by 23\% between $B=1$ to $B=32$.  For this workload, $B
\ge 32$ maximizes throughput.


\begin{comment}
\myparagraph{Subtleties of adaptive batching:} 
Batching is commonly understood to trade off higher latency at low loads
for better throughput at high loads.  Our use of adaptive, bounded
batching actually improves on both metrics. In exploring the impact of
the batch size bound --- which impacts behavior when the workload is
CPU-bound --- we first confirmed the intuition that larger batch
sizes improve throughput, but only to a point. Fig.~\ref{fig:batch}
compares the throughput of the CPU-bound benchmark used in
Fig.~\ref{fig:short10:roundtrips}.  The experiment shows that batches
as low as $B=32$ can deliver maximal throughput for this
workload. (All experiments in \S\ref{sec:eval} were conducted with a
maximal batch size of $B=64$ packets per iteration.)  Because the
algorithm is adaptive, $B$ does not impact latency when the dataplane
is not saturated; we confirmed
this experimentally at the 99th percentile latency for
\texttt{memcache}.
\end{comment}


%\input{figs/float-batch}
\input{figs/float-batch-mutilate}

While tuning \ix performance, we ran into an unexpected hardware limitation
that was triggerred at high packet rates with small average batch sizes 
(i.e. before the dataplane was saturated): 
the high rate of PCIe writes required to post fresh
descriptors at every iteration led to performance degradation as we
scaled the number of cores.  To avoid this bottleneck, we
simply coalesced PCIe writes on the receive path so that we
replenished at least 32 descriptor entries at a time.  Luckily, we did
not have to coalesce PCIe writes on the transmit path, as that would
have impacted latency.


\begin{comment}
  \myparagraph{Using interrupts as a fallback:} Some applications
  service requests that require extended intervals of compute time. We
  intend for these requests to be delegated to from elastic threads to
  background threads in order to ensure that elastic threads remain
  responsive.  However, \ix can also be modified to better tolerate
  unanticipated delays during application processing in elastic
  threads.  One option would be to use interrupts as a fallback
  mode. On the receive side, the NIC would fire an interrupt whenever
  the recieve descriptor ring is almost full. The dataplane could then
  move packets from the receive ring to a software structure, averting
  buffer underrun. On the transmit side, NIC would fire an interrupt
  whenever the transmit ring becomes empty so that it can be
  refilled. Such an interrupt would only need to be armed when there
  is additional transmit data pending. A desirable property of this
  approach is that neither interrupt would be triggered as long as
  elastic threads are sufficiently responsive, but if an elastic
  thread misbehaves, the \ix dataplane would be able to regain control
  and catch up on network processing.
\end{comment}


\myparagraph{Limitations of current prototype:} The current \ix
implementation does not yet exploit IOMMUs or VT-d. Instead, it maps
descriptor rings directly into \ix memory, using the Linux pagemap
interface to determine physical addresses.  Although this choice puts
some level of trust into the \ix dataplane, application code remains
securely isolated. In the future we plan on using IOMMU support to
further isolate \ix dataplanes. We anticipate overhead will be low
because of our use of large pages.  Also, the \ix prototype currently
does not take advantage of the NIC's SR-IOV capabilities, but instead
allocates entire physical devices to dataplanes.

We also plan to add support for interrupts to the \ix dataplanes. The
\ix execution model assumes some cooperation from application code
running in elastic threads.  Specifically, applications should handle
events in a quick, non-blocking manner; operations with extended
execution times are expected to be delegated to background threads
rather than execute within the context of elastic threads.  The \ix
dataplane is designed around polling, with the provision that
interrupts can be configured as fallback optimization to refresh
receive descriptor rings when the ring is nearly full and to refill
the transmit queues when it is empty (steps (1) and (6) in
Fig~\ref{fig:dataplane}). Occasional timer interrupts are also
required to ensure full TCP compliance in the event an elastic thread
blocks for an extended period.

\myparagraph{Future work:} This paper focused primarily on the \ix
dataplane architecture. \ix is designed and implemented to support the
dynamic addition and removal of elastic threads in order to achieve
energy proportional and resource efficient computing. So far we have
tested only static configurations. In future work, we will explore
control plane issues, including a dynamic runtime that rebalances
hardware queues between available elastic threads in a manner that
maintains both throughput and latency constraints.

We will also explore the synergies between \ix and networking
protocols designed to support microsecond-level latencies and the
reduced buffering characteristics of \ix deployments, such as
DCTCP~\cite{DBLP:conf/sigcomm/AlizadehGMPPPSS10} and
ECN~\cite{ramakrishnan2001addition}. Note that the \ix dataplane is
not specific to TCP/IP. The same design principles can benefit
alternative, potentially application specific, network protocols, as
well as high-performance protocols for non-volatile memory
access. Finally, we will investigate library support for alternative
APIs on top of our low-level interface, such as
MegaPipe~\cite{DBLP:conf/osdi/HanMCR12}, cooperative
threading~\cite{DBLP:conf/sosp/BehrenCZNB03}, and rule-based
models~\cite{DBLP:conf/hotos/StutsmanO13}.

%The current prototype
%supports any stateless offload NIC with multiple queues and RSS
%support.  \ix load balances RSS flow groups onto queues, but we plan
%to explore using Intel's Flow Director~\cite{intel:82599} for outbound
%connections rather than reversing the Toeplitz hash.
