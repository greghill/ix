
\section{Discussion}
\label{sec:disc}


\myparagraph{What makes \ix fast:} The results in \S\ref{sec:eval}
show that a networking stack can be implemented in a protected OS
kernel and still deliver wire-rate performance for most benchmarks.
The tight coupling of the dataplane architecture, using only a minimal
amount of batching to amortize transition costs, causes application
logic to be scheduled at the right time, which is essential for
latency-sensitive workloads.  Therefore, the benefits of \ix go beyond
just minimizing kernel overheads. The lack of intermediate buffers
allows for efficient, application-specific implementations of I/O
abstractions such as our event library. The zero-copy approach helps
even when the user-level libraries add a level of copying, as it is
the case for the \texttt{libevent} compatible interfaces in our event
library.  The extra copy occurs much closer to the actual use, thereby
increasing cache locality.  Finally, we carefully tuned \ix for
multi-core scalability, eliminating constructs that introduce
synchronization or coherence traffic.

\input{figs/float-batch}

\myparagraph{Subtleties of adaptive batching:} All experiments in
\S\ref{sec:eval} were conducted with a maximal batch size of $B=64$
packets per iteration. In exploring the impact of the batch size, we
first confirmed the intuition that smaller values of $B$ reduce tail
latency.  Fig.~\ref{fig:batch} compares the throughout of a CPU-bound
microbenchmark for different values of B, and shows that batches as
low as $B=32$ can deliver maximal throughput for this workload.
However, we ran into unexpected limitation of adaptive batch when
running \ix at high packet rates, on many cores, and with small
average batch sizes: the high rate of PCIe writes required to post
fresh descriptors at every iteration lead to overall degraded
performance as we added cores to \ix.  To avoid the problem, we simply
coalesced PCIe writes on the receive path so that we replenished at
least 32 descriptor entries at a time.  Luckily, we did not have to
coalesce PCIe writes on the transmit path as that would have impacted
latency.


\begin{comment}
\myparagraph{Using interrupts as a fallback:} Some applications
service requests that require extended intervals of
compute time. We intend for these requests to be delegated
to background threads rather than elastic threads in order
to ensure that elastic threads remain responsive.
However, \ix could also be modified to better tolorate
unanticipated delays during application processing in elastic threads.
One option would be to use interrupts as a fallback mode. On the receive side, we
could program the NIC to fire an interrupt whenever the
recieve descriptor ring is almost full. The dataplane could
then move packets from the receive ring to a structure in software, averting
buffer underrun. On the transmit side, we could program
the NIC to fire an interrupt whenever the transmit ring becomes
empty so that it can be refilled. Such an interrupt would only need
to be armed when there is additional transmit data pending. A desirable
property of this approach is that neither interrupt would
be triggered as long as elastic threads are sufficiently responsive,
but if an elastic thread misbehaves, the \ix dataplane would
be able to regain control and catch up on network processing.
\end{comment}

%\myparagraph{Hardware Bottlenecks:} We achieved very high packet
%rates with \ix, often saturating the bandwidth limit of our 10 GbE NIC.
%One unexpected performance bottleneck was the NIC and its DMA engine.
%We discovered that minimizes register writes was an effective optimization.
%\adam{finish this section}

\myparagraph{Limitations of current prototype:} Our current
implementation does not yet exploit IOMMUs or VT-d. \adam{new:}Instead, it maps
descriptor rings directly into \ix memory, using the Linux
pagemap interface to determine physical addresses.  Although
this choice puts some level of trust into \ix, the applications remain
securely isolated. In the future we plan on using IOMMU support to
further isolate \ix dataplanes, and we anticipate overhead will be
low because of our use of large pages.

\adam{new:}We also intend to add support for interrupts in
\ix dataplanes. The executation model of \ix assumes some cooperation from applications.
Specifically they should handle events in a quick, non-blocking manner;
operations with extended execution times are expected to be delegated
to background threads rather than execute within the context of
elastic threads.  The \ix dataplane is designed around polling, with
the provision that interrupts can be configured as fallback
optimization to refresh receive descriptor rings when the ring is
nearly full and to refill the transmit queues when it is empty (steps
(1) and (6) in Fig~\ref{fig:dataplane}). Occasional timer interrupts
are also required to ensure full TCP compliance in the event an elastic
thread blocks for an extended period.

\myparagraph{Future work:} This paper focused on the \ix dataplane
design. \ix is designed and implemented to support the dynamic
addition and removal of elastic threads in order to achieve energy
proportional and resource efficient computing. So far we have tested
only static configurations. In future work, we will design a dynamic
runtime that rebalances hardware queues between available elastic
threads in a manner that maintains throughput and latency constraints.
We will also explore the synergies between \ix and networking
protocols (TCP-based or otherwise) designed to support microsecond-level latencies and reduced
buffering characteristics of \ix deployments such as
DCTCP~\cite{DBLP:conf/sigcomm/AlizadehGMPPPSS10} and
ECN~\cite{ramakrishnan2001addition}. Finally, we will investigate \adam{new:}
adding library support for alternative APIs on top of our low-level interface, such as
MegaPipe~\cite{DBLP:conf/osdi/HanMCR12}, cooperative
threading~\cite{DBLP:conf/sosp/BehrenCZNB03}, and rule-based
models~\cite{DBLP:conf/hotos/StutsmanO13}.

%The current prototype
%supports any stateless offload NIC with multiple queues and RSS
%support.  \ix load balances RSS flow groups onto queues, but we plan
%to explore using Intel's Flow Director~\cite{intel:82599} for outbound
%connections rather than reversing the Toeplitz hash.
