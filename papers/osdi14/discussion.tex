
\section{Discussion}
\label{sec:disc}


\myparagraph{What makes \ix fast:} The results in \S\ref{sec:eval}
show that a networking stack can be implemented in a protected OS
kernel and still deliver wire-rate performance for most benchmarks.
The tight coupling of the dataplane architecture, using only a minimal
amount of batching to amortize transition costs, causes application
logic to be scheduled at the right time, which is essential for
latency-sensitive workloads.  Therefore, the benefits of \ix go beyond
just minimizing kernel overheads. The lack of intermediate buffers
allows for efficient, application-specific implementations of I/O
abstractions such as our event library. The zero-copy approach helps
even when the user-level libraries add a level of copying, as it is
the case for the \texttt{libevent} compatible interfaces in our event
library.  The extra copy occurs much closer to the actual use, thereby
increasing cache locality.  Finally, we carefully tuned \ix for
multi-core scalability, eliminating constructs that introduce
synchronization or coherence traffic.

\input{figs/float-batch}

\myparagraph{Subtleties of adaptive batching:} All experiments in
\S\ref{sec:eval} were conducted with a maximal batch size of $B=64$
packets per iteration. In exploring the impact of the batch size \edb{ ---which impacts behavior when the workload is CPU-bound ---}, we
first confirmed the intuition that smaller values of $B$ reduce tail
latency. \edb{Similarly, adaptive batching improves on fixed-sized batching when the workload is not CPU-bound}.  \edb{Finally, we performed sensitivity analysis on the impact of the bound $B$ on throughput, as shown in }Fig.~\ref{fig:batch}, which compares the throughout of the
CPU-bound benchmarked used in Fig.~\ref{fig:short10:roundtrips}. The
experiment shows that batches as low as $B=32$ can deliver maximal
throughput for this workload.  However, we ran into an unexpected
limitation of adaptive batch when running \ix at high packet rates
with small average batch sizes; The high rate of PCIe writes required
to post fresh descriptors at every iteration led to performance
degradation as we scaled the number of cores used by \ix.  To avoid
this problem, we simply coalesced PCIe writes on the receive path so
that we replenished at least 32 descriptor entries at a time.
Luckily, we did not have to coalesce PCIe writes on the transmit path,
as that would have impacted latency.


\begin{comment}
  \myparagraph{Using interrupts as a fallback:} Some applications
  service requests that require extended intervals of compute time. We
  intend for these requests to be delegated to from elastic threads to
  background threads in order to ensure that elastic threads remain
  responsive.  However, \ix can also be modified to better tolerate
  unanticipated delays during application processing in elastic
  threads.  One option would be to use interrupts as a fallback
  mode. On the receive side, the NIC would fire an interrupt whenever
  the recieve descriptor ring is almost full. The dataplane could then
  move packets from the receive ring to a software structure, averting
  buffer underrun. On the transmit side, NIC would fire an interrupt
  whenever the transmit ring becomes empty so that it can be
  refilled. Such an interrupt would only need to be armed when there
  is additional transmit data pending. A desirable property of this
  approach is that neither interrupt would be triggered as long as
  elastic threads are sufficiently responsive, but if an elastic
  thread misbehaves, the \ix dataplane would be able to regain control
  and catch up on network processing.
\end{comment}

%\myparagraph{Hardware Bottlenecks:} We achieved very high packet
%rates with \ix, often saturating the bandwidth limit of our 10 GbE NIC.
%One unexpected performance bottleneck was the NIC and its DMA engine.
%We discovered that minimizes register writes was an effective optimization.
%\adam{finish this section}

\myparagraph{Limitations of current prototype:} Our current
implementation does not yet exploit IOMMUs or VT-d. Instead, it maps
descriptor rings directly into \ix memory, using the Linux pagemap
interface to determine physical addresses.  Although this choice puts
some level of trust into the \ix dataplane, application code remains
securely isolated. In the future we plan on using IOMMU support to
further isolate \ix dataplanes. We anticipate overhead will be low
because of our use of large pages.

\edb{ADD SENTENCE ON SR-IOV.  Discuss whether this is trivial or not, and whether the RSS limitatiosn associated with SR-IOV impact the design XXX}

We also intend to add support for interrupts in \ix dataplanes. The
execution model of the \ix dataplane assumes some cooperation from
application code running in elastic threads.  Specifically it should
handle events in a quick, non-blocking manner; operations with
extended execution times are expected to be delegated to background
threads rather than execute within the context of elastic threads.
The \ix dataplane is designed around polling, with the provision that
interrupts can be configured as fallback optimization to refresh
receive descriptor rings when the ring is nearly full and to refill
the transmit queues when it is empty (steps (1) and (6) in
Fig~\ref{fig:dataplane}). Occasional timer interrupts are also
required to ensure full TCP compliance in the event an elastic thread
blocks for an extended period.

\myparagraph{Future work:} This paper focused primarily on the \ix
dataplane architecture. \ix is designed and implemented to support the
dynamic addition and removal of elastic threads in order to achieve
energy proportional and resource efficient computing. So far we have
tested only static configurations. In future work, we will explore
control plane issues, including a
dynamic runtime that rebalances hardware queues between available
elastic threads in a manner that maintains throughput and latency
constraints.  We will also explore the synergies between \ix and
networking protocols designed to support microsecond-level latencies
and the reduced buffering characteristics of \ix deployments, such as
DCTCP~\cite{DBLP:conf/sigcomm/AlizadehGMPPPSS10} and
ECN~\cite{ramakrishnan2001addition}. Note that the \ix dataplane is
not specific to TCP/IP. The same design principles can benefit
alternative, potentially application specific, network protocols, as
well as high-performance protocols for non-volatile memory
access. Finally, we will investigate library support for alternative
APIs on top of our low-level interface, such as
MegaPipe~\cite{DBLP:conf/osdi/HanMCR12}, cooperative
threading~\cite{DBLP:conf/sosp/BehrenCZNB03}, and rule-based
models~\cite{DBLP:conf/hotos/StutsmanO13}.

%The current prototype
%supports any stateless offload NIC with multiple queues and RSS
%support.  \ix load balances RSS flow groups onto queues, but we plan
%to explore using Intel's Flow Director~\cite{intel:82599} for outbound
%connections rather than reversing the Toeplitz hash.
