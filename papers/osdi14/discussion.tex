
\section{Discussion}
\label{sec:disc}

We now discuss a number of important lessons, design issues, and
limitations of the current \ix implementation.

\myparagraph{What makes \ix faster:} The results in \S\ref{sec:eval}
show that a networking stack can be implemented in a protected OS
kernel and still deliver wire-rate performance for most benchmarks.
The tight coupling of the dataplane architecture, using only a minimal
amount of batching to amortize transition costs, causes application
logic to be scheduled at the right time, which is essential for
latency-sensitive workloads.  Therefore, the benefits of \ix go beyond
just minimizing kernel overheads. Similar to
Exokernel~\cite{DBLP:conf/sosp/EnglerKO95}, the lack of intermediate
buffers pushes all abstractions outside of the OS kernel, allowing for
efficient, application-specific implementations.  In particular, the
zero-copy approach helps even when the user-level libraries add a
level of copying, as it is the case for \texttt{libevent}
compatibility.  The extra copy occurs much closer to the actual use,
thereby increasing cache locality.  Finally, we took great care in
tuning the implementation of \ix for multi-core scalability,
eliminating constructs that introduce synchronization or coherence
traffic.

% \myparagraph{Selecting \ix's TCP stack:} 
% The \ix kernel includes a complete network protocol stack with
% RFC-compliant support for TCP, UDP, ARP, ICMP and LACP.  It however
% explicitly lacks a complete IP-layer and is therefore only a capable
% networking endpoint, but not of handling routing, bridging or
% filtering functions.  It also totally lacks a socket layer and its
% associated API and semantics.  For our implementation of TCP, we chose
% to lwIP~\cite{dunkels2001design} as a starting point for our code base because of its
% modularity and is maturity as a compliant, feature-rich networking
% stack.  lwIP was however designed with memory-efficiency in mind,
% primarily for embedded environments.  We radically changed the data
% structures that organize protocol control blocks for scalability, as
% well as for finer-grain, precise timer management.  However, we did
% not (yet) attempt to optimize the code paths for performance, and
% consequently believe that our results have room for improvement.

\myparagraph{Challenges in API design:} \adam{TODO}

\todo Lessons learned: importance of hardware knobs and
micro-architectural effects.  \christos{should we get into that? at
  least say that this will be better with Hasswell, more cores, better
  nics. the trends are in our favor}

\myparagraph{Limitations of current prototype:} The curent prototype
supports any stateless offload NIC with multiple queues and RSS
support. The NICs we currently use support RSS group of 16 queues,
which is inadequate given current multi-core trends. Nevertheless, the
trend in NICs seems to be towards support for larger number of
queues~\cite{refs,nsdi14}. We also plan to explore using Intel's Flow
Director~\cite{intel:82599} to provide connection
affinity~\cite{DBLP:conf/eurosys/PesterevSZM12} as an alternative to
our current flow hashing implementation. Our current implementation
does not exploit IOMMUs or VT-d. Instead, it uses Dune to map
descriptor rings directly into \ix memory as a form of
paravirtualization~\cite{DBLP:conf/sosp/BarhamDFHHHN03}.  Although
this choice puts some level of trust into \ix, the applications remain
securely isolated.  \christos{maybe something about the packet rate
  limitatations of nics}

\myparagraph{Future work:} This paper focused on the \ix dataplane
design. \ix is designed and implemented to support the dynamic
addition and removal of elastic threads in order to achieve energy
proportional and resource efficient computing. So far we have tested
only static configurations. In future work, we will design a dynamic
runtime that rebalances hardware queues between available elastic
threads in a manner that maintains throughput and latency constraints.
%
We will also explore the synergies between \ix and networking
protocols designed to support microsecond-level latencies and reduced
buffering characteristics of \ix deployments such as
DCTCP~\cite{DBLP:conf/sigcomm/AlizadehGMPPPSS10} and
ECN~\cite{ramakrishnan2001addition}.


 % Looking ahead, we envision that an
% \ix-style model may provide a higher performance alternative to the
% current OS-based and hypervisor-based implementations of virtual
% switches~\cite{openvswitch} and IP-level features (e.g., iptables).
% In particular, the security model of \ix, which enforces modularity
% without introducing noticeable overheads, appears like a great match
% to meet the protection, multi-tenancy, and scalability requirements of
% network function virtualization~\cite{etsi:NFV}.




%\todo Use of dataplane vs. virtual machines. 

