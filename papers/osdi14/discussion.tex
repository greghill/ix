
\section{Discussion}
\label{sec:disc}


\myparagraph{What makes \ix faster:} The results in \S\ref{sec:eval}
show that a networking stack can be implemented in a protected OS
kernel and still deliver wire-rate performance for most benchmarks.
The tight coupling of the dataplane architecture, using only a minimal
amount of batching to amortize transition costs, causes application
logic to be scheduled at the right time, which is essential for
latency-sensitive workloads.  Therefore, the benefits of \ix go beyond
just minimizing kernel overheads. Similar to
Exokernel~\cite{DBLP:conf/sosp/EnglerKO95}, the lack of intermediate
buffers pushes all abstractions outside of the OS kernel, allowing for
efficient, application-specific implementations.  In particular, the
zero-copy approach helps even when the user-level libraries add a
level of copying, as it is the case for compatibility interfaces in
our event library.  The extra copy occurs much closer to the actual
use, thereby increasing cache locality.  Finally, we took great care
in tuning the implementation of \ix for multi-core scalability,
eliminating constructs that introduce synchronization or coherence
traffic.

\myparagraph{Using interrupts as a fallback:}
\edb{ALT. TO NEXT PAR.} 
The security model of \ix assumes some cooperation from applications,
specifically to handle events in a quick, non-blocking manner;
operations with extended execution times are expected to be delegated
to background threads rather than execute within the context of
elastic threads.  The \ix dataplane is designed around polling, with
the provision that interrupts can be configured as fallback
optimization to refresh receive descriptor rings when the ring is
nearly full and to refill the transmit queues when it is empty (step
(1) and (6) in Fig~\ref{fig:dataplane}).  Timer interrupts exist
primarily to detect runaway elastic threads and guarantee resource
revocation.


\myparagraph{Using interrupts as a fallback:} Some applications
service requests that require extended intervals of
compute time. We intend for these requests to be delegated
to background threads rather than elastic threads in order
to ensure that elastic threads remain responsive.
However, \ix could also be modified to better tolorate
unanticipated delays during application processing in elastic threads.
One option would be to use interrupts as a fallback mode. On the receive side, we
could program the NIC to fire an interrupt whenever the
recieve descriptor ring is almost full. The dataplane could
then move packets from the receive ring to a structure in software, averting
buffer underrun. On the transmit side, we could program
the NIC to fire an interrupt whenever the transmit ring becomes
empty so that it can be refilled. Such an interrupt would only need
to be armed when there is additional transmit data pending. A desirable
property of this approach is that neither interrupt would
be triggered as long as elastic threads are sufficiently responsive,
but if an elastic thread misbehaves, the \ix dataplane would
be able to regain control and catch up on network processing.


\myparagraph{Hardware Bottlenecks:} We achieved very high packet
rates with \ix, often saturating the bandwidth limit of our 10 GbE NIC.
One unexpected performance bottleneck was the NIC and its DMA engine.
We discovered that minimizes register writes was an effective optimization.
\adam{finish this section}


\myparagraph{Sublteties of adaptive batching:} \edb{TODO}


\myparagraph{Limitations of current prototype:} The current prototype
supports any stateless offload NIC with multiple queues and RSS
support. \edb{REDO\sout{The NICs we currently use support RSS group of 16 queues,
which is inadequate given current multi-core trends}}. Nevertheless, the
trend in NICs seems to be towards support for larger number of
queues~\cite{radhakrishnan2014senic}. We also plan to explore using Intel's Flow
Director~\cite{intel:82599} to provide connection
affinity~\cite{DBLP:conf/eurosys/PesterevSZM12} as an alternative to
our current flow hashing implementation. Our current implementation
does not exploit IOMMUs or VT-d. Instead, it uses Dune to map
descriptor rings directly into \ix memory as a form of
paravirtualization~\cite{DBLP:conf/sosp/BarhamDFHHHN03}.  Although
this choice puts some level of trust into \ix, the applications remain
securely isolated.

\myparagraph{Future work:} This paper focused on the \ix dataplane
design. \ix is designed and implemented to support the dynamic
addition and removal of elastic threads in order to achieve energy
proportional and resource efficient computing. So far we have tested
only static configurations. In future work, we will design a dynamic
runtime that rebalances hardware queues between available elastic
threads in a manner that maintains throughput and latency constraints.
%
We will also explore the synergies between \ix and networking
protocols designed to support microsecond-level latencies and reduced
buffering characteristics of \ix deployments such as
DCTCP~\cite{DBLP:conf/sigcomm/AlizadehGMPPPSS10} and
ECN~\cite{ramakrishnan2001addition}. Finally, we will investigate \ix
implementations of alternative APIs, such as
MegaPipe~\cite{DBLP:conf/osdi/HanMCR12}, cooperative
threading~\cite{DBLP:conf/sosp/BehrenCZNB03}, and rule-based
models~\cite{DBLP:conf/hotos/StutsmanO13}.

