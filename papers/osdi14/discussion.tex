
\section{Discussion}
\label{sec:disc}


\myparagraph{What makes \ix fast:} The results in \S\ref{sec:eval}
show that a networking stack can be implemented in a protected OS
kernel and still deliver wire-rate performance for most benchmarks.
The tight coupling of the dataplane architecture, using only a minimal
amount of batching to amortize transition costs, causes application
logic to be scheduled at the right time, which is essential for
latency-sensitive workloads.  Therefore, the benefits of \ix go beyond
just minimizing kernel overheads. The lack of intermediate buffers
allows for efficient, application-specific implementations of I/O
abstractions such as our event library. The zero-copy approach helps
even when the user-level libraries add a level of copying, as it is
the case for the \texttt{libevent} compatible interfaces in our event
library.  The extra copy occurs much closer to the actual use, thereby
increasing cache locality.  Finally, we carefully tuned \ix for
multi-core scalability, eliminating constructs that introduce
synchronization or coherence traffic.



\myparagraph{Subtleties of adaptive batching:} 
\input{figs/float-batch}

Batching is commonly understood to trade off better throughput at high
loads for higher latency at low loads.  Our use of adaptive, bounded
batching actually improves on both metrics. In exploring the impact of
the batch size bound --- which impacts behavior when the workload is
CPU-bound --- we first confirmed the intuition that larger batch
sizes improve throughput, but only to a point. Fig.~\ref{fig:batch}
compares the throughput of the CPU-bound benchmark used in
Fig.~\ref{fig:short10:roundtrips}.  The experiment shows that batches
as low as $B=32$ can deliver maximal throughput for this
workload. (All experiments in \S\ref{sec:eval} were conducted with a
maximal batch size of $B=64$ packets per iteration.)  Because the
algorithm is adaptive, $B$ does not impact latency when the dataplane
is not saturated; we confirmed
this experimentally at the 99th percentile latency for
\texttt{memcache}.

However, we ran into an unexpected hardware limitation
when running \ix at high packet rates with small average batch sizes 
(i.e. when the dataplane is not saturated): 
the high rate of PCIe writes required to post fresh
descriptors at every iteration led to performance degradation as we
scaled the number of cores used by \ix.  To avoid this problem, we
simply coalesced PCIe writes on the receive path so that we
replenished at least 32 descriptor entries at a time.  Luckily, we did
not have to coalesce PCIe writes on the transmit path, as that would
have impacted latency.


\begin{comment}
  \myparagraph{Using interrupts as a fallback:} Some applications
  service requests that require extended intervals of compute time. We
  intend for these requests to be delegated to from elastic threads to
  background threads in order to ensure that elastic threads remain
  responsive.  However, \ix can also be modified to better tolerate
  unanticipated delays during application processing in elastic
  threads.  One option would be to use interrupts as a fallback
  mode. On the receive side, the NIC would fire an interrupt whenever
  the recieve descriptor ring is almost full. The dataplane could then
  move packets from the receive ring to a software structure, averting
  buffer underrun. On the transmit side, NIC would fire an interrupt
  whenever the transmit ring becomes empty so that it can be
  refilled. Such an interrupt would only need to be armed when there
  is additional transmit data pending. A desirable property of this
  approach is that neither interrupt would be triggered as long as
  elastic threads are sufficiently responsive, but if an elastic
  thread misbehaves, the \ix dataplane would be able to regain control
  and catch up on network processing.
\end{comment}


\myparagraph{Limitations of current prototype:} Our current
implementation does not yet exploit IOMMUs or VT-d. Instead, it maps
descriptor rings directly into \ix memory, using the Linux pagemap
interface to determine physical addresses.  Although this choice puts
some level of trust into the \ix dataplane, application code remains
securely isolated. In the future we plan on using IOMMU support to
further isolate \ix dataplanes. We anticipate overhead will be low
because of our use of large pages.
Also, our prototype currently does not take advantage of the NIC's SR-IOV capabilities, but instead allocates entire physical devices to dataplanes.

Furthermore, we intend to add support for interrupts to \ix dataplanes. The
execution model of the \ix dataplane assumes some cooperation from
application code running in elastic threads.  Specifically it should
handle events in a quick, non-blocking manner; operations with
extended execution times are expected to be delegated to background
threads rather than execute within the context of elastic threads.
The \ix dataplane is designed around polling, with the provision that
interrupts can be configured as fallback optimization to refresh
receive descriptor rings when the ring is nearly full and to refill
the transmit queues when it is empty (steps (1) and (6) in
Fig~\ref{fig:dataplane}). Occasional timer interrupts are also
required to ensure full TCP compliance in the event an elastic thread
blocks for an extended period.

\myparagraph{Future work:} This paper focused primarily on the \ix
dataplane architecture. \ix is designed and implemented to support the
dynamic addition and removal of elastic threads in order to achieve
energy proportional and resource efficient computing. So far we have
tested only static configurations. In future work, we will explore
control plane issues, including a
dynamic runtime that rebalances hardware queues between available
elastic threads in a manner that maintains throughput and latency
constraints.  We will also explore the synergies between \ix and
networking protocols designed to support microsecond-level latencies
and the reduced buffering characteristics of \ix deployments, such as
DCTCP~\cite{DBLP:conf/sigcomm/AlizadehGMPPPSS10} and
ECN~\cite{ramakrishnan2001addition}. Note that the \ix dataplane is
not specific to TCP/IP. The same design principles can benefit
alternative, potentially application specific, network protocols, as
well as high-performance protocols for non-volatile memory
access. Finally, we will investigate library support for alternative
APIs on top of our low-level interface, such as
MegaPipe~\cite{DBLP:conf/osdi/HanMCR12}, cooperative
threading~\cite{DBLP:conf/sosp/BehrenCZNB03}, and rule-based
models~\cite{DBLP:conf/hotos/StutsmanO13}.

%The current prototype
%supports any stateless offload NIC with multiple queues and RSS
%support.  \ix load balances RSS flow groups onto queues, but we plan
%to explore using Intel's Flow Director~\cite{intel:82599} for outbound
%connections rather than reversing the Toeplitz hash.
