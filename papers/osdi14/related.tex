

\section{Discussion}

\christos{I think these issues should be discussed within the
  implementation and eval sections}
\todo Lessons learned: importance of hardware knobs and micro-architectural effects.

\todo libevent compatibility

\todo Use of dataplane vs. virtual machines. 

\section{Related Work}

We organize the discussion topically, while avoiding redundancy with
the commentary already exposed in \S\ref{sec:motivation:current}.


\myparagraph{Separation of control and data plane:} The notion of
architected separation between control and data plane is pervasise in
the design of networking protocols and devices.  In systems,
virtualization also separates the control and execution functions. For
example, a type-2
hypervisors~\cite{DBLP:journals/tocs/BugnionDRSW12,misc/kivity07kvm}
use a Linux environment for control, and a separate virtualization
module for execution.  We leverage Dune and the same virtualization
hardware to run the \ix as a process of the control plane.
Arrakis~\cite{peter2013arrakis,arrakisTR13} recently proposed to
separate the networking stack into a distinct dataplane.  Our work
most closely ressembles Arrakis, but with noticeable differences:
first, Arrakis uses the Barrelfish multikernel as its control plane
and run-time layer, whereas the \ix control plane runs on a vanilla
Linux foundation.  Second, Arrakis runs its own, homegrown, networking
stack at userlevel, whereas \ix uses runs its networking stack
protected from the application.

\myparagraph{User-level networking stacks:} Our work shares the same
motivation as mTCP~\cite{jeong2014mtcp} in providing highly-scable
networking stack for web-scale applications, including in the presence
of high connection count and churn.  Both also focus on bounding the
latency of RPC, and were designed to use the same standard Intel NIC.
mTCP has an asymmetrical model where the first hyperthread of each
core runs the networking stack and the second one the application.
Also, the application and networking stack share the same address
space without protection.  \ix can use both hyperthreads symmetrically
and protects the protocol stack.  \edb{PLACEHOLDER:} Our results show
that, for the same workloads, \ix typically outperforms mTCP by 3x on
CPU-bound workloads, and provides lower latency.
 
\myparagraph{Hardware and protocol specialization:} Our work applies
specialization in software to commodity servers and networking
equipment.  Although our work shows that latencies and jitter can be
substantially reduced, on the same hardware, by using \ix rather than
standard Linux stacks or even userlevel stacks, we are still outside
of the latency range enabled by specialized adapters that expose RDMA
directly to applications.  Commercial Infiband RDMA adapters offer
RDMA and remote procedure calls in
$1-3$\microsecond~\cite{DBLP:conf/sosp/OngaroRSOR11,Jose:2011:MDH,mitchell:rdma,dragojevic14farm}.
Our current evaluation of \ix suggests that there is still room for
improvements.  Indeed, on the same hardware, we've measured UDP
one-way latencies that were a fraction of the NetPIPE latencies,
suggesing that the TCP codebase could be substantially optimized in
future work.

\paragraph{Native, zero-copy API.}

\todo IOLITE ~\cite{DBLP:journals/tocs/PaiDZ00}
\todo edb{MORE}

\paragraph{Adaptive, batched run to completion}

\todo TODO

\paragraph{Coherence-free, flow-consistent processing}

\todo Affinity Accept, mTCP, ...

\paragraph{Buffering at the NIC edge}

\todo DCTCP (actually belongs there?)


\paragraph{Improvements to existing operating systems.}

\paragraph{API design.}


\paragraph{Hardware specialization - Dataplanes.}

\paragraph{Operating system architecture.}

\paragraph{Domain-specific and library operating systems.}
