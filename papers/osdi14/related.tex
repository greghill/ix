

\section{Discussion}

\christos{I think these issues should be discussed within the
  implementation and eval sections}
\todo Lessons learned: importance of hardware knobs and micro-architectural effects.

\todo libevent compatibility

\todo Use of dataplane vs. virtual machines. 

\section{Related Work}

We organize the discussion topically, while avoiding redundancy with
the commentary already exposed in \S\ref{sec:motivation:current}.


\myparagraph{Separation of control and data plane:} The notion of
architected separation between control and data plane is pervasise in
the design of networking protocols and devices.  In systems,
virtualization also separates the control and execution functions. For
example, a type-2
hypervisors~\cite{DBLP:journals/tocs/BugnionDRSW12,misc/kivity07kvm}
use a Linux environment for control, and a separate virtualization
module for execution.  We leverage Dune and the same virtualization
hardware to run the \ix as a process of the control plane.
Arrakis~\cite{peter2013arrakis,arrakisTR13} recently proposed to
separate the networking stack into a distinct dataplane.  Our work
most closely ressembles Arrakis, but with noticeable differences:
first, Arrakis uses the Barrelfish multikernel as its control plane
and run-time layer, whereas the \ix control plane runs on a vanilla
Linux foundation.  Second, Arrakis runs its own, homegrown, networking
stack at userlevel, whereas \ix uses runs its networking stack
protected from the application.

\myparagraph{User-level networking stacks:} Our work shares the same
motivation as mTCP~\cite{jeong2014mtcp} in providing highly-scable
networking stack for web-scale applications, including in the presence
of high connection count and churn.  Both also focus on bounding the
latency of RPC, and were designed to use the same standard Intel NIC.
mTCP has an asymmetrical model where the first hyperthread of each
core runs the networking stack and the second one the application.
Also, the application and networking stack share the same address
space without protection.  \ix can use both hyperthreads symmetrically
and protects the protocol stack.  \edb{PLACEHOLDER:} Our results show
that, for the same workloads, \ix typically outperforms mTCP by 3x on
CPU-bound workloads, and provides lower latency.
 
\myparagraph{Hardware and protocol specialization:} Our work applies
specialization in software to commodity servers and networking
equipment.  Although our work shows that latencies and jitter can be
substantially reduced, on the same hardware, by using \ix rather than
standard Linux stacks or even userlevel stacks, we are still outside
of the latency range enabled by specialized adapters that expose RDMA
directly to applications.  Commercial Infiband RDMA adapters offer
RDMA and remote procedure calls in
$1-3$\microsecond~\cite{DBLP:conf/sosp/OngaroRSOR11,Jose:2011:MDH,mitchell:rdma,dragojevic14farm}.
Our current evaluation of \ix suggests that there is still room for
improvements.  Indeed, on the same hardware, we've measured UDP
one-way latencies that were a fraction of the NetPIPE latencies,
suggesing that the TCP codebase could be substantially optimized in
future work.

\myparagraph{Asynchronous and zero-copy interactions:}
Many systems address the system interaction limitations of the POSIX
model, and in particular inability to batch requests across sockets,
For example, mTCP~\cite{jeong2014mtcp},
MegaPipe~\cite{han2012megapipe} and NetMap~\cite{rizzo2012netmap} all
use some shared memory queue or ring for communication between the
networking stack and the application.  The queue or ring is required
because of the asynchronous execution of the two components.  In
contrast, \ix uses an array mechanisms and an interleaved execution
model, which enables coherence-free execution.  The semantics of POSIX
sockets also prevent non-blocking, zero-copy transfer of data. Systems
such as IO-lite~~\cite{DBLP:journals/tocs/PaiDZ00} use memory
protection mechanisms to unify buffering.  \ix enables a zero-copy
model: on receive, it relies on memory protection to safely expose
incoming packets to application; on send, \ix explicitely notifies the
application when the data has been acknowledged by the peer.



\paragraph{Adaptive, batched run to completion}

\todo TODO

\paragraph{Coherence-free, flow-consistent processing}

\todo Affinity Accept, mTCP, ...

\paragraph{Buffering at the NIC edge}

\todo DCTCP (actually belongs there?)


\paragraph{Improvements to existing operating systems.}

\paragraph{API design.}


\paragraph{Hardware specialization - Dataplanes.}

\paragraph{Operating system architecture.}

\paragraph{Domain-specific and library operating systems.}

\subsection{Misc. to cite}

Stuff to cite (that is not otherwise cited) ... that we should either incorporate or drop:


\begin{itemize}

\oldcite{Alizadeh DCTCP}{DBLP:conf/sigcomm/AlizadehGMPPPSS10}
\oldcite{Libra JVM}{DBLP:conf/vee/AmmonsABSGKKRHW07}
\oldcite{Scheduler activations}{DBLP:journals/tocs/AndersonBLL92}
\oldcite{Atikoglu - Workload analysis fo large-scale key-value store}{Atikoglu:2012:WAL}
\oldcite{Basu - Efficient virutal memory for big memory servers}{DBLP:conf/isca/BasuGCHS13}
\oldcite{Multikernel}{DBLP:conf/sosp/BaumannBDHIPRSS09}
\oldcite{RadixVM}{DBLP:conf/eurosys/ClementsKZ13}
\oldcite{TCP Onloading}{DBLP:journals/cacm/DeanB13}
\oldcite{Unikernel - case for single app OS}{DBLP:conf/asplos/MadhavapeddyMRSSGSHC13}
\oldcite{Waldspurger - CPU balloning}{DBLP:conf/osdi/Waldspurger02}
\oldcite{Drepper - cost of virtualization}{DBLP:journals/queue/Drepper08}
\oldcite{Disco}{DBLP:journals/tocs/BugnionDGR97}
\oldcite{Salzer -E2E principle}{DBLP:journals/tocs/SaltzerRC84}
\end{itemize}

